{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9582370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, unix_timestamp, when\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"US Accidents Severity Prediction\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "080bb710",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"US_Accidents_March23.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d7fe953",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df \\\n",
    "    .drop(\"ID\",\"Source\", \"Zipcode\", \"Timezone\", \"Airport_Code\", \"Amenity\",\n",
    "          \"Bump\", \"Give_Way\", \"No_Exit\", \"Railway\", \"Description\", \"County\",\n",
    "          \"Roundabout\", \"Station\", \"Stop\", \"Nautical_Twilight\", \"Astronomical_Twilight\", \"Country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ba9943f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+------------------+-------+-------+------------+--------------------+------------+-----+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+--------+--------+---------------+--------------+------------+--------------+--------------+--------+\n",
      "|Severity|         Start_Lat|         Start_Lng|End_Lat|End_Lng|Distance(mi)|              Street|        City|State|  Weather_Timestamp|Temperature(F)|Wind_Chill(F)|Humidity(%)|Pressure(in)|Visibility(mi)|Wind_Direction|Wind_Speed(mph)|Precipitation(in)|Weather_Condition|Crossing|Junction|Traffic_Calming|Traffic_Signal|Turning_Loop|Sunrise_Sunset|Civil_Twilight|Duration|\n",
      "+--------+------------------+------------------+-------+-------+------------+--------------------+------------+-----+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+--------+--------+---------------+--------------+------------+--------------+--------------+--------+\n",
      "|       3|         39.865147|        -84.058723|   NULL|   NULL|        0.01|              I-70 E|      Dayton|   OH|2016-02-08 05:58:00|          36.9|         NULL|       91.0|       29.68|          10.0|          Calm|           NULL|             0.02|       Light Rain|   false|   false|          false|         false|       false|         Night|         Night|   314.0|\n",
      "|       2| 39.92805900000001|        -82.831184|   NULL|   NULL|        0.01|            Brice Rd|Reynoldsburg|   OH|2016-02-08 05:51:00|          37.9|         NULL|      100.0|       29.65|          10.0|          Calm|           NULL|              0.0|       Light Rain|   false|   false|          false|         false|       false|         Night|         Night|    30.0|\n",
      "|       2|         39.063148|        -84.032608|   NULL|   NULL|        0.01|      State Route 32|Williamsburg|   OH|2016-02-08 06:56:00|          36.0|         33.3|      100.0|       29.67|          10.0|            SW|            3.5|             NULL|         Overcast|   false|   false|          false|          true|       false|         Night|         Night|    30.0|\n",
      "|       3|         39.747753|-84.20558199999998|   NULL|   NULL|        0.01|              I-75 S|      Dayton|   OH|2016-02-08 07:38:00|          35.1|         31.0|       96.0|       29.64|           9.0|            SW|            4.6|             NULL|    Mostly Cloudy|   false|   false|          false|         false|       false|         Night|           Day|    30.0|\n",
      "|       2|         39.627781|        -84.188354|   NULL|   NULL|        0.01|Miamisburg Center...|      Dayton|   OH|2016-02-08 07:53:00|          36.0|         33.3|       89.0|       29.65|           6.0|            SW|            3.5|             NULL|    Mostly Cloudy|   false|   false|          false|          true|       false|           Day|           Day|    30.0|\n",
      "|       3|40.100590000000004|-82.92519399999998|   NULL|   NULL|        0.01|      Westerville Rd| Westerville|   OH|2016-02-08 07:51:00|          37.9|         35.5|       97.0|       29.63|           7.0|           SSW|            3.5|             0.03|       Light Rain|   false|   false|          false|         false|       false|           Day|           Day|    30.0|\n",
      "|       2|         39.758274|-84.23050699999997|   NULL|   NULL|         0.0|      N Woodward Ave|      Dayton|   OH|2016-02-08 07:56:00|          34.0|         31.0|      100.0|       29.66|           7.0|           WSW|            3.5|             NULL|         Overcast|   false|   false|          false|         false|       false|           Day|           Day|    30.0|\n",
      "|       3|         39.770382|        -84.194901|   NULL|   NULL|        0.01|           N Main St|      Dayton|   OH|2016-02-08 07:56:00|          34.0|         31.0|      100.0|       29.66|           7.0|           WSW|            3.5|             NULL|         Overcast|   false|   false|          false|         false|       false|           Day|           Day|    30.0|\n",
      "|       2|         39.778061|        -84.172005|   NULL|   NULL|         0.0|      Notre Dame Ave|      Dayton|   OH|2016-02-08 07:58:00|          33.3|         NULL|       99.0|       29.67|           5.0|            SW|            1.2|             NULL|    Mostly Cloudy|   false|   false|          false|         false|       false|           Day|           Day|    30.0|\n",
      "|       3|40.100590000000004|-82.92519399999998|   NULL|   NULL|        0.01|      Westerville Rd| Westerville|   OH|2016-02-08 08:28:00|          37.4|         33.8|      100.0|       29.62|           3.0|           SSW|            4.6|             0.02|       Light Rain|   false|   false|          false|         false|       false|           Day|           Day|    30.0|\n",
      "|       3|         39.952812|        -83.119293|   NULL|   NULL|        0.01|         Outerbelt S|    Columbus|   OH|2016-02-08 07:50:00|          35.6|         30.7|       93.0|       29.64|           5.0|           WNW|            5.8|             NULL|             Rain|    true|    true|          false|         false|       false|           Day|           Day|    30.0|\n",
      "|       3|         39.932709|         -82.83091|   NULL|   NULL|        0.01|              I-70 E|Reynoldsburg|   OH|2016-02-08 08:28:00|          37.4|         33.8|      100.0|       29.62|           3.0|           SSW|            4.6|             0.02|       Light Rain|   false|   false|          false|         false|       false|           Day|           Day|    30.0|\n",
      "|       2|         39.737633|-84.14993299999998|   NULL|   NULL|         0.0|      Watervliet Ave|      Dayton|   OH|2016-02-08 08:28:00|          33.8|         NULL|      100.0|       29.63|           3.0|            SW|            2.3|             NULL|         Overcast|   false|   false|          false|         false|       false|           Day|           Day|    30.0|\n",
      "|       2|          39.79076|        -84.241547|   NULL|   NULL|        0.01|           Salem Ave|      Dayton|   OH|2016-02-08 08:56:00|          36.0|         31.1|       89.0|       29.65|          10.0|            NW|            5.8|             NULL|    Mostly Cloudy|   false|   false|          false|          true|       false|           Day|           Day|    30.0|\n",
      "|       2|         39.972038|        -82.913521|   NULL|   NULL|        0.01|          E Broad St|    Columbus|   OH|2016-02-08 08:28:00|          37.4|         33.8|      100.0|       29.62|           3.0|           SSW|            4.6|             0.02|       Light Rain|   false|   false|          false|          true|       false|           Day|           Day|    30.0|\n",
      "|       2|         39.745888|         -84.17041|   NULL|   NULL|        0.01|         Glencoe Ave|      Dayton|   OH|2016-02-08 08:28:00|          33.8|         NULL|      100.0|       29.63|           3.0|            SW|            2.3|             NULL|         Overcast|   false|   false|          false|         false|       false|           Day|           Day|    30.0|\n",
      "|       2|         39.748329|        -84.224007|   NULL|   NULL|        0.01|S James H McGee Blvd|      Dayton|   OH|2016-02-08 08:58:00|          35.6|         NULL|       99.0|       29.65|           7.0|           WSW|            2.3|             NULL|    Mostly Cloudy|   false|   false|          false|         false|       false|           Day|           Day|    30.0|\n",
      "|       2|         39.752174|        -84.239952|   NULL|   NULL|         0.0|         Delphos Ave|      Dayton|   OH|2016-02-08 08:56:00|          36.0|         31.1|       89.0|       29.65|          10.0|            NW|            5.8|             NULL|    Mostly Cloudy|   false|   false|          false|         false|       false|           Day|           Day|    30.0|\n",
      "|       2|         39.740669|        -84.184135|   NULL|   NULL|        0.01|          Rubicon St|      Dayton|   OH|2016-02-08 09:38:00|          37.4|         32.1|       93.0|       29.63|          10.0|           WSW|            6.9|             NULL|         Overcast|    true|   false|          false|          true|       false|           Day|           Day|    30.0|\n",
      "|       2|         39.790703|        -84.244461|   NULL|   NULL|        0.01|     W Hillcrest Ave|      Dayton|   OH|2016-02-08 09:56:00|          36.0|         30.3|       89.0|       29.65|          10.0|          West|            6.9|             NULL|    Mostly Cloudy|   false|   false|          false|         false|       false|           Day|           Day|    30.0|\n",
      "+--------+------------------+------------------+-------+-------+------------+--------------------+------------+-----+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+--------+--------+---------------+--------------+------------+--------------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df2 \\\n",
    "    .withColumn(\"Start_TS\", to_timestamp(col(\"Start_Time\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "    .withColumn(\"End_TS\", to_timestamp(col(\"End_Time\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "    .withColumn(\"Duration\", ((unix_timestamp(col(\"End_TS\")) - unix_timestamp(col(\"Start_TS\"))) / 60).cast(DoubleType())) \\\n",
    "    .drop(\"Start_TS\", \"End_TS\", \"Start_Time\", \"End_Time\")\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3077859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = [ \"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\", \"Crossing\", \"Traffic_Signal\",\n",
    "      \"Wind_Speed(mph)\", \"Precipitation(in)\", \"Weather_Condition\", \"Wind_Direction\", \"Junction\", \"Duration\",\"Severity\",\n",
    "      \"Civil_Twilight\", \"Sunrise_Sunset\", \"State\", \"City_Cleaned\", \"Wind_Chill(F)\",  \"Street_Cleaned\" ]\n",
    "\n",
    "\n",
    "# Hangi sütunları işleyeceğimizi tanımla\n",
    "columns_to_clean = [\"City\", \"Street\"]\n",
    "top_n = 128\n",
    "\n",
    "\n",
    "# Sık geçen değerleri belirleyip \"Other\" ile gruplayan fonksiyon\n",
    "def clean_column(df, column_name, top_n=128):\n",
    "    top_values_df = df.groupBy(column_name).count().orderBy(col(\"count\").desc()).limit(top_n)\n",
    "    top_values_list = [row[column_name] for row in top_values_df.collect()]\n",
    "   \n",
    "    cleaned_col_name = f\"{column_name}_Cleaned\"\n",
    "    df = df.withColumn(\n",
    "        cleaned_col_name,\n",
    "        when(col(column_name).isin(top_values_list), col(column_name)).otherwise(\"Other\")\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d0c9973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+--------------+--------+--------------+---------------+-----------------+-----------------+--------------+--------+--------+--------+--------------+--------------+-----+------------+-------------+--------------+\n",
      "|Temperature(F)|Humidity(%)|Pressure(in)|Visibility(mi)|Crossing|Traffic_Signal|Wind_Speed(mph)|Precipitation(in)|Weather_Condition|Wind_Direction|Junction|Duration|Severity|Civil_Twilight|Sunrise_Sunset|State|City_Cleaned|Wind_Chill(F)|Street_Cleaned|\n",
      "+--------------+-----------+------------+--------------+--------+--------------+---------------+-----------------+-----------------+--------------+--------+--------+--------+--------------+--------------+-----+------------+-------------+--------------+\n",
      "|          36.9|       91.0|       29.68|          10.0|   false|         false|           NULL|             0.02|       Light Rain|          Calm|   false|   314.0|       3|         Night|         Night|   OH|      Dayton|         NULL|        I-70 E|\n",
      "|          37.9|      100.0|       29.65|          10.0|   false|         false|           NULL|              0.0|       Light Rain|          Calm|   false|    30.0|       2|         Night|         Night|   OH|       Other|         NULL|         Other|\n",
      "|          36.0|      100.0|       29.67|          10.0|   false|          true|            3.5|             NULL|         Overcast|            SW|   false|    30.0|       2|         Night|         Night|   OH|       Other|         33.3|         Other|\n",
      "|          35.1|       96.0|       29.64|           9.0|   false|         false|            4.6|             NULL|    Mostly Cloudy|            SW|   false|    30.0|       3|           Day|         Night|   OH|      Dayton|         31.0|        I-75 S|\n",
      "|          36.0|       89.0|       29.65|           6.0|   false|          true|            3.5|             NULL|    Mostly Cloudy|            SW|   false|    30.0|       2|           Day|           Day|   OH|      Dayton|         33.3|         Other|\n",
      "|          37.9|       97.0|       29.63|           7.0|   false|         false|            3.5|             0.03|       Light Rain|           SSW|   false|    30.0|       3|           Day|           Day|   OH|       Other|         35.5|         Other|\n",
      "|          34.0|      100.0|       29.66|           7.0|   false|         false|            3.5|             NULL|         Overcast|           WSW|   false|    30.0|       2|           Day|           Day|   OH|      Dayton|         31.0|         Other|\n",
      "|          34.0|      100.0|       29.66|           7.0|   false|         false|            3.5|             NULL|         Overcast|           WSW|   false|    30.0|       3|           Day|           Day|   OH|      Dayton|         31.0|         Other|\n",
      "|          33.3|       99.0|       29.67|           5.0|   false|         false|            1.2|             NULL|    Mostly Cloudy|            SW|   false|    30.0|       2|           Day|           Day|   OH|      Dayton|         NULL|         Other|\n",
      "|          37.4|      100.0|       29.62|           3.0|   false|         false|            4.6|             0.02|       Light Rain|           SSW|   false|    30.0|       3|           Day|           Day|   OH|       Other|         33.8|         Other|\n",
      "|          35.6|       93.0|       29.64|           5.0|    true|         false|            5.8|             NULL|             Rain|           WNW|    true|    30.0|       3|           Day|           Day|   OH|    Columbus|         30.7|         Other|\n",
      "|          37.4|      100.0|       29.62|           3.0|   false|         false|            4.6|             0.02|       Light Rain|           SSW|   false|    30.0|       3|           Day|           Day|   OH|       Other|         33.8|        I-70 E|\n",
      "|          33.8|      100.0|       29.63|           3.0|   false|         false|            2.3|             NULL|         Overcast|            SW|   false|    30.0|       2|           Day|           Day|   OH|      Dayton|         NULL|         Other|\n",
      "|          36.0|       89.0|       29.65|          10.0|   false|          true|            5.8|             NULL|    Mostly Cloudy|            NW|   false|    30.0|       2|           Day|           Day|   OH|      Dayton|         31.1|         Other|\n",
      "|          37.4|      100.0|       29.62|           3.0|   false|          true|            4.6|             0.02|       Light Rain|           SSW|   false|    30.0|       2|           Day|           Day|   OH|    Columbus|         33.8|         Other|\n",
      "|          33.8|      100.0|       29.63|           3.0|   false|         false|            2.3|             NULL|         Overcast|            SW|   false|    30.0|       2|           Day|           Day|   OH|      Dayton|         NULL|         Other|\n",
      "|          35.6|       99.0|       29.65|           7.0|   false|         false|            2.3|             NULL|    Mostly Cloudy|           WSW|   false|    30.0|       2|           Day|           Day|   OH|      Dayton|         NULL|         Other|\n",
      "|          36.0|       89.0|       29.65|          10.0|   false|         false|            5.8|             NULL|    Mostly Cloudy|            NW|   false|    30.0|       2|           Day|           Day|   OH|      Dayton|         31.1|         Other|\n",
      "|          37.4|       93.0|       29.63|          10.0|    true|          true|            6.9|             NULL|         Overcast|           WSW|   false|    30.0|       2|           Day|           Day|   OH|      Dayton|         32.1|         Other|\n",
      "|          36.0|       89.0|       29.65|          10.0|   false|         false|            6.9|             NULL|    Mostly Cloudy|          West|   false|    30.0|       2|           Day|           Day|   OH|      Dayton|         30.3|         Other|\n",
      "+--------------+-----------+------------+--------------+--------+--------------+---------------+-----------------+-----------------+--------------+--------+--------+--------+--------------+--------------+-----+------------+-------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Her sütun için işlemi uygula\n",
    "for col_name in columns_to_clean:\n",
    "    df3 = clean_column(df3, col_name, top_n=top_n)\n",
    "    \n",
    "df_selected = df3.select(*selected_cols)\n",
    "\n",
    "categorical_cols = [\n",
    "    \"Weather_Condition\", \"Wind_Direction\", \"Civil_Twilight\",\n",
    "    \"Sunrise_Sunset\", \"State\", \"City_Cleaned\", \"Street_Cleaned\"\n",
    "]\n",
    "\n",
    "\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=col + \"_Idx\", handleInvalid=\"keep\")\n",
    "    for col in categorical_cols\n",
    "]\n",
    "\n",
    "feature_cols = [\"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\", \"Precipitation(in)\", \"Wind_Chill(F)\", \"Traffic_Signal\",\n",
    "    \"Weather_Condition_Idx\", \"Wind_Direction_Idx\", \"Civil_Twilight_Idx\",\n",
    "    \"Sunrise_Sunset_Idx\", \"State_Idx\", \"City_Cleaned_Idx\", \"Street_Cleaned_Idx\"\n",
    "]\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "\n",
    "# 7) Random Forest sınıflayıcı (parametrelere daha sonra grid ile dokunacağız)\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"Severity\",\n",
    "    featuresCol=\"features\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "df_selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53ad389d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+--------------+--------+--------------+---------------+-----------------+-----------------+--------------+--------+--------+--------+--------------+--------------+-----+------------+-------------+--------------+\n",
      "|Temperature(F)|Humidity(%)|Pressure(in)|Visibility(mi)|Crossing|Traffic_Signal|Wind_Speed(mph)|Precipitation(in)|Weather_Condition|Wind_Direction|Junction|Duration|Severity|Civil_Twilight|Sunrise_Sunset|State|City_Cleaned|Wind_Chill(F)|Street_Cleaned|\n",
      "+--------------+-----------+------------+--------------+--------+--------------+---------------+-----------------+-----------------+--------------+--------+--------+--------+--------------+--------------+-----+------------+-------------+--------------+\n",
      "|          37.9|       97.0|       29.63|           7.0|   false|         false|            3.5|             0.03|       Light Rain|           SSW|   false|    30.0|       3|           Day|           Day|   OH|       Other|         35.5|         Other|\n",
      "|          37.4|      100.0|       29.62|           3.0|   false|         false|            4.6|             0.02|       Light Rain|           SSW|   false|    30.0|       3|           Day|           Day|   OH|       Other|         33.8|         Other|\n",
      "|          37.4|      100.0|       29.62|           3.0|   false|         false|            4.6|             0.02|       Light Rain|           SSW|   false|    30.0|       3|           Day|           Day|   OH|       Other|         33.8|        I-70 E|\n",
      "|          37.4|      100.0|       29.62|           3.0|   false|          true|            4.6|             0.02|       Light Rain|           SSW|   false|    30.0|       2|           Day|           Day|   OH|    Columbus|         33.8|         Other|\n",
      "|          33.8|      100.0|       29.62|           2.0|   false|         false|            4.6|             0.01|       Light Snow|           NNW|   false|    30.0|       2|           Day|           Day|   OH|    Columbus|         29.6|         Other|\n",
      "|          35.1|       89.0|       29.65|           6.0|   false|          true|            8.1|             0.02|         Overcast|           WSW|   false|    30.0|       2|           Day|           Day|   OH|       Other|         28.6|         Other|\n",
      "|          37.0|       96.0|       29.63|           8.0|   false|         false|            5.8|              0.0|         Overcast|          West|   false|    45.0|       3|           Day|           Day|   OH|    Columbus|         32.4|         Other|\n",
      "|          37.9|      100.0|        29.6|           5.0|   false|         false|            4.6|             0.01|       Light Snow|          West|   false|    30.0|       2|           Day|           Day|   OH|       Other|         34.4|         Other|\n",
      "|          36.0|       86.0|       29.63|           7.0|   false|          true|            9.2|              0.0|       Light Snow|          West|   false|    45.0|       2|           Day|           Day|   OH|      Dayton|         29.0|         Other|\n",
      "|          39.9|       70.0|       29.61|          10.0|   false|          true|           11.5|              0.0|    Mostly Cloudy|           WNW|   false|    45.0|       2|           Day|           Day|   OH|      Dayton|         32.9|         Other|\n",
      "|          37.0|       89.0|       29.61|          10.0|   false|          true|            6.9|              0.0|         Overcast|          West|   false|    30.0|       2|           Day|           Day|   OH|       Other|         31.6|         Other|\n",
      "|          34.2|       97.0|       29.66|          10.0|   false|         false|            3.5|             0.02|       Light Rain|            NW|   false|    45.0|       2|           Day|           Day|   OH|      Dayton|         31.2|         Other|\n",
      "|          33.1|       92.0|       29.63|           7.0|   false|         false|            8.1|              0.0|       Light Snow|           WNW|   false|    45.0|       2|         Night|         Night|   OH|    Columbus|         26.1|         Other|\n",
      "|          33.1|       92.0|       29.63|           9.0|   false|          true|            9.2|              0.0|         Overcast|           WNW|   false|   137.0|       2|         Night|         Night|   OH|    Columbus|         25.5|         Other|\n",
      "|          22.8|       89.0|       29.69|           4.0|   false|         false|           11.5|              0.0|       Light Snow|            SW|   false|    30.0|       2|         Night|         Night|   OH|      Dayton|         11.5|         Other|\n",
      "|          26.6|       86.0|       29.62|           1.5|   false|         false|            8.1|             0.01|       Light Snow|          West|   false|    30.0|       3|         Night|         Night|   OH|    Columbus|         18.2|         Other|\n",
      "|          21.0|       85.0|       29.67|           0.8|    true|         false|            9.2|             0.01|       Light Snow|            SW|   false|    30.0|       2|         Night|         Night|   OH|       Other|         10.6|         Other|\n",
      "|          23.0|       86.0|       29.71|           3.0|    true|          true|           10.4|              0.0|       Light Snow|            SW|   false|    30.0|       2|         Night|         Night|   OH|      Dayton|         12.4|         Other|\n",
      "|          19.9|       89.0|       29.75|           1.8|   false|         false|           13.8|              0.0|       Light Snow|           WSW|   false|    30.0|       2|         Night|         Night|   OH|       Other|          6.7|         Other|\n",
      "|          25.0|       88.0|       29.65|           1.0|   false|         false|            9.2|             0.02|       Light Snow|            SW|    true|    30.0|       3|         Night|         Night|   OH|    Columbus|         15.5|         Other|\n",
      "+--------------+-----------+------------+--------------+--------+--------------+---------------+-----------------+-----------------+--------------+--------+--------+--------+--------------+--------------+-----+------------+-------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9. NA temizliği ve veri bölme\n",
    "df_no_na = df_selected.dropna().cache()\n",
    "train, test = df_no_na.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "df_no_na.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c4cd84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Pipeline\n",
    "pipeline = Pipeline(stages=indexers + [assembler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72c60d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Hiperparametre ızgarası\n",
    "paramGrid = (ParamGridBuilder()\n",
    "    .addGrid(rf.numTrees, [20, 50, 100])\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15])\n",
    "    .addGrid(rf.maxBins,  [216, 256])\n",
    "    .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3568deaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Evaluator'ı tanımla\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Severity\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"    # ya da \"accuracy\", \"weightedPrecision\", vs.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88dc8160",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    parallelism=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19a21009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aslay\\bert_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\aslay\\AppData\\Local\\Temp\\ipykernel_15612\\3014251591.py\", line 4, in <module>\n",
      "    cvModel = cv.fit(train)\n",
      "              ^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\pyspark\\ml\\base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\pyspark\\ml\\tuning.py\", line 847, in _fit\n",
      "    for j, metric, subModel in pool.imap_unordered(lambda f: f(), tasks):\n",
      "  File \"C:\\Users\\aslay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\pool.py\", line 873, in next\n",
      "    raise value\n",
      "  File \"C:\\Users\\aslay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\pyspark\\ml\\tuning.py\", line 847, in <lambda>\n",
      "    for j, metric, subModel in pool.imap_unordered(lambda f: f(), tasks):\n",
      "                                                             ^^^\n",
      "  File \"C:\\spark\\python\\pyspark\\util.py\", line 342, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\pyspark\\ml\\tuning.py\", line 113, in singleTask\n",
      "    index, model = next(modelIter)\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\pyspark\\ml\\base.py\", line 98, in __next__\n",
      "    return index, self.fitSingleModel(index)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\pyspark\\ml\\base.py\", line 156, in fitSingleModel\n",
      "    return estimator.fit(dataset, paramMaps[index])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\pyspark\\ml\\base.py\", line 203, in fit\n",
      "    return self.copy(params)._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\pyspark\\ml\\pipeline.py\", line 134, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\pyspark\\ml\\base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\pyspark\\ml\\wrapper.py\", line 381, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\pyspark\\ml\\wrapper.py\", line 378, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\pyspark\\errors\\exceptions\\captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aslay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] Varolan bir bağlantı uzaktaki bir ana bilgisayar tarafından zorla kapatıldı\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aslay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] Varolan bir bağlantı uzaktaki bir ana bilgisayar tarafından zorla kapatıldı\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aslay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] Varolan bir bağlantı uzaktaki bir ana bilgisayar tarafından zorla kapatıldı\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] Hedef makine etkin olarak reddettiğinden bağlantı kurulamadı",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m train, test \u001b[38;5;241m=\u001b[39m df_selected\u001b[38;5;241m.\u001b[39mrandomSplit([\u001b[38;5;241m0.8\u001b[39m,\u001b[38;5;241m0.2\u001b[39m], seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m cvModel \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[0;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[0;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[0;32m    846\u001b[0m )\n\u001b[1;32m--> 847\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubModel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap_unordered\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics_all\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[1;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[1;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit.<locals>.<lambda>\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[0;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[0;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[0;32m    846\u001b[0m )\n\u001b[1;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, tasks):\n\u001b[0;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\util.py:342\u001b[0m, in \u001b[0;36minheritable_thread_target.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39msetLocalProperties(properties)\n\u001b[1;32m--> 342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\tuning.py:113\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[1;34m()\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingleTask\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, Transformer]:\n\u001b[1;32m--> 113\u001b[0m     index, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodelIter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\base.py:98\u001b[0m, in \u001b[0;36m_FitMultipleIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfitSingleModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\base.py:156\u001b[0m, in \u001b[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[1;34m(index)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfitSingleModel\u001b[39m(index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m M:\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparamMaps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(10061, 'Hedef makine etkin olarak reddettiğinden bağlantı kurulamadı', None, 10061, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2179\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2176\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[0;32m   2177\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2179\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[0;32m   2181\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[0;32m   2182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\ipykernel\\zmqshell.py:559\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[1;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[0;32m    553\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    554\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    556\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    560\u001b[0m }\n\u001b[0;32m    562\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[0;32m    563\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[1;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[1;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] Hedef makine etkin olarak reddettiğinden bağlantı kurulamadı"
     ]
    }
   ],
   "source": [
    "# 11) Veri böl (önce eğitim, sonra CV.fit)\n",
    "train, test = df_selected.randomSplit([0.8,0.2], seed=42)\n",
    "\n",
    "cvModel = cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e88eb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12) En iyi parametreler\n",
    "bestRF = cvModel.bestModel.stages[-1]\n",
    "print(\"=== En İyi RF Parametreleri ===\")\n",
    "print(f\" numTrees = {bestRF.getNumTrees}\")\n",
    "print(f\" maxDepth = {bestRF.getOrDefault('maxDepth')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0197fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save(\"models/us_accidents_severity_rf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e96fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8523\n",
      "Weighted Precision: 0.8104\n",
      "Weighted Recall: 0.8523\n",
      "F1 Score: 0.7844\n"
     ]
    }
   ],
   "source": [
    "# 11. Tahmin üret\n",
    "preds = cvModel.transform(test)\n",
    "df_no_na.unpersist()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Severity\", predictionCol=\"prediction\")\n",
    "\n",
    "accuracy = evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n",
    "precision = evaluator.setMetricName(\"weightedPrecision\").evaluate(predictions)\n",
    "recall = evaluator.setMetricName(\"weightedRecall\").evaluate(predictions)\n",
    "f1 = evaluator.setMetricName(\"f1\").evaluate(predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Weighted Precision: {precision:.4f}\")\n",
    "print(f\"Weighted Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ae6eaa2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] Hedef makine etkin olarak reddettiğinden bağlantı kurulamadı",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_no_na\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprintSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Display the schema of the cleaned DataFrame\u001b[39;00m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\dataframe.py:625\u001b[0m, in \u001b[0;36mDataFrame.printSchema\u001b[1;34m(self, level)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mschema()\u001b[38;5;241m.\u001b[39mtreeString(level))\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 625\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtreeString())\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[1;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] Hedef makine etkin olarak reddettiğinden bağlantı kurulamadı"
     ]
    }
   ],
   "source": [
    "df_no_na.printSchema()  # Display the schema of the cleaned DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f02921ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, unix_timestamp, when\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# 1) SparkSession oluştur\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"US Accidents Severity CV\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2) CSV'i oku ve gereksiz sütunları at\n",
    "df = spark.read.csv(\"US_Accidents_March23.csv\", header=True, inferSchema=True)\n",
    "df = df.drop(\n",
    "    \"ID\",\"Source\",\"Zipcode\",\"Timezone\",\"Airport_Code\",\"Amenity\",\n",
    "    \"Bump\",\"Give_Way\",\"No_Exit\",\"Railway\",\"Description\",\"County\",\n",
    "    \"Roundabout\",\"Station\",\"Stop\",\"Nautical_Twilight\",\n",
    "    \"Astronomical_Twilight\",\"Country\"\n",
    ")\n",
    "\n",
    "# 3) Başlangıç / Bitiş zamanlarını timestamp'e çevir, Duration (dakika) hesapla\n",
    "df = (\n",
    "    df.withColumn(\"Start_TS\", to_timestamp(col(\"Start_Time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "      .withColumn(\"End_TS\",   to_timestamp(col(\"End_Time\"),   \"yyyy-MM-dd HH:mm:ss\"))\n",
    "      .withColumn(\n",
    "          \"Duration\",\n",
    "          ((unix_timestamp(col(\"End_TS\")) - unix_timestamp(col(\"Start_TS\"))) / 60)\n",
    "          .cast(DoubleType())\n",
    "      )\n",
    "      .drop(\"Start_TS\",\"End_TS\",\"Start_Time\",\"End_Time\")\n",
    ")\n",
    "\n",
    "# 4) City/Street için cardinality'yi top 32 ile sınırlayan fonksiyon\n",
    "def clean_column(df, column_name, top_n=32):\n",
    "    top_vals = [\n",
    "        r[column_name] for r in\n",
    "        df.groupBy(column_name).count()\n",
    "          .orderBy(col(\"count\").desc())\n",
    "          .limit(top_n).collect()\n",
    "    ]\n",
    "    return df.withColumn(\n",
    "        f\"{column_name}_Cleaned\",\n",
    "        when(col(column_name).isin(top_vals), col(column_name)).otherwise(\"Other\")\n",
    "    )\n",
    "\n",
    "for c in [\"City\", \"Street\"]:\n",
    "    df = clean_column(df, c, top_n=32)\n",
    "\n",
    "# 5) Modelde kullanacağımız sütunları tanımla\n",
    "selected_cols = [\n",
    "    \"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\",\n",
    "    \"Crossing\", \"Traffic_Signal\", \"Wind_Speed(mph)\", \"Precipitation(in)\",\n",
    "    \"Weather_Condition\", \"Wind_Direction\", \"Junction\", \"Duration\", \"Severity\",\n",
    "    \"Civil_Twilight\", \"Sunrise_Sunset\", \"State\", \"City_Cleaned\",\n",
    "    \"Wind_Chill(F)\", \"Street_Cleaned\"\n",
    "]\n",
    "\n",
    "# 6) Kategorik sütunlar ve VectorAssembler için feature listesi\n",
    "categorical_cols = [\n",
    "    \"Weather_Condition\", \"Wind_Direction\",\n",
    "    \"Civil_Twilight\",     \"Sunrise_Sunset\",\n",
    "    \"State\",              \"City_Cleaned\",\n",
    "    \"Street_Cleaned\"\n",
    "]\n",
    "\n",
    "feature_cols = [\n",
    "    \"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\", \"Precipitation(in)\", \"Wind_Chill(F)\", \"Traffic_Signal\",\n",
    "    \"Weather_Condition_Idx\", \"Wind_Direction_Idx\",\n",
    "    \"Civil_Twilight_Idx\",     \"Sunrise_Sunset_Idx\",\n",
    "    \"State_Idx\",              \"City_Cleaned_Idx\",\n",
    "    \"Street_Cleaned_Idx\"\n",
    "]\n",
    "\n",
    "# 7) Son olarak seç, NA'ları at ve cachele\n",
    "df_selected = df.select(*selected_cols)\n",
    "df_no_na   = df_selected.dropna().cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0596a41",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o5138.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 251.0 failed 1 times, most recent failure: Lost task 12.0 in stage 251.0 (TID 3612) (MSI executor driver): java.lang.OutOfMemoryError: Java heap space\r\n\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$6451/694568505.apply(Unknown Source)\r\n\tat scala.Array$.tabulate(Array.scala:418)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$6438/1019762470.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$2859/626766273.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2605/184492444.apply(Unknown Source)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.lang.OutOfMemoryError: Java heap space\r\n\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$6451/694568505.apply(Unknown Source)\r\n\tat scala.Array$.tabulate(Array.scala:418)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$6438/1019762470.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$2859/626766273.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2605/184492444.apply(Unknown Source)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# 9) Eğitim/Test böl ve CV'yi çalıştır\u001b[39;00m\n\u001b[0;32m     49\u001b[0m train, test \u001b[38;5;241m=\u001b[39m df_no_na\u001b[38;5;241m.\u001b[39mrandomSplit([\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.2\u001b[39m], seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m cvModel \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🏆 En iyi CV F1 skoru:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mmax\u001b[39m(cvModel\u001b[38;5;241m.\u001b[39mavgMetrics))\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# 10) Test kümesinde değerlendir\u001b[39;00m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    841\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[0;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[0;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[0;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[0;32m    846\u001b[0m )\n\u001b[1;32m--> 847\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubModel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap_unordered\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics_all\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcollectSubModelsParam\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[1;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[1;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[0;32m    123\u001b[0m job, i, func, args, kwds \u001b[38;5;241m=\u001b[39m task\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit.<locals>.<lambda>\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    841\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[0;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[0;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[0;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[0;32m    846\u001b[0m )\n\u001b[1;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, tasks):\n\u001b[0;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\util.py:342\u001b[0m, in \u001b[0;36minheritable_thread_target.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    341\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39msetLocalProperties(properties)\n\u001b[1;32m--> 342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\tuning.py:113\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[1;34m()\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingleTask\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, Transformer]:\n\u001b[1;32m--> 113\u001b[0m     index, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodelIter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     metric \u001b[38;5;241m=\u001b[39m eva\u001b[38;5;241m.\u001b[39mevaluate(model\u001b[38;5;241m.\u001b[39mtransform(validation, epm[index]))\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\base.py:98\u001b[0m, in \u001b[0;36m_FitMultipleIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo models remaining.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfitSingleModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\base.py:156\u001b[0m, in \u001b[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[1;34m(index)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfitSingleModel\u001b[39m(index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m M:\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparamMaps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[1;32m--> 203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o5138.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 251.0 failed 1 times, most recent failure: Lost task 12.0 in stage 251.0 (TID 3612) (MSI executor driver): java.lang.OutOfMemoryError: Java heap space\r\n\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$6451/694568505.apply(Unknown Source)\r\n\tat scala.Array$.tabulate(Array.scala:418)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$6438/1019762470.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$2859/626766273.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2605/184492444.apply(Unknown Source)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.lang.OutOfMemoryError: Java heap space\r\n\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$6451/694568505.apply(Unknown Source)\r\n\tat scala.Array$.tabulate(Array.scala:418)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$6438/1019762470.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$2859/626766273.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2605/184492444.apply(Unknown Source)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aslay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] Varolan bir bağlantı uzaktaki bir ana bilgisayar tarafından zorla kapatıldı\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aslay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] Varolan bir bağlantı uzaktaki bir ana bilgisayar tarafından zorla kapatıldı\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 8) Pipeline + RF + Hiperparametre Izgarası\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=c + \"_Idx\", handleInvalid=\"keep\")\n",
    "    for c in categorical_cols\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"Severity\",\n",
    "    featuresCol=\"features\"\n",
    ")\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "    .addGrid(rf.numTrees, [20, 50])\n",
    "    .addGrid(rf.maxDepth, [5, 10])\n",
    "    .addGrid(rf.maxBins, [256])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Severity\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + [assembler, rf])\n",
    "\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator          = pipeline,\n",
    "    estimatorParamMaps = paramGrid,\n",
    "    evaluator          = evaluator,\n",
    "    trainRatio         = 0.8,\n",
    "    parallelism        = 1\n",
    ")\n",
    "\n",
    "# 9) Eğitim/Test böl ve CV'yi çalıştır\n",
    "train, test = df_no_na.randomSplit([0.8, 0.2], seed=42)\n",
    "tvsModel = tvs.fit(train)\n",
    "bestModel = tvsModel.bestModel\n",
    "\n",
    "print(\"🏆 En iyi CV F1 skoru:\", max(cvModel.avgMetrics))\n",
    "\n",
    "# 10) Test kümesinde değerlendir\n",
    "preds = cvModel.transform(test)\n",
    "test_f1 = evaluator.evaluate(preds)\n",
    "test_acc = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Severity\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ").evaluate(preds)\n",
    "\n",
    "print(f\"Test F1 Skoru:    {test_f1:.4f}\")\n",
    "print(f\"Test Doğruluk:    {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfbe93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# 11) Eğitilmiş Modeli Kaydet\n",
    "# En iyi pipeline modeli (PipelineModel) cvModel.bestModel\n",
    "best_pipeline_model = cvModel.bestModel\n",
    "\n",
    "# Modeli disk'e yaz\n",
    "best_pipeline_model.write() \\\n",
    "    .overwrite() \\\n",
    "    .save(\"models/us_accidents_severity_rf_cv_best\")\n",
    "\n",
    "print(\"✅ Model başarıyla kaydedildi: models/us_accidents_severity_rf_cv_best\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036ab0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c9ee440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 1) SparkSession’ı daha yüksek bellek konfigürasyonuyla aç\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"US Accidents Severity CV - Test\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e89f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 2) Veri Hazırlığı\n",
    "\n",
    "from pyspark.sql.functions import col, to_timestamp, unix_timestamp, when\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# CSV’yi oku\n",
    "df = spark.read.csv(\"US_Accidents_March23.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Gereksiz sütunları at\n",
    "df = df.drop(\n",
    "    \"ID\",\"Source\",\"Zipcode\",\"Timezone\",\"Airport_Code\",\"Amenity\",\n",
    "    \"Bump\",\"Give_Way\",\"No_Exit\",\"Railway\",\"Description\",\"County\",\n",
    "    \"Roundabout\",\"Station\",\"Stop\",\"Nautical_Twilight\",\"Astronomical_Twilight\",\"Country\"\n",
    ")\n",
    "\n",
    "# Timestamp → Duration (dakika)\n",
    "df = df.withColumn(\"Start_TS\",  to_timestamp(col(\"Start_Time\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "       .withColumn(\"End_TS\",    to_timestamp(col(\"End_Time\"),   \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "       .withColumn(\"Duration\", ((unix_timestamp(col(\"End_TS\")) - unix_timestamp(col(\"Start_TS\"))) / 60).cast(DoubleType())) \\\n",
    "       .drop(\"Start_TS\",\"End_TS\",\"Start_Time\",\"End_Time\")\n",
    "\n",
    "# Cardinality’si yüksek City/Street’i top_n=32 dışındakileri \"Other\" yap\n",
    "def clean_column(df, column_name, top_n=32):\n",
    "    top_vals = [r[column_name] for r in\n",
    "                df.groupBy(column_name).count()\n",
    "                  .orderBy(col(\"count\").desc())\n",
    "                  .limit(top_n).collect()]\n",
    "    return df.withColumn(\n",
    "        f\"{column_name}_Cleaned\",\n",
    "        when(col(column_name).isin(top_vals), col(column_name)).otherwise(\"Other\")\n",
    "    )\n",
    "\n",
    "for c in [\"City\",\"Street\"]:\n",
    "    df = clean_column(df, c, top_n=32)\n",
    "\n",
    "# Kullanacağımız sütunlar\n",
    "selected_cols = [\n",
    "    \"Temperature(F)\",\"Humidity(%)\",\"Pressure(in)\",\"Visibility(mi)\",\n",
    "    \"Crossing\",\"Traffic_Signal\",\"Wind_Speed(mph)\",\"Precipitation(in)\",\n",
    "    \"Weather_Condition\",\"Wind_Direction\",\"Junction\",\"Duration\",\"Severity\",\n",
    "    \"Civil_Twilight\",\"Sunrise_Sunset\",\"State\",\"City_Cleaned\",\n",
    "    \"Wind_Chill(F)\",\"Street_Cleaned\"\n",
    "]\n",
    "df_selected = df.select(*selected_cols)\n",
    "\n",
    "# NA’ları at ve cache\n",
    "df_no_na = df_selected.dropna().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fd22ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 3) Pipeline + RF + ParamGrid + TrainValidationSplit\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Kategorikleri indexle\n",
    "categorical_cols = [\n",
    "    \"Weather_Condition\",\"Wind_Direction\",\"Civil_Twilight\",\n",
    "    \"Sunrise_Sunset\",\"State\",\"City_Cleaned\",\"Street_Cleaned\"\n",
    "]\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=c+\"_Idx\", handleInvalid=\"keep\")\n",
    "    for c in categorical_cols\n",
    "]\n",
    "\n",
    "# Assembler\n",
    "feature_cols = [\n",
    "    \"Temperature(F)\",\"Humidity(%)\",\"Pressure(in)\",\"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\",\"Precipitation(in)\",\"Wind_Chill(F)\",\"Traffic_Signal\",\n",
    "    \"Weather_Condition_Idx\",\"Wind_Direction_Idx\",\"Civil_Twilight_Idx\",\n",
    "    \"Sunrise_Sunset_Idx\",\"State_Idx\",\"City_Cleaned_Idx\",\"Street_Cleaned_Idx\"\n",
    "]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "\n",
    "# RandomForest (parametreler TrainValidationSplit’ten gelecek)\n",
    "rf = RandomForestClassifier(labelCol=\"Severity\", featuresCol=\"features\")\n",
    "\n",
    "# Daraltılmış parametre ızgarası\n",
    "paramGrid = (ParamGridBuilder()\n",
    "    .addGrid(rf.numTrees, [20, 50])      # 100 kaldırıldı\n",
    "    .addGrid(rf.maxDepth, [5, 10])       # 15 kaldırıldı\n",
    "    .addGrid(rf.maxBins, [256])          # 512 kaldırıldı\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# F1 metriğiyle evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Severity\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=indexers + [assembler, rf])\n",
    "\n",
    "# TrainValidationSplit (paralellik=1, Windows'ta güvenli)\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator          = pipeline,\n",
    "    estimatorParamMaps = paramGrid,\n",
    "    evaluator          = evaluator,\n",
    "    trainRatio         = 0.8,\n",
    "    parallelism        = 1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e1cf254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔀 4.1) Veriyi train/test olarak bölüyoruz…\n",
      "    → train: 4175117 satır, test: 1042802 satır\n",
      "🔄 4.2) Train’den %20 örnek alınıyor (küçük eğitim seti)…\n",
      "    → small_train: 835471 satır\n",
      "⚙️ 4.3) TrainValidationSplit ile model eğitimi başlıyor…\n",
      "    ✅ Eğtirim tamamlandı\n",
      "    🔍 En iyi parametreler: numTrees=50, maxDepth=10, maxBins=256\n",
      "    🏆 En iyi TVS F1 skoru (küçük veri): 0.7914\n",
      "📊 4.4) Test kümesinde değerlendirme yapılıyor…\n",
      "    Test F1    : 0.7919\n",
      "    Test Acc   : 0.8544\n",
      "💾 4.5) Modeli kaydediyoruz…\n",
      "    ✅ Model kaydedildi → models/us_accidents_rf_tvs_small\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 4) Küçük veri örneğiyle fit & değerlendir & kaydet (ilerlemeyi takip edelim)\n",
    "\n",
    "# Spark loglarını INFO seviyesinde göster\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "# 4.1) Split\n",
    "print(\"🔀 4.1) Veriyi train/test olarak bölüyoruz…\")\n",
    "train, test = df_no_na.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"    → train: {train.count()} satır, test: {test.count()} satır\")\n",
    "\n",
    "# 4.2) Sadece %20 örnek al\n",
    "print(\"🔄 4.2) Train’den %20 örnek alınıyor (küçük eğitim seti)…\")\n",
    "small_train = train.sample(fraction=0.2, seed=42)\n",
    "print(f\"    → small_train: {small_train.count()} satır\")\n",
    "\n",
    "# 4.3) Fit\n",
    "print(\"⚙️ 4.3) TrainValidationSplit ile model eğitimi başlıyor…\")\n",
    "tvsModel = tvs.fit(small_train)\n",
    "print(\"    ✅ Eğtirim tamamlandı\")\n",
    "\n",
    "bestModel = tvsModel.bestModel\n",
    "\n",
    "# Hangi parametreler en iyiye denk düştü?\n",
    "bestParams = tvsModel.getEstimatorParamMaps()[tvsModel.validationMetrics.index(max(tvsModel.validationMetrics))]\n",
    "print(f\"    🔍 En iyi parametreler: numTrees={bestParams[rf.numTrees]}, \"\n",
    "      f\"maxDepth={bestParams[rf.maxDepth]}, maxBins={bestParams[rf.maxBins]}\")\n",
    "print(f\"    🏆 En iyi TVS F1 skoru (küçük veri): {max(tvsModel.validationMetrics):.4f}\")\n",
    "\n",
    "# 4.4) Test kümesinde değerlendir\n",
    "print(\"📊 4.4) Test kümesinde değerlendirme yapılıyor…\")\n",
    "preds = bestModel.transform(test)\n",
    "f1_test = evaluator.evaluate(preds)\n",
    "acc_test = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Severity\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ").evaluate(preds)\n",
    "\n",
    "print(f\"    Test F1    : {f1_test:.4f}\")\n",
    "print(f\"    Test Acc   : {acc_test:.4f}\")\n",
    "\n",
    "# 4.5) Modeli kaydet\n",
    "print(\"💾 4.5) Modeli kaydediyoruz…\")\n",
    "bestModel.write().overwrite().save(\"models/us_accidents_rf_tvs_small\")\n",
    "print(\"    ✅ Model kaydedildi → models/us_accidents_rf_tvs_small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37eeb39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f02ec632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☑️ Aşama A: Özelliklerin üretilmesi (index + assemble)…\n",
      "   → Toplam örnek: 5217919 satır\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ——————————————————————————————————————\n",
    "# A) Özellik-öncesi pipeline’ı çalıştır ve cache’le\n",
    "print(\"☑️ Aşama A: Özelliklerin üretilmesi (index + assemble)…\")\n",
    "pipeline_features = Pipeline(stages=indexers + [assembler])\n",
    "df_feats = pipeline_features.fit(df_no_na).transform(df_no_na) \\\n",
    "    .select(\"features\", \"Severity\") \\\n",
    "    .cache()\n",
    "# materialize cache\n",
    "print(f\"   → Toplam örnek: {df_feats.count()} satır\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d195b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☑️ Aşama B: RF final model eğitimi başlıyor…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aslay\\bert_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\aslay\\AppData\\Local\\Temp\\ipykernel_28620\\3752981426.py\", line 9, in <module>\n",
      "    model_full = rf_final.fit(df_feats)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\pyspark\\ml\\base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\pyspark\\ml\\wrapper.py\", line 381, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\pyspark\\ml\\wrapper.py\", line 378, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\pyspark\\errors\\exceptions\\captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aslay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] Varolan bir bağlantı uzaktaki bir ana bilgisayar tarafından zorla kapatıldı\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] Hedef makine etkin olarak reddettiğinden bağlantı kurulamadı",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      2\u001b[0m rf_final \u001b[38;5;241m=\u001b[39m RandomForestClassifier(\n\u001b[0;32m      3\u001b[0m     labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeverity\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     maxBins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m      8\u001b[0m )\n\u001b[1;32m----> 9\u001b[0m model_full \u001b[38;5;241m=\u001b[39m \u001b[43mrf_final\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_feats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   ✅ Final model eğitimi tamamlandı\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(10061, 'Hedef makine etkin olarak reddettiğinden bağlantı kurulamadı', None, 10061, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2179\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2176\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[0;32m   2177\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2179\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[0;32m   2181\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[0;32m   2182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\ipykernel\\zmqshell.py:559\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[1;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[0;32m    553\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    554\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    556\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    560\u001b[0m }\n\u001b[0;32m    562\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[0;32m    563\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[1;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[1;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] Hedef makine etkin olarak reddettiğinden bağlantı kurulamadı"
     ]
    }
   ],
   "source": [
    "print(\"☑️ Aşama B: RF final model eğitimi başlıyor…\")\n",
    "rf_final = RandomForestClassifier(\n",
    "    labelCol=\"Severity\",\n",
    "    featuresCol=\"features\",\n",
    "    numTrees=50,\n",
    "    maxDepth=10,\n",
    "    maxBins=256\n",
    ")\n",
    "model_full = rf_final.fit(df_feats)\n",
    "print(\"   ✅ Final model eğitimi tamamlandı\\n\")\n",
    "\n",
    "# ——————————————————————————————————————\n",
    "# C) Modeli kaydet\n",
    "print(\"💾 Final modeli kaydediyoruz…\")\n",
    "model_full.write().overwrite().save(\"models/us_accidents_rf_final\")\n",
    "print(\"   ✅ Model kaydedildi → models/us_accidents_rf_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b3cb75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c42e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, unix_timestamp, when\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression,\n",
    "    DecisionTreeClassifier,\n",
    "    RandomForestClassifier,\n",
    "    GBTClassifier\n",
    ")\n",
    "# If you have XGBoost4J‑Spark installed, uncomment:\n",
    "from sparkxgb import XGBoostClassifier\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator\n",
    "\n",
    "# SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"US Accidents Severity – Full Pipeline\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbf856bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ df_no_na ready: 5,217,919 rows, 18 columns\n"
     ]
    }
   ],
   "source": [
    "# 2.1) Read CSV & drop unwanted columns\n",
    "df = spark.read.csv(\"US_Accidents_March23.csv\", header=True, inferSchema=True)\n",
    "\n",
    "drop_cols = [\n",
    "    \"ID\",\"Source\",\"Zipcode\",\"Timezone\",\"Airport_Code\",\"Amenity\",\"Bump\",\n",
    "    \"Give_Way\",\"No_Exit\",\"Railway\",\"Description\",\"County\",\"Roundabout\",\n",
    "    \"Station\",\"Stop\",\"Nautical_Twilight\",\"Astronomical_Twilight\",\"Country\"\n",
    "]\n",
    "df = df.drop(*drop_cols)\n",
    "\n",
    "# 2.2) Compute Duration (in minutes) from Start_Time / End_Time\n",
    "df = (\n",
    "    df\n",
    "    .withColumn(\"Start_TS\", to_timestamp(col(\"Start_Time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .withColumn(\"End_TS\",   to_timestamp(col(\"End_Time\"),   \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .withColumn(\"Duration\",\n",
    "        ((unix_timestamp(col(\"End_TS\")) - unix_timestamp(col(\"Start_TS\"))) / 60)\n",
    "        .cast(DoubleType())\n",
    "    )\n",
    "    .drop(\"Start_TS\",\"End_TS\",\"Start_Time\",\"End_Time\")\n",
    ")\n",
    "\n",
    "# 2.3) Reduce cardinality of City & Street (keep top 32, rest → \"Other\")\n",
    "def clean_column(df, column_name, top_n=32):\n",
    "    top_vals = [\n",
    "        r[column_name] for r in\n",
    "        df.groupBy(column_name).count()\n",
    "          .orderBy(col(\"count\").desc())\n",
    "          .limit(top_n)\n",
    "          .collect()\n",
    "    ]\n",
    "    return df.withColumn(\n",
    "        f\"{column_name}_Cleaned\",\n",
    "        when(col(column_name).isin(top_vals), col(column_name)).otherwise(\"Other\")\n",
    "    )\n",
    "\n",
    "for c in [\"City\",\"Street\"]:\n",
    "    df = clean_column(df, c, top_n=32)\n",
    "\n",
    "# 2.4) Select only the features we want\n",
    "selected_cols = [\n",
    "    \"Temperature(F)\",\"Humidity(%)\",\"Pressure(in)\",\"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\",\"Precipitation(in)\",\"Wind_Chill(F)\",\"Traffic_Signal\",\n",
    "    \"Weather_Condition\",\"Wind_Direction\",\"Junction\",\"Duration\",\"Severity\",\n",
    "    \"Civil_Twilight\",\"Sunrise_Sunset\",\"State\",\"City_Cleaned\",\"Street_Cleaned\"\n",
    "]\n",
    "df_selected = df.select(*selected_cols)\n",
    "\n",
    "# 2.5) Drop any rows with nulls & cache\n",
    "df_no_na = df_selected.dropna().cache()\n",
    "print(f\"✅ df_no_na ready: {df_no_na.count():,} rows, {len(df_no_na.columns)} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c22c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1) Categorical columns & indexers\n",
    "categorical_cols = [\n",
    "    \"Weather_Condition\",\"Wind_Direction\",\"Civil_Twilight\",\n",
    "    \"Sunrise_Sunset\",\"State\",\"City_Cleaned\",\"Street_Cleaned\"\n",
    "]\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=c+\"_Idx\", handleInvalid=\"keep\")\n",
    "    for c in categorical_cols\n",
    "]\n",
    "\n",
    "# 3.2) Numeric + indexed categorical → feature vector\n",
    "feature_cols = [\n",
    "    \"Temperature(F)\",\"Humidity(%)\",\"Pressure(in)\",\"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\",\"Precipitation(in)\",\"Wind_Chill(F)\",\"Traffic_Signal\"\n",
    "] + [c + \"_Idx\" for c in categorical_cols]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bd5e2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔀 Full split → train: 4,175,117, test: 1,042,802\n",
      "🔄 small_train: 835,471 rows\n",
      "🔄 small_val  : 836,531 rows\n"
     ]
    }
   ],
   "source": [
    "# 4.1) Full train/test split\n",
    "train_full, test_full = df_no_na.randomSplit([0.8,0.2], seed=42)\n",
    "print(f\"🔀 Full split → train: {train_full.count():,}, test: {test_full.count():,}\")\n",
    "\n",
    "# 4.2) From train_full, get small_train & small_val (each ~20% of full)\n",
    "small_train, small_val, _ = train_full.randomSplit([0.2,0.2,0.6], seed=42)\n",
    "print(f\"🔄 small_train: {small_train.count():,} rows\")\n",
    "print(f\"🔄 small_val  : {small_val.count():,} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfd8db14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️  Training & evaluating → LogisticRegression\n",
      "    ▶ small_val  F1 = 0.7842, Acc = 0.8517\n",
      "    💾 Saved → models/us_accidents_logisticregression_small\n",
      "\n",
      "⚙️  Training & evaluating → DecisionTree\n",
      "    ▶ small_val  F1 = 0.7961, Acc = 0.8537\n",
      "    💾 Saved → models/us_accidents_decisiontree_small\n",
      "\n",
      "⚙️  Training & evaluating → RandomForest\n",
      "    ▶ small_val  F1 = 0.7865, Acc = 0.8526\n",
      "    💾 Saved → models/us_accidents_randomforest_small\n",
      "\n",
      "⚙️  Training & evaluating → GBT_OvR\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o4428.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 1675.0 failed 1 times, most recent failure: Lost task 9.0 in stage 1675.0 (TID 27917) (MSI executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:99)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\nCaused by: java.io.EOFException\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\r\n\t... 24 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions$lzycompute(MulticlassMetrics.scala:61)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions(MulticlassMetrics.scala:52)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labelCountByClass$lzycompute(MulticlassMetrics.scala:66)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labelCountByClass(MulticlassMetrics.scala:64)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.weightedFMeasure(MulticlassMetrics.scala:227)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.weightedFMeasure$lzycompute(MulticlassMetrics.scala:235)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.weightedFMeasure(MulticlassMetrics.scala:235)\r\n\tat org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator.evaluate(MulticlassClassificationEvaluator.scala:152)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:99)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\nCaused by: java.io.EOFException\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\r\n\t... 24 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 70\u001b[0m\n\u001b[0;32m     66\u001b[0m model \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mfit(small_train)\n\u001b[0;32m     68\u001b[0m preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(small_val)\n\u001b[0;32m     69\u001b[0m f1, acc \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m---> 70\u001b[0m     \u001b[43mf1_evaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     71\u001b[0m     acc_evaluator\u001b[38;5;241m.\u001b[39mevaluate(preds)\n\u001b[0;32m     72\u001b[0m )\n\u001b[0;32m     74\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/us_accidents_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_small\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m model\u001b[38;5;241m.\u001b[39mwrite()\u001b[38;5;241m.\u001b[39moverwrite()\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\evaluation.py:111\u001b[0m, in \u001b[0;36mEvaluator.evaluate\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_evaluate(dataset)\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\evaluation.py:148\u001b[0m, in \u001b[0;36mJavaEvaluator._evaluate\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o4428.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 1675.0 failed 1 times, most recent failure: Lost task 9.0 in stage 1675.0 (TID 27917) (MSI executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:99)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\nCaused by: java.io.EOFException\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\r\n\t... 24 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions$lzycompute(MulticlassMetrics.scala:61)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions(MulticlassMetrics.scala:52)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labelCountByClass$lzycompute(MulticlassMetrics.scala:66)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labelCountByClass(MulticlassMetrics.scala:64)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.weightedFMeasure(MulticlassMetrics.scala:227)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.weightedFMeasure$lzycompute(MulticlassMetrics.scala:235)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.weightedFMeasure(MulticlassMetrics.scala:235)\r\n\tat org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator.evaluate(MulticlassClassificationEvaluator.scala:152)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:99)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\nCaused by: java.io.EOFException\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\r\n\t... 24 more\r\n"
     ]
    }
   ],
   "source": [
    "# 5.1) Define the classifiers to compare\n",
    "from pyspark.ml.classification import OneVsRest, NaiveBayes\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression,\n",
    "    DecisionTreeClassifier,\n",
    "    RandomForestClassifier,\n",
    "    GBTClassifier,\n",
    "    OneVsRest,\n",
    ")\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 1) Define models, upping maxBins for trees & OvR parallelism\n",
    "models_to_try = {\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        labelCol=\"Severity\", featuresCol=\"features\",\n",
    "        maxIter=20, family=\"multinomial\"\n",
    "    ),\n",
    "\n",
    "    \"DecisionTree\": DecisionTreeClassifier(\n",
    "        labelCol=\"Severity\", featuresCol=\"features\",\n",
    "        maxDepth=10, maxBins=512\n",
    "    ),\n",
    "\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        labelCol=\"Severity\", featuresCol=\"features\",\n",
    "        numTrees=20, maxBins=512\n",
    "    ),\n",
    "\n",
    "    \"GBT_OvR\": OneVsRest(\n",
    "        classifier=GBTClassifier(\n",
    "            labelCol=\"Severity\", featuresCol=\"features\",\n",
    "            maxIter=20, maxBins=512\n",
    "        ),\n",
    "        labelCol=\"Severity\", featuresCol=\"features\"\n",
    "    ),\n",
    "\n",
    "    # (optional) if you have XGBoost4J-Spark\n",
    "    # \"XGBoost_OvR\": OneVsRest(\n",
    "    #     classifier=XGBoostClassifier(\n",
    "    #         labelCol=\"Severity\", featuresCol=\"features\",\n",
    "    #         numRound=50\n",
    "    #     ),\n",
    "    #     labelCol=\"Severity\", featuresCol=\"features\"\n",
    "    # ),\n",
    "\n",
    "    \"NaiveBayes\": NaiveBayes(\n",
    "        labelCol=\"Severity\", featuresCol=\"features\",\n",
    "        smoothing=1.0, modelType=\"multinomial\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "# 2) Evaluators\n",
    "f1_evaluator  = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Severity\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "acc_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Severity\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "# 3) Loop over each model on small_train / small_val\n",
    "results = []\n",
    "for name, clf in models_to_try.items():\n",
    "    print(f\"\\n⚙️  Training & evaluating → {name}\")\n",
    "    pipe  = Pipeline(stages=indexers + [assembler, clf])\n",
    "    model = pipe.fit(small_train)\n",
    "\n",
    "    preds = model.transform(small_val)\n",
    "    f1, acc = (\n",
    "        f1_evaluator.evaluate(preds),\n",
    "        acc_evaluator.evaluate(preds)\n",
    "    )\n",
    "\n",
    "    path = f\"models/us_accidents_{name.lower()}_small\"\n",
    "    model.write().overwrite().save(path)\n",
    "\n",
    "    print(f\"    ▶ small_val  F1 = {f1:.4f}, Acc = {acc:.4f}\")\n",
    "    print(f\"    💾 Saved → {path}\")\n",
    "    results.append((name, f1, acc, path))\n",
    "\n",
    "# 4) Pick best by F1\n",
    "best_name, best_f1, best_acc, best_path = max(results, key=lambda x: x[1])\n",
    "print(f\"\\n🏆 Best small model → {best_name} (F1={best_f1:.4f}, Acc={best_acc:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f445b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2efd4c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hazır veri: 5,217,919 satır, 18 sütun\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# US Accidents Severity Prediction — Model Karşılaştırması\n",
    "# LightGBM, XGBoost, LogisticRegression, DecisionTree, RandomForest, GBT\n",
    "\n",
    "# %% [markdown]\n",
    "# **1) Spark Session & Kütüphaneler**\n",
    "# - findspark ile SparkSession başlatılıyor\n",
    "\n",
    "# %%\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, unix_timestamp, when\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"US Accidents Severity - Model Comparison\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# %% [markdown]\n",
    "# **2) Veri Hazırlığı**\n",
    "# - CSV okuma, gereksiz sütunları atma\n",
    "# - Start/End zamanı → Duration\n",
    "# - City/Street cardinality düşürme (top_n=32)\n",
    "# - Seçilen sütunlar, NA temizleme\n",
    "\n",
    "# %%\n",
    "df = spark.read.csv(\"US_Accidents_March23.csv\", header=True, inferSchema=True)\n",
    "# Gerekli drop'lar\n",
    "drop_cols = [\"ID\",\"Source\",\"Zipcode\",\"Timezone\",\"Airport_Code\",\"Amenity\",\"Bump\",\n",
    "             \"Give_Way\",\"No_Exit\",\"Railway\",\"Description\",\"County\",\"Roundabout\",\n",
    "             \"Station\",\"Stop\",\"Nautical_Twilight\",\"Astronomical_Twilight\",\"Country\"]\n",
    "df2 = df.drop(*drop_cols)\n",
    "# Duration hesaplama\n",
    "df3 = (df2\n",
    "       .withColumn(\"Start_TS\", to_timestamp(col(\"Start_Time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "       .withColumn(\"End_TS\",   to_timestamp(col(\"End_Time\"),   \"yyyy-MM-dd HH:mm:ss\"))\n",
    "       .withColumn(\"Duration\",\n",
    "                   ((unix_timestamp(col(\"End_TS\")) - unix_timestamp(col(\"Start_TS\"))) / 60)\n",
    "                   .cast(DoubleType()))\n",
    "       .drop(\"Start_TS\",\"End_TS\",\"Start_Time\",\"End_Time\")\n",
    ")\n",
    "# Cardinality düşürme\n",
    "\n",
    "def clean_column(df, column_name, top_n=32):\n",
    "    top_vals = [r[column_name] for r in\n",
    "                df.groupBy(column_name).count()\n",
    "                  .orderBy(col(\"count\").desc())\n",
    "                  .limit(top_n).collect()]\n",
    "    return df.withColumn(\n",
    "        f\"{column_name}_Cleaned\",\n",
    "        when(col(column_name).isin(top_vals), col(column_name)).otherwise(\"Other\")\n",
    "    )\n",
    "for c in [\"City\",\"Street\"]:\n",
    "    df3 = clean_column(df3, c, top_n=32)\n",
    "# Seçilen sütunlar\n",
    "selected_cols = [\n",
    "    \"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\", \"Precipitation(in)\", \"Wind_Chill(F)\", \"Traffic_Signal\",\n",
    "    \"Weather_Condition\", \"Wind_Direction\", \"Junction\", \"Duration\", \"Severity\",\n",
    "    \"Civil_Twilight\", \"Sunrise_Sunset\", \"State\", \"City_Cleaned\", \"Street_Cleaned\"\n",
    "]\n",
    "df_selected = df3.select(*selected_cols)\n",
    "# Kategorik ve feature listeleri\n",
    "categorical_cols = [\n",
    "    \"Weather_Condition\",\"Wind_Direction\",\"Civil_Twilight\",\n",
    "    \"Sunrise_Sunset\",\"State\",\"City_Cleaned\",\"Street_Cleaned\"\n",
    "]\n",
    "feature_cols = [\n",
    "    \"Temperature(F)\",\"Humidity(%)\",\"Pressure(in)\",\"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\",\"Precipitation(in)\",\"Wind_Chill(F)\",\"Traffic_Signal\"\n",
    "] + [c + \"_Idx\" for c in categorical_cols]\n",
    "# NA temizle & cache\n",
    "\n",
    "df_no_na = df_selected.dropna().cache()\n",
    "print(f\"✅ Hazır veri: {df_no_na.count():,} satır, {len(df_no_na.columns)} sütun\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07871a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ df_no_na: 5,217,919 rows × 18 cols\n",
      "🔀 train 4,175,117, test 1,042,802\n",
      "🔄 small_train 835,471, small_val 3,339,646\n",
      "\n",
      "⚙️ Training lr on small_train…\n",
      "   → lr  small F1: 0.7845, Acc: 0.8520\n",
      "   ✅ saved → models/us_accidents_lr_small\n",
      "\n",
      "⚙️ Training dt on small_train…\n",
      "   → dt  small F1: 0.7912, Acc: 0.8537\n",
      "   ✅ saved → models/us_accidents_dt_small\n",
      "\n",
      "⚙️ Training rf on small_train…\n",
      "   → rf  small F1: 0.7843, Acc: 0.8522\n",
      "   ✅ saved → models/us_accidents_rf_small\n",
      "\n",
      "⚙️ Training nb on small_train…\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o216.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 321.0 failed 1 times, most recent failure: Lost task 9.0 in stage 321.0 (TID 2584) (MSI executor driver): java.lang.RuntimeException: Vector values MUST NOT be Negative, NaN or Infinity, but got [-2.9,72.0,30.2,8.0,10.4,0.0,-19.8,0.0,5.0,5.0,1.0,1.0,8.0,12.0,0.0]\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.lang.RuntimeException: Vector values MUST NOT be Negative, NaN or Infinity, but got [-2.9,72.0,30.2,8.0,10.4,0.0,-19.8,0.0,5.0,5.0,1.0,1.0,8.0,12.0,0.0]\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 136\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m⚙️ Training \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on small_train…\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    135\u001b[0m pipe \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39m[label_indexer] \u001b[38;5;241m+\u001b[39m cat_indexers \u001b[38;5;241m+\u001b[39m [assembler, clf])\n\u001b[1;32m--> 136\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmall_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(small_val)\n\u001b[0;32m    138\u001b[0m f1  \u001b[38;5;241m=\u001b[39m f1_eval\u001b[38;5;241m.\u001b[39mevaluate(preds)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o216.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 321.0 failed 1 times, most recent failure: Lost task 9.0 in stage 321.0 (TID 2584) (MSI executor driver): java.lang.RuntimeException: Vector values MUST NOT be Negative, NaN or Infinity, but got [-2.9,72.0,30.2,8.0,10.4,0.0,-19.8,0.0,5.0,5.0,1.0,1.0,8.0,12.0,0.0]\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.lang.RuntimeException: Vector values MUST NOT be Negative, NaN or Infinity, but got [-2.9,72.0,30.2,8.0,10.4,0.0,-19.8,0.0,5.0,5.0,1.0,1.0,8.0,12.0,0.0]\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # 1) Spark setup + imports\n",
    "\n",
    "# %% [python]\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, unix_timestamp, when\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression,\n",
    "    DecisionTreeClassifier,\n",
    "    RandomForestClassifier,\n",
    "    NaiveBayes,\n",
    "    GBTClassifier,\n",
    "    OneVsRest\n",
    ")\n",
    "\n",
    "# start session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"US Accidents – Multi‑Model Small‑Train\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\",\"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\",\"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# %% [markdown]\n",
    "# # 2) Data prep & `df_no_na`\n",
    "\n",
    "# %% [python]\n",
    "# 2.1) Read + drop\n",
    "df = spark.read.csv(\"US_Accidents_March23.csv\", header=True, inferSchema=True)\n",
    "drop_cols = [\"ID\",\"Source\",\"Zipcode\",\"Timezone\",\"Airport_Code\",\"Amenity\",\"Bump\",\n",
    "             \"Give_Way\",\"No_Exit\",\"Railway\",\"Description\",\"County\",\"Roundabout\",\n",
    "             \"Station\",\"Stop\",\"Nautical_Twilight\",\"Astronomical_Twilight\",\"Country\"]\n",
    "df = df.drop(*drop_cols)\n",
    "\n",
    "# 2.2) Duration (mins)\n",
    "df = df.withColumn(\"Start_TS\", to_timestamp(\"Start_Time\")) \\\n",
    "       .withColumn(\"End_TS\",   to_timestamp(\"End_Time\")) \\\n",
    "       .withColumn(\"Duration\",\n",
    "           ((unix_timestamp(\"End_TS\") - unix_timestamp(\"Start_TS\"))/60).cast(DoubleType())\n",
    "       ) \\\n",
    "       .drop(\"Start_TS\",\"End_TS\",\"Start_Time\",\"End_Time\")\n",
    "\n",
    "# 2.3) Cardinality‑reduce City & Street to top 32\n",
    "def clean_column(df, colname, top_n=32):\n",
    "    tops = [r[colname]\n",
    "            for r in df.groupBy(colname).count()\n",
    "                      .orderBy(\"count\", ascending=False)\n",
    "                      .limit(top_n).collect()]\n",
    "    return df.withColumn(colname+\"_Cleaned\",\n",
    "            when(col(colname).isin(tops), col(colname)).otherwise(\"Other\"))\n",
    "\n",
    "for c in (\"City\",\"Street\"):\n",
    "    df = clean_column(df, c, top_n=32)\n",
    "\n",
    "# 2.4) Select features + Severity\n",
    "selected = [\n",
    "    \"Temperature(F)\",\"Humidity(%)\",\"Pressure(in)\",\"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\",\"Precipitation(in)\",\"Wind_Chill(F)\",\"Traffic_Signal\",\n",
    "    \"Weather_Condition\",\"Wind_Direction\",\"Junction\",\"Duration\",\n",
    "    \"Civil_Twilight\",\"Sunrise_Sunset\",\"State\",\"City_Cleaned\",\"Street_Cleaned\",\n",
    "    \"Severity\"\n",
    "]\n",
    "df = df.select(*selected)\n",
    "\n",
    "# 2.5) Drop NA & cache\n",
    "df_no_na = df.dropna().cache()\n",
    "print(f\"✅ df_no_na: {df_no_na.count():,} rows × {len(df_no_na.columns)} cols\")\n",
    "\n",
    "# %% [markdown]\n",
    "# # 3) Label‐index + categorical indexers + assembler\n",
    "\n",
    "# %% [python]\n",
    "# 3.1) Severity → label (0…3)\n",
    "label_indexer = StringIndexer(inputCol=\"Severity\", outputCol=\"label\", handleInvalid=\"keep\")\n",
    "\n",
    "# 3.2) categoricals\n",
    "cats = [\"Weather_Condition\",\"Wind_Direction\",\"Civil_Twilight\",\n",
    "        \"Sunrise_Sunset\",\"State\",\"City_Cleaned\",\"Street_Cleaned\"]\n",
    "cat_indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=c+\"_Idx\", handleInvalid=\"keep\")\n",
    "    for c in cats\n",
    "]\n",
    "\n",
    "# 3.3) feature columns\n",
    "numeric = [\"Temperature(F)\",\"Humidity(%)\",\"Pressure(in)\",\"Visibility(mi)\",\n",
    "           \"Wind_Speed(mph)\",\"Precipitation(in)\",\"Wind_Chill(F)\",\"Traffic_Signal\"]\n",
    "features = numeric + [c+\"_Idx\" for c in cats]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "\n",
    "# %% [markdown]\n",
    "# # 4) Train/Test split + small subset\n",
    "\n",
    "# %% [python]\n",
    "train, test = df_no_na.randomSplit([0.8,0.2], seed=42)\n",
    "print(f\"🔀 train {train.count():,}, test {test.count():,}\")\n",
    "\n",
    "small_train, small_val = train.randomSplit([0.2,0.8], seed=42)\n",
    "print(f\"🔄 small_train {small_train.count():,}, small_val {small_val.count():,}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d283a3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\aslay\\bert_env\\lib\\site-packages (3.0.2)\n",
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Collecting catboost\n",
      "  Downloading catboost-1.2.8-cp312-cp312-win_amd64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\aslay\\bert_env\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\aslay\\bert_env\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\aslay\\bert_env\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Collecting graphviz (from catboost)\n",
      "  Downloading graphviz-0.21-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\aslay\\bert_env\\lib\\site-packages (from catboost) (3.9.1.post1)\n",
      "Requirement already satisfied: pandas>=0.24 in c:\\users\\aslay\\bert_env\\lib\\site-packages (from catboost) (2.2.2)\n",
      "Collecting plotly (from catboost)\n",
      "  Downloading plotly-6.2.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: six in c:\\users\\aslay\\bert_env\\lib\\site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\aslay\\bert_env\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aslay\\bert_env\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aslay\\bert_env\\lib\\site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aslay\\bert_env\\lib\\site-packages (from pandas>=0.24->catboost) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aslay\\bert_env\\lib\\site-packages (from pandas>=0.24->catboost) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\aslay\\bert_env\\lib\\site-packages (from matplotlib->catboost) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\aslay\\bert_env\\lib\\site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\aslay\\bert_env\\lib\\site-packages (from matplotlib->catboost) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\aslay\\bert_env\\lib\\site-packages (from matplotlib->catboost) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aslay\\bert_env\\lib\\site-packages (from matplotlib->catboost) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\aslay\\bert_env\\lib\\site-packages (from matplotlib->catboost) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\aslay\\bert_env\\lib\\site-packages (from matplotlib->catboost) (3.1.2)\n",
      "Collecting narwhals>=1.15.1 (from plotly->catboost)\n",
      "  Downloading narwhals-1.47.1-py3-none-any.whl.metadata (11 kB)\n",
      "Downloading lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.5/1.5 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 3.3 MB/s eta 0:00:00\n",
      "Downloading catboost-1.2.8-cp312-cp312-win_amd64.whl (102.4 MB)\n",
      "   ---------------------------------------- 0.0/102.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.8/102.4 MB 3.7 MB/s eta 0:00:28\n",
      "    --------------------------------------- 1.6/102.4 MB 4.2 MB/s eta 0:00:25\n",
      "   - -------------------------------------- 2.6/102.4 MB 4.3 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 3.4/102.4 MB 4.2 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 4.2/102.4 MB 4.1 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 5.0/102.4 MB 4.2 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 5.8/102.4 MB 4.1 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 6.6/102.4 MB 4.1 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 7.3/102.4 MB 4.1 MB/s eta 0:00:24\n",
      "   --- ------------------------------------ 8.4/102.4 MB 4.1 MB/s eta 0:00:23\n",
      "   --- ------------------------------------ 9.4/102.4 MB 4.2 MB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 10.5/102.4 MB 4.2 MB/s eta 0:00:22\n",
      "   ---- ----------------------------------- 11.0/102.4 MB 4.2 MB/s eta 0:00:22\n",
      "   ---- ----------------------------------- 11.5/102.4 MB 4.1 MB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 12.1/102.4 MB 3.9 MB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 12.6/102.4 MB 3.8 MB/s eta 0:00:24\n",
      "   ----- ---------------------------------- 12.8/102.4 MB 3.7 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 13.1/102.4 MB 3.6 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 13.9/102.4 MB 3.5 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 14.4/102.4 MB 3.5 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 14.9/102.4 MB 3.4 MB/s eta 0:00:26\n",
      "   ------ --------------------------------- 15.5/102.4 MB 3.4 MB/s eta 0:00:26\n",
      "   ------ --------------------------------- 16.0/102.4 MB 3.4 MB/s eta 0:00:26\n",
      "   ------ --------------------------------- 16.3/102.4 MB 3.4 MB/s eta 0:00:26\n",
      "   ------ --------------------------------- 16.8/102.4 MB 3.3 MB/s eta 0:00:27\n",
      "   ------ --------------------------------- 17.0/102.4 MB 3.2 MB/s eta 0:00:27\n",
      "   ------ --------------------------------- 17.6/102.4 MB 3.2 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 18.1/102.4 MB 3.1 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 18.6/102.4 MB 3.1 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 19.1/102.4 MB 3.1 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 19.7/102.4 MB 3.1 MB/s eta 0:00:28\n",
      "   ------- -------------------------------- 20.2/102.4 MB 3.1 MB/s eta 0:00:27\n",
      "   -------- ------------------------------- 20.7/102.4 MB 3.1 MB/s eta 0:00:27\n",
      "   -------- ------------------------------- 21.5/102.4 MB 3.0 MB/s eta 0:00:27\n",
      "   -------- ------------------------------- 22.0/102.4 MB 3.1 MB/s eta 0:00:27\n",
      "   -------- ------------------------------- 22.8/102.4 MB 3.0 MB/s eta 0:00:27\n",
      "   --------- ------------------------------ 23.3/102.4 MB 3.1 MB/s eta 0:00:26\n",
      "   --------- ------------------------------ 23.9/102.4 MB 3.0 MB/s eta 0:00:26\n",
      "   --------- ------------------------------ 24.1/102.4 MB 3.0 MB/s eta 0:00:27\n",
      "   --------- ------------------------------ 24.4/102.4 MB 2.9 MB/s eta 0:00:27\n",
      "   --------- ------------------------------ 24.4/102.4 MB 2.9 MB/s eta 0:00:27\n",
      "   --------- ------------------------------ 24.6/102.4 MB 2.8 MB/s eta 0:00:28\n",
      "   --------- ------------------------------ 24.9/102.4 MB 2.8 MB/s eta 0:00:28\n",
      "   --------- ------------------------------ 25.2/102.4 MB 2.8 MB/s eta 0:00:28\n",
      "   ---------- ----------------------------- 25.7/102.4 MB 2.7 MB/s eta 0:00:28\n",
      "   ---------- ----------------------------- 26.0/102.4 MB 2.7 MB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 26.2/102.4 MB 2.7 MB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 26.5/102.4 MB 2.7 MB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 27.0/102.4 MB 2.7 MB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 27.3/102.4 MB 2.6 MB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 27.8/102.4 MB 2.6 MB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 28.0/102.4 MB 2.6 MB/s eta 0:00:29\n",
      "   ----------- ---------------------------- 28.6/102.4 MB 2.6 MB/s eta 0:00:29\n",
      "   ----------- ---------------------------- 28.8/102.4 MB 2.6 MB/s eta 0:00:29\n",
      "   ----------- ---------------------------- 29.1/102.4 MB 2.6 MB/s eta 0:00:29\n",
      "   ----------- ---------------------------- 29.6/102.4 MB 2.6 MB/s eta 0:00:29\n",
      "   ----------- ---------------------------- 30.1/102.4 MB 2.6 MB/s eta 0:00:29\n",
      "   ----------- ---------------------------- 30.4/102.4 MB 2.5 MB/s eta 0:00:29\n",
      "   ------------ --------------------------- 30.9/102.4 MB 2.5 MB/s eta 0:00:29\n",
      "   ------------ --------------------------- 31.5/102.4 MB 2.5 MB/s eta 0:00:29\n",
      "   ------------ --------------------------- 31.7/102.4 MB 2.5 MB/s eta 0:00:29\n",
      "   ------------ --------------------------- 32.2/102.4 MB 2.5 MB/s eta 0:00:28\n",
      "   ------------ --------------------------- 32.8/102.4 MB 2.5 MB/s eta 0:00:28\n",
      "   ------------ --------------------------- 33.0/102.4 MB 2.5 MB/s eta 0:00:28\n",
      "   ------------- -------------------------- 33.6/102.4 MB 2.5 MB/s eta 0:00:28\n",
      "   ------------- -------------------------- 33.8/102.4 MB 2.5 MB/s eta 0:00:28\n",
      "   ------------- -------------------------- 34.3/102.4 MB 2.5 MB/s eta 0:00:28\n",
      "   ------------- -------------------------- 34.9/102.4 MB 2.5 MB/s eta 0:00:28\n",
      "   ------------- -------------------------- 35.1/102.4 MB 2.5 MB/s eta 0:00:28\n",
      "   ------------- -------------------------- 35.4/102.4 MB 2.5 MB/s eta 0:00:28\n",
      "   -------------- ------------------------- 35.9/102.4 MB 2.4 MB/s eta 0:00:28\n",
      "   -------------- ------------------------- 36.2/102.4 MB 2.4 MB/s eta 0:00:28\n",
      "   -------------- ------------------------- 36.7/102.4 MB 2.4 MB/s eta 0:00:27\n",
      "   -------------- ------------------------- 37.2/102.4 MB 2.4 MB/s eta 0:00:27\n",
      "   -------------- ------------------------- 37.7/102.4 MB 2.4 MB/s eta 0:00:27\n",
      "   -------------- ------------------------- 38.3/102.4 MB 2.4 MB/s eta 0:00:27\n",
      "   --------------- ------------------------ 38.8/102.4 MB 2.4 MB/s eta 0:00:27\n",
      "   --------------- ------------------------ 38.8/102.4 MB 2.4 MB/s eta 0:00:27\n",
      "   --------------- ------------------------ 39.3/102.4 MB 2.4 MB/s eta 0:00:27\n",
      "   --------------- ------------------------ 39.8/102.4 MB 2.4 MB/s eta 0:00:26\n",
      "   --------------- ------------------------ 40.4/102.4 MB 2.4 MB/s eta 0:00:26\n",
      "   --------------- ------------------------ 40.6/102.4 MB 2.4 MB/s eta 0:00:26\n",
      "   ---------------- ----------------------- 41.2/102.4 MB 2.4 MB/s eta 0:00:26\n",
      "   ---------------- ----------------------- 41.4/102.4 MB 2.4 MB/s eta 0:00:26\n",
      "   ---------------- ----------------------- 41.7/102.4 MB 2.4 MB/s eta 0:00:26\n",
      "   ---------------- ----------------------- 42.2/102.4 MB 2.4 MB/s eta 0:00:26\n",
      "   ---------------- ----------------------- 42.7/102.4 MB 2.4 MB/s eta 0:00:26\n",
      "   ---------------- ----------------------- 43.3/102.4 MB 2.4 MB/s eta 0:00:25\n",
      "   ---------------- ----------------------- 43.5/102.4 MB 2.4 MB/s eta 0:00:25\n",
      "   ----------------- ---------------------- 44.0/102.4 MB 2.4 MB/s eta 0:00:25\n",
      "   ----------------- ---------------------- 44.6/102.4 MB 2.4 MB/s eta 0:00:25\n",
      "   ----------------- ---------------------- 45.1/102.4 MB 2.4 MB/s eta 0:00:25\n",
      "   ----------------- ---------------------- 45.4/102.4 MB 2.4 MB/s eta 0:00:25\n",
      "   ----------------- ---------------------- 45.9/102.4 MB 2.4 MB/s eta 0:00:24\n",
      "   ------------------ --------------------- 46.4/102.4 MB 2.4 MB/s eta 0:00:24\n",
      "   ------------------ --------------------- 46.7/102.4 MB 2.4 MB/s eta 0:00:24\n",
      "   ------------------ --------------------- 47.2/102.4 MB 2.4 MB/s eta 0:00:24\n",
      "   ------------------ --------------------- 47.7/102.4 MB 2.4 MB/s eta 0:00:24\n",
      "   ------------------ --------------------- 48.2/102.4 MB 2.4 MB/s eta 0:00:23\n",
      "   ------------------- -------------------- 48.8/102.4 MB 2.4 MB/s eta 0:00:23\n",
      "   ------------------- -------------------- 49.3/102.4 MB 2.4 MB/s eta 0:00:23\n",
      "   ------------------- -------------------- 49.8/102.4 MB 2.4 MB/s eta 0:00:23\n",
      "   ------------------- -------------------- 50.3/102.4 MB 2.4 MB/s eta 0:00:22\n",
      "   ------------------- -------------------- 50.6/102.4 MB 2.4 MB/s eta 0:00:22\n",
      "   ------------------- -------------------- 51.1/102.4 MB 2.4 MB/s eta 0:00:22\n",
      "   -------------------- ------------------- 51.6/102.4 MB 2.4 MB/s eta 0:00:22\n",
      "   -------------------- ------------------- 52.2/102.4 MB 2.4 MB/s eta 0:00:22\n",
      "   -------------------- ------------------- 52.7/102.4 MB 2.4 MB/s eta 0:00:22\n",
      "   -------------------- ------------------- 53.0/102.4 MB 2.4 MB/s eta 0:00:21\n",
      "   -------------------- ------------------- 53.7/102.4 MB 2.4 MB/s eta 0:00:21\n",
      "   --------------------- ------------------ 54.0/102.4 MB 2.4 MB/s eta 0:00:21\n",
      "   --------------------- ------------------ 54.5/102.4 MB 2.4 MB/s eta 0:00:21\n",
      "   --------------------- ------------------ 54.8/102.4 MB 2.4 MB/s eta 0:00:21\n",
      "   --------------------- ------------------ 55.3/102.4 MB 2.4 MB/s eta 0:00:20\n",
      "   --------------------- ------------------ 55.8/102.4 MB 2.4 MB/s eta 0:00:20\n",
      "   --------------------- ------------------ 56.1/102.4 MB 2.4 MB/s eta 0:00:20\n",
      "   ---------------------- ----------------- 56.6/102.4 MB 2.4 MB/s eta 0:00:20\n",
      "   ---------------------- ----------------- 57.4/102.4 MB 2.4 MB/s eta 0:00:20\n",
      "   ---------------------- ----------------- 57.9/102.4 MB 2.4 MB/s eta 0:00:19\n",
      "   ---------------------- ----------------- 58.5/102.4 MB 2.4 MB/s eta 0:00:19\n",
      "   ----------------------- ---------------- 59.0/102.4 MB 2.4 MB/s eta 0:00:19\n",
      "   ----------------------- ---------------- 59.2/102.4 MB 2.4 MB/s eta 0:00:19\n",
      "   ----------------------- ---------------- 59.8/102.4 MB 2.4 MB/s eta 0:00:19\n",
      "   ----------------------- ---------------- 60.6/102.4 MB 2.4 MB/s eta 0:00:18\n",
      "   ----------------------- ---------------- 61.1/102.4 MB 2.4 MB/s eta 0:00:18\n",
      "   ------------------------ --------------- 61.9/102.4 MB 2.4 MB/s eta 0:00:17\n",
      "   ------------------------ --------------- 62.7/102.4 MB 2.4 MB/s eta 0:00:17\n",
      "   ------------------------ --------------- 63.4/102.4 MB 2.4 MB/s eta 0:00:17\n",
      "   ------------------------ --------------- 63.7/102.4 MB 2.4 MB/s eta 0:00:17\n",
      "   ------------------------- -------------- 64.5/102.4 MB 2.4 MB/s eta 0:00:16\n",
      "   ------------------------- -------------- 65.0/102.4 MB 2.4 MB/s eta 0:00:16\n",
      "   ------------------------- -------------- 65.8/102.4 MB 2.4 MB/s eta 0:00:16\n",
      "   ------------------------- -------------- 66.3/102.4 MB 2.4 MB/s eta 0:00:15\n",
      "   -------------------------- ------------- 66.6/102.4 MB 2.4 MB/s eta 0:00:15\n",
      "   -------------------------- ------------- 67.1/102.4 MB 2.4 MB/s eta 0:00:15\n",
      "   -------------------------- ------------- 67.6/102.4 MB 2.4 MB/s eta 0:00:15\n",
      "   -------------------------- ------------- 68.2/102.4 MB 2.4 MB/s eta 0:00:15\n",
      "   -------------------------- ------------- 68.7/102.4 MB 2.4 MB/s eta 0:00:14\n",
      "   --------------------------- ------------ 69.2/102.4 MB 2.4 MB/s eta 0:00:14\n",
      "   --------------------------- ------------ 69.7/102.4 MB 2.4 MB/s eta 0:00:14\n",
      "   --------------------------- ------------ 70.5/102.4 MB 2.4 MB/s eta 0:00:14\n",
      "   --------------------------- ------------ 70.8/102.4 MB 2.4 MB/s eta 0:00:14\n",
      "   --------------------------- ------------ 71.6/102.4 MB 2.4 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 72.1/102.4 MB 2.4 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 72.6/102.4 MB 2.4 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 73.4/102.4 MB 2.4 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 73.9/102.4 MB 2.5 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 74.4/102.4 MB 2.4 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 75.0/102.4 MB 2.4 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 75.8/102.4 MB 2.4 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 76.3/102.4 MB 2.4 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 76.5/102.4 MB 2.4 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 77.3/102.4 MB 2.4 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 77.9/102.4 MB 2.4 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 78.6/102.4 MB 2.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 79.4/102.4 MB 2.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 80.0/102.4 MB 2.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 80.7/102.4 MB 2.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 81.5/102.4 MB 2.4 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 82.1/102.4 MB 2.4 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 82.6/102.4 MB 2.4 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 82.8/102.4 MB 2.3 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 83.4/102.4 MB 2.4 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 83.6/102.4 MB 2.4 MB/s eta 0:00:08\n",
      "   -------------------------------- ------- 84.1/102.4 MB 2.4 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 84.9/102.4 MB 2.4 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 85.7/102.4 MB 2.4 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 86.2/102.4 MB 2.4 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 86.8/102.4 MB 2.4 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 87.3/102.4 MB 2.4 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 88.1/102.4 MB 2.4 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 88.6/102.4 MB 2.4 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 89.1/102.4 MB 2.4 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 89.7/102.4 MB 2.4 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 90.4/102.4 MB 2.4 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 91.0/102.4 MB 2.4 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 91.5/102.4 MB 2.4 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 92.0/102.4 MB 2.4 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 92.5/102.4 MB 2.4 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 93.3/102.4 MB 2.4 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 94.1/102.4 MB 2.4 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 94.9/102.4 MB 2.4 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 95.4/102.4 MB 2.4 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 96.2/102.4 MB 2.4 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 96.7/102.4 MB 2.4 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 97.3/102.4 MB 2.5 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 98.0/102.4 MB 2.5 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 98.8/102.4 MB 2.5 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 99.6/102.4 MB 2.5 MB/s eta 0:00:02\n",
      "   ---------------------------------------  100.7/102.4 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  101.7/102.4 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  102.2/102.4 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 102.4/102.4 MB 2.6 MB/s eta 0:00:00\n",
      "Downloading graphviz-0.21-py3-none-any.whl (47 kB)\n",
      "Downloading plotly-6.2.0-py3-none-any.whl (9.6 MB)\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.6 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.0/9.6 MB 2.6 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.8/9.6 MB 2.8 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.6/9.6 MB 3.1 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.4/9.6 MB 3.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.2/9.6 MB 3.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.2/9.6 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.0/9.6 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.1/9.6 MB 3.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.9/9.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.7/9.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.6/9.6 MB 3.9 MB/s eta 0:00:00\n",
      "Downloading narwhals-1.47.1-py3-none-any.whl (375 kB)\n",
      "Installing collected packages: narwhals, graphviz, plotly, lightgbm, catboost\n",
      "\n",
      "   ---------------------------------------- 0/5 [narwhals]\n",
      "   ---------------------------------------- 0/5 [narwhals]\n",
      "   ---------------------------------------- 0/5 [narwhals]\n",
      "   ---------------------------------------- 0/5 [narwhals]\n",
      "   ---------------------------------------- 0/5 [narwhals]\n",
      "   ---------------------------------------- 0/5 [narwhals]\n",
      "   ---------------------------------------- 0/5 [narwhals]\n",
      "   ---------------------------------------- 0/5 [narwhals]\n",
      "   ---------------------------------------- 0/5 [narwhals]\n",
      "   ---------------------------------------- 0/5 [narwhals]\n",
      "   ---------------------------------------- 0/5 [narwhals]\n",
      "   -------- ------------------------------- 1/5 [graphviz]\n",
      "   -------- ------------------------------- 1/5 [graphviz]\n",
      "   -------- ------------------------------- 1/5 [graphviz]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ---------------- ----------------------- 2/5 [plotly]\n",
      "   ------------------------ --------------- 3/5 [lightgbm]\n",
      "   -------------------------------- ------- 4/5 [catboost]\n",
      "   -------------------------------- ------- 4/5 [catboost]\n",
      "   -------------------------------- ------- 4/5 [catboost]\n",
      "   -------------------------------- ------- 4/5 [catboost]\n",
      "   -------------------------------- ------- 4/5 [catboost]\n",
      "   -------------------------------- ------- 4/5 [catboost]\n",
      "   -------------------------------- ------- 4/5 [catboost]\n",
      "   -------------------------------- ------- 4/5 [catboost]\n",
      "   -------------------------------- ------- 4/5 [catboost]\n",
      "   -------------------------------- ------- 4/5 [catboost]\n",
      "   -------------------------------- ------- 4/5 [catboost]\n",
      "   -------------------------------- ------- 4/5 [catboost]\n",
      "   -------------------------------- ------- 4/5 [catboost]\n",
      "   ---------------------------------------- 5/5 [catboost]\n",
      "\n",
      "Successfully installed catboost-1.2.8 graphviz-0.21 lightgbm-4.6.0 narwhals-1.47.1 plotly-6.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install xgboost lightgbm catboost scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627e3145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbfeff77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Training GBT on small_train…\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3423.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 347.0 failed 1 times, most recent failure: Lost task 7.0 in stage 347.0 (TID 2793) (MSI executor driver): java.lang.RuntimeException: Labels MUST be in {0, 1}, but got 3.0\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\r\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\r\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1226)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2487)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2488)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1228)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1221)\r\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\r\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:333)\r\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:61)\r\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$1(GBTClassifier.scala:201)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:170)\r\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:58)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.lang.RuntimeException: Labels MUST be in {0, 1}, but got 3.0\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\r\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\r\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1226)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2487)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m⚙️ Training \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on small_train…\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m pipe \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39m[label_indexer] \u001b[38;5;241m+\u001b[39m cat_indexers \u001b[38;5;241m+\u001b[39m [assembler, clf])\n\u001b[1;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmall_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(small_val)\n\u001b[0;32m     15\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_eval\u001b[38;5;241m.\u001b[39mevaluate(preds)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o3423.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 347.0 failed 1 times, most recent failure: Lost task 7.0 in stage 347.0 (TID 2793) (MSI executor driver): java.lang.RuntimeException: Labels MUST be in {0, 1}, but got 3.0\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\r\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\r\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1226)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2487)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2488)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1228)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1221)\r\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\r\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:333)\r\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:61)\r\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$1(GBTClassifier.scala:201)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:170)\r\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:58)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.lang.RuntimeException: Labels MUST be in {0, 1}, but got 3.0\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\r\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\r\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1226)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2487)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "# Spark modelleri\n",
    "spark_models = {\n",
    "    \"GBT\": GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=50),\n",
    "    \"LogisticRegression\": LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=20),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxBins=256, maxDepth=10),\n",
    "    \"RandomForest\": RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50, maxBins=256),\n",
    "    \n",
    "}\n",
    "\n",
    "for name, clf in spark_models.items():\n",
    "    print(f\"\\n⚙️ Training {name} on small_train…\")\n",
    "    pipe = Pipeline(stages=[label_indexer] + cat_indexers + [assembler, clf])\n",
    "    model = pipe.fit(small_train)\n",
    "    preds = model.transform(small_val)\n",
    "    f1 = f1_eval.evaluate(preds)\n",
    "    acc = acc_eval.evaluate(preds)\n",
    "    print(f\"   → {name}: F1={f1:.4f}, Acc={acc:.4f}\")\n",
    "    path = f\"models/us_accidents_{name.lower()}_small\"\n",
    "    model.write().overwrite().save(path)\n",
    "    print(f\"   💾 saved → {path}\")\n",
    "\n",
    "# %% [python]\n",
    "# 5) scikit‐learn / XGBoost / LightGBM / CatBoost Experiments (pandas örneği)\n",
    "pd_sample = (\n",
    "    small_train.orderBy(rand()).limit(50000).toPandas()\n",
    ")\n",
    "X = pd_sample[[*feature_cols]].astype(float)\n",
    "y = pd_sample[\"Severity\"].astype(int)\n",
    "\n",
    "pd_val = (\n",
    "    small_val.orderBy(rand()).limit(10000).toPandas()\n",
    ")\n",
    "X_val = pd_val[[*feature_cols]].astype(float)\n",
    "y_val = pd_val[\"Severity\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "795b57d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ df_no_na hazır: 5,217,919 kayıt, 18 sütun\n",
      "🔀 Bölme → train: 4,175,117, test: 1,042,802\n",
      "🔄 small_train: 668,070, small_val: 167,401\n"
     ]
    }
   ],
   "source": [
    "# %% [python]\n",
    "# 1) Spark ve Kütüphaneleri Başlat\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, unix_timestamp, when, rand\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression, DecisionTreeClassifier,\n",
    "    RandomForestClassifier, GBTClassifier\n",
    ")\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"US Accidents Severity Experiments\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\",\"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\",\"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# %% [python]\n",
    "# 2) Veri Hazırlığı\n",
    "# 2.1) CSV'yi oku\n",
    "df = spark.read.csv(\"US_Accidents_March23.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 2.2) İstenmeyen sütunları düş\n",
    "drop_cols = [\n",
    "    \"ID\",\"Source\",\"Zipcode\",\"Timezone\",\"Airport_Code\",\"Amenity\",\"Bump\",\n",
    "    \"Give_Way\",\"No_Exit\",\"Railway\",\"Description\",\"County\",\"Roundabout\",\n",
    "    \"Station\",\"Stop\",\"Nautical_Twilight\",\"Astronomical_Twilight\",\"Country\"\n",
    "]\n",
    "df2 = df.drop(*drop_cols)\n",
    "\n",
    "# 2.3) Start/End → Duration hesapla\n",
    "df3 = (\n",
    "    df2\n",
    "    .withColumn(\"Start_TS\", to_timestamp(col(\"Start_Time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .withColumn(\"End_TS\",   to_timestamp(col(\"End_Time\"),   \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .withColumn(\"Duration\",\n",
    "        ((unix_timestamp(col(\"End_TS\")) - unix_timestamp(col(\"Start_TS\"))) / 60)\n",
    "        .cast(DoubleType())\n",
    "    )\n",
    "    .drop(\"Start_TS\",\"End_TS\",\"Start_Time\",\"End_Time\")\n",
    ")\n",
    "\n",
    "# 2.4) City/Street cardinality düşür\n",
    "\n",
    "def clean_column(df, column_name, top_n=32):\n",
    "    top_vals = [\n",
    "        r[column_name] for r in\n",
    "        df.groupBy(column_name).count()\n",
    "          .orderBy(col(\"count\").desc())\n",
    "          .limit(top_n)\n",
    "          .collect()\n",
    "    ]\n",
    "    return df.withColumn(\n",
    "        f\"{column_name}_Cleaned\",\n",
    "        when(col(column_name).isin(top_vals), col(column_name)).otherwise(\"Other\")\n",
    "    )\n",
    "\n",
    "for c in [\"City\",\"Street\"]:\n",
    "    df3 = clean_column(df3, c, top_n=32)\n",
    "\n",
    "# 2.5) Sütunları seç\n",
    "selected_cols = [\n",
    "    \"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\", \"Precipitation(in)\", \"Wind_Chill(F)\", \"Traffic_Signal\",\n",
    "    \"Weather_Condition\", \"Wind_Direction\", \"Junction\", \"Duration\", \"Severity\",\n",
    "    \"Civil_Twilight\", \"Sunrise_Sunset\", \"State\", \"City_Cleaned\", \"Street_Cleaned\"\n",
    "]\n",
    "df_selected = df3.select(*selected_cols)\n",
    "\n",
    "# 2.6) Kategorik ve feature listeleri\n",
    "categorical_cols = [\n",
    "    \"Weather_Condition\",\"Wind_Direction\",\"Civil_Twilight\",\n",
    "    \"Sunrise_Sunset\",\"State\",\"City_Cleaned\",\"Street_Cleaned\"\n",
    "]\n",
    "feature_cols = [\n",
    "    \"Temperature(F)\",\"Humidity(%)\",\"Pressure(in)\",\"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\",\"Precipitation(in)\",\"Wind_Chill(F)\",\"Traffic_Signal\"\n",
    "] + [c + \"_Idx\" for c in categorical_cols]\n",
    "\n",
    "# 2.7) NA at ve cache\n",
    "\n",
    "df_no_na = df_selected.dropna().cache()\n",
    "print(f\"✅ df_no_na hazır: {df_no_na.count():,} kayıt, {len(df_no_na.columns)} sütun\")\n",
    "\n",
    "# %% [python]\n",
    "# 3) Train/Test Bölme ve Küçük Alt Küme Oluşturma\n",
    "train, test = df_no_na.randomSplit([0.8,0.2], seed=42)\n",
    "print(f\"🔀 Bölme → train: {train.count():,}, test: {test.count():,}\")\n",
    "\n",
    "# Küçük alt küme (spark tarafı)\n",
    "small = train.sample(fraction=0.2, seed=42)\n",
    "small_train, small_val = small.randomSplit([0.8,0.2], seed=42)\n",
    "print(f\"🔄 small_train: {small_train.count():,}, small_val: {small_val.count():,}\")\n",
    "\n",
    "# %% [python]\n",
    "# 4) Spark ML ile küçük alt kümede modelleri dene ve kaydet\n",
    "# Label indexer\n",
    "label_indexer = StringIndexer(inputCol=\"Severity\", outputCol=\"label\", handleInvalid=\"keep\")\n",
    "# Categorical indexers\n",
    "cat_indexers = [StringIndexer(inputCol=c, outputCol=c+\"_Idx\", handleInvalid=\"keep\")\n",
    "                for c in categorical_cols]\n",
    "# Assembler\\ nassembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "# Evaluatorlar\n",
    "f1_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "acc_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05451f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Training LogisticRegression on small_train…\n",
      "   → LogisticRegression: F1=0.7862, Acc=0.8531\n",
      "   💾 saved → models/us_accidents_logisticregression_small\n",
      "\n",
      "⚙️ Training DecisionTree on small_train…\n",
      "   → DecisionTree: F1=0.7979, Acc=0.8549\n",
      "   💾 saved → models/us_accidents_decisiontree_small\n",
      "\n",
      "⚙️ Training RandomForest on small_train…\n",
      "   → RandomForest: F1=0.7860, Acc=0.8535\n",
      "   💾 saved → models/us_accidents_randomforest_small\n"
     ]
    }
   ],
   "source": [
    "# Spark modelleri\n",
    "spark_models = {\n",
    "    \"LogisticRegression\": LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=20),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxBins=256, maxDepth=10),\n",
    "    \"RandomForest\": RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50, maxBins=256),\n",
    "    \n",
    "}\n",
    "\n",
    "for name, clf in spark_models.items():\n",
    "    print(f\"\\n⚙️ Training {name} on small_train…\")\n",
    "    pipe = Pipeline(stages=[label_indexer] + cat_indexers + [assembler, clf])\n",
    "    model = pipe.fit(small_train)\n",
    "    preds = model.transform(small_val)\n",
    "    f1 = f1_eval.evaluate(preds)\n",
    "    acc = acc_eval.evaluate(preds)\n",
    "    print(f\"   → {name}: F1={f1:.4f}, Acc={acc:.4f}\")\n",
    "    path = f\"models/us_accidents_{name.lower()}_small\"\n",
    "    model.write().overwrite().save(path)\n",
    "    print(f\"   💾 saved → {path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5616f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Spark DataFrame'leri Pandas'a dönüştürülüyor...\n",
      "✅ Pandas DataFrame'leri hazır.\n",
      "   X_train shape (Pandas): (50000, 17), y_train shape (Pandas): (50000,)\n",
      "   X_val shape (Pandas): (10000, 17), y_val shape (Pandas): (10000,)\n",
      "\n",
      "⚙️ **XGBoost** modeli eğitimi (küçük Pandas veri)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aslay\\bert_env\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [13:12:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   → **XGBoost**: F1=0.8356, Acc=0.8689\n",
      "   💾 Model kaydedildi → models/us_accidents_xgboost_small.pkl\n",
      "\n",
      "⚙️ **LightGBM** modeli eğitimi (küçük Pandas veri)...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003151 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1285\n",
      "[LightGBM] [Info] Number of data points in the train set: 50000, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score -4.412898\n",
      "[LightGBM] [Info] Start training from score -0.162072\n",
      "[LightGBM] [Info] Start training from score -2.176834\n",
      "[LightGBM] [Info] Start training from score -3.725543\n",
      "   → **LightGBM**: F1=0.8346, Acc=0.8699\n",
      "   💾 Model kaydedildi → models/us_accidents_lightgbm_small.pkl\n",
      "\n",
      "⚙️ **CatBoost** modeli eğitimi (küçük Pandas veri)...\n",
      "   → **CatBoost**: F1=0.8188, Acc=0.8667\n",
      "   💾 Model kaydedildi → models/us_accidents_catboost_small.pkl\n",
      "\n",
      "--- Scikit-learn (Pandas) Modellerinin Sonuçları ---\n",
      "Model: XGBoost, F1: 0.8356, Acc: 0.8689\n",
      "Model: LightGBM, F1: 0.8346, Acc: 0.8699\n",
      "Model: CatBoost, F1: 0.8188, Acc: 0.8667\n"
     ]
    }
   ],
   "source": [
    "# %% [python]\n",
    "# Gerekli ek kütüphaneleri import edin (önceki imports geçerli)\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import joblib\n",
    "\n",
    "# ÖNEMLİ: feature_cols_for_assembler ve assembler'ın doğru tanımlandığından emin olun.\n",
    "# Bu değişkenlerin bu hücreden önce tanımlanmış olması gerekmektedir.\n",
    "# Örneğin:\n",
    "# numerical_cols_for_assembler = [\"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\",\n",
    "#                                 \"Wind_Speed(mph)\", \"Precipitation(in)\", \"Wind_Chill(F)\", \"Traffic_Signal\",\n",
    "#                                 \"Junction\", \"Duration\"]\n",
    "# categorical_cols = [\"Weather_Condition\", \"Wind_Direction\", \"Civil_Twilight\", \"Sunrise_Sunset\",\n",
    "#                     \"State\", \"City_Cleaned\", \"Street_Cleaned\"]\n",
    "# feature_cols_for_assembler = numerical_cols_for_assembler + [c + \"_Idx\" for c in categorical_cols]\n",
    "# assembler = VectorAssembler(inputCols=feature_cols_for_assembler, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "# cat_indexers = [StringIndexer(inputCol=c, outputCol=c+\"_Idx\", handleInvalid=\"keep\") for c in categorical_cols]\n",
    "# feature_pipeline_for_pandas = Pipeline(stages=cat_indexers + [assembler])\n",
    "# feature_pipeline_model_for_pandas = feature_pipeline_for_pandas.fit(small_train) # small_train bu noktada tanımlı olmalı\n",
    "\n",
    "# small_train ve small_val'ı dönüştür ve Pandas DataFrame'e çek\n",
    "print(\"\\n🔄 Spark DataFrame'leri Pandas'a dönüştürülüyor...\")\n",
    "pd_sample_transformed = feature_pipeline_model_for_pandas.transform(small_train)\n",
    "pd_sample = pd_sample_transformed.orderBy(rand()).limit(50000).toPandas()\n",
    "\n",
    "# 'features' vektör sütununu ve orijinal 'Severity' hedef sütununu al\n",
    "X = pd.DataFrame(pd_sample['features'].apply(lambda x: x.toArray()).tolist())\n",
    "# !!! DÜZELTME: Severity değerlerini 0'dan başlayacak şekilde ayarla\n",
    "y = (pd_sample[\"Severity\"] - 1).astype(int) # Severity'den 1 çıkarıp 0,1,2,3 yapıyoruz\n",
    "\n",
    "pd_val_transformed = feature_pipeline_model_for_pandas.transform(small_val)\n",
    "pd_val = pd_val_transformed.orderBy(rand()).limit(10000).toPandas()\n",
    "\n",
    "X_val = pd.DataFrame(pd_val['features'].apply(lambda x: x.toArray()).tolist())\n",
    "# !!! DÜZELTME: Severity değerlerini 0'dan başlayacak şekilde ayarla\n",
    "y_val = (pd_val[\"Severity\"] - 1).astype(int) # Severity'den 1 çıkarıp 0,1,2,3 yapıyoruz\n",
    "\n",
    "print(f\"✅ Pandas DataFrame'leri hazır.\")\n",
    "print(f\"   X_train shape (Pandas): {X.shape}, y_train shape (Pandas): {y.shape}\")\n",
    "print(f\"   X_val shape (Pandas): {X_val.shape}, y_val shape (Pandas): {y_val.shape}\")\n",
    "\n",
    "# Model tanımlamaları (Çoklu Sınıflandırma için ayarlanmış parametreler)\n",
    "# num_severity_classes doğru olmalı, çünkü y'yi güncelledik.\n",
    "num_severity_classes = y.nunique() # unique sınıf sayısını dinamik olarak bulalım\n",
    "\n",
    "sk_models = {\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        objective='multi:softmax', num_class=num_severity_classes, eval_metric='mlogloss', use_label_encoder=False, random_state=42\n",
    "    ),\n",
    "    \"LightGBM\": LGBMClassifier(\n",
    "        objective='multiclass', num_class=num_severity_classes, metric='multi_logloss', random_state=42\n",
    "    ),\n",
    "    \"CatBoost\": CatBoostClassifier(\n",
    "        iterations=100, learning_rate=0.1, depth=6,\n",
    "        loss_function='MultiClass',\n",
    "        verbose=0, random_seed=42\n",
    "    )\n",
    "}\n",
    "\n",
    "results_sklearn = []\n",
    "for name, clf in sk_models.items():\n",
    "    print(f\"\\n⚙️ **{name}** modeli eğitimi (küçük Pandas veri)...\")\n",
    "    clf.fit(X, y)\n",
    "    preds = clf.predict(X_val)\n",
    "    \n",
    "    # F1 ve Accuracy hesaplama (çoklu sınıflandırma için 'weighted' average)\n",
    "    f1 = f1_score(y_val, preds, average='weighted')\n",
    "    acc = accuracy_score(y_val, preds)\n",
    "    \n",
    "    print(f\"   → **{name}**: F1={f1:.4f}, Acc={acc:.4f}\")\n",
    "    \n",
    "    # Modeli kaydet\n",
    "    path = f\"models/us_accidents_{name.lower()}_small.pkl\"\n",
    "    joblib.dump(clf, path)\n",
    "    print(f\"   💾 Model kaydedildi → {path}\")\n",
    "    results_sklearn.append((name, f1, acc))\n",
    "\n",
    "# ---\n",
    "### Sonuçların Karşılaştırılması\n",
    "print(\"\\n--- Scikit-learn (Pandas) Modellerinin Sonuçları ---\")\n",
    "for res in results_sklearn:\n",
    "    print(f\"Model: {res[0]}, F1: {res[1]:.4f}, Acc: {res[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ca479da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ df_no_na hazır: 5,217,919 kayıt, 18 sütun\n",
      "🔀 Bölme → train: 4,175,117, test: 1,042,802\n",
      "🔄 small_train: 668,070, small_val: 167,401\n",
      "\n",
      "⚙️ **LR** modeli eğitimi (küçük PySpark veri)...\n",
      "🏆 **LR** — small_val F1: 0.7862, Acc: 0.8532\n",
      "💾 **LR** modeli kaydedildi → models/us_accidents_lr_small_spark\n",
      "\n",
      "⚙️ **DT** modeli eğitimi (küçük PySpark veri)...\n",
      "🏆 **DT** — small_val F1: 0.8246, Acc: 0.8631\n",
      "💾 **DT** modeli kaydedildi → models/us_accidents_dt_small_spark\n",
      "\n",
      "⚙️ **RF** modeli eğitimi (küçük PySpark veri)...\n",
      "🏆 **RF** — small_val F1: 0.8104, Acc: 0.8609\n",
      "💾 **RF** modeli kaydedildi → models/us_accidents_rf_small_spark\n",
      "\n",
      "--- Spark ML (PySpark) Modellerinin Küçük Verideki Sonuçları ---\n",
      "Model: LR, F1: 0.7862, Acc: 0.8532\n",
      "Model: DT, F1: 0.8246, Acc: 0.8631\n",
      "Model: RF, F1: 0.8104, Acc: 0.8609\n",
      "\n",
      "✨ Küçük veride en iyi performans gösteren Spark ML modeli: DT\n",
      "\n",
      "⚙️ Final **DT** eğitimi tüm `train` verisi üzerinde…\n",
      "💾 Final DT modeli kaydedildi → models/us_accidents_dt_full_spark\n",
      "\n",
      "📊 Final DT modelinin 'test' verisi üzerinde değerlendirilmesi...\n",
      "🎉 Final DT — Test Seti F1: 0.8110, Acc: 0.8550\n",
      "\n",
      "Spark Session durduruldu.\n"
     ]
    }
   ],
   "source": [
    "# %% [python]\n",
    "# 1) Spark ve Kütüphaneleri Başlat\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, unix_timestamp, when, rand\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression, DecisionTreeClassifier,\n",
    "    RandomForestClassifier # GBTClassifier'ı çoklu sınıflandırma için varsayılan olarak desteklemediği için kaldırdık\n",
    ")\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"US Accidents Severity Experiments - PySpark MLlib\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\",\"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\",\"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# %% [python]\n",
    "# 2) Veri Hazırlığı\n",
    "# 2.1) CSV'yi oku\n",
    "df = spark.read.csv(\"US_Accidents_March23.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 2.2) İstenmeyen sütunları düş\n",
    "drop_cols = [\n",
    "    \"ID\",\"Source\",\"Zipcode\",\"Timezone\",\"Airport_Code\",\"Amenity\",\"Bump\",\n",
    "    \"Give_Way\",\"No_Exit\",\"Railway\",\"Description\",\"County\",\"Roundabout\",\n",
    "    \"Station\",\"Stop\",\"Nautical_Twilight\",\"Astronomical_Twilight\",\"Country\"\n",
    "]\n",
    "df2 = df.drop(*drop_cols)\n",
    "\n",
    "# 2.3) Start/End → Duration hesapla\n",
    "df3 = (\n",
    "    df2\n",
    "    .withColumn(\"Start_TS\", to_timestamp(col(\"Start_Time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .withColumn(\"End_TS\",   to_timestamp(col(\"End_Time\"),   \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .withColumn(\"Duration\",\n",
    "        ((unix_timestamp(col(\"End_TS\")) - unix_timestamp(col(\"Start_TS\"))) / 60)\n",
    "        .cast(DoubleType())\n",
    "    )\n",
    "    .drop(\"Start_TS\",\"End_TS\",\"Start_Time\",\"End_Time\")\n",
    ")\n",
    "\n",
    "# 2.4) City/Street cardinality düşür\n",
    "def clean_column(df, column_name, top_n=32):\n",
    "    top_vals = [\n",
    "        r[column_name] for r in\n",
    "        df.groupBy(column_name).count()\n",
    "          .orderBy(col(\"count\").desc())\n",
    "          .limit(top_n)\n",
    "          .collect()\n",
    "    ]\n",
    "    return df.withColumn(\n",
    "        f\"{column_name}_Cleaned\",\n",
    "        when(col(column_name).isin(top_vals), col(column_name)).otherwise(\"Other\")\n",
    "    )\n",
    "\n",
    "for c in [\"City\",\"Street\"]:\n",
    "    df3 = clean_column(df3, c, top_n=32)\n",
    "\n",
    "# 2.5) Sütunları seç (Severity'yi orijinal haliyle bırakıyoruz)\n",
    "selected_cols = [\n",
    "    \"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\", \"Precipitation(in)\", \"Wind_Chill(F)\", \"Traffic_Signal\",\n",
    "    \"Weather_Condition\", \"Wind_Direction\", \"Junction\", \"Duration\", \"Severity\", # Severity orijinal haliyle kalıyor\n",
    "    \"Civil_Twilight\", \"Sunrise_Sunset\", \"State\", \"City_Cleaned\", \"Street_Cleaned\"\n",
    "]\n",
    "df_selected = df3.select(*selected_cols)\n",
    "\n",
    "# 2.6) Kategorik ve feature listeleri\n",
    "categorical_cols = [\n",
    "    \"Weather_Condition\",\"Wind_Direction\",\"Civil_Twilight\",\n",
    "    \"Sunrise_Sunset\",\"State\",\"City_Cleaned\",\"Street_Cleaned\"\n",
    "]\n",
    "numerical_cols_for_assembler = [\n",
    "    \"Temperature(F)\",\"Humidity(%)\",\"Pressure(in)\",\"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\",\"Precipitation(in)\",\"Wind_Chill(F)\",\"Traffic_Signal\",\n",
    "    \"Junction\", \"Duration\"\n",
    "]\n",
    "\n",
    "# Assembler'a gidecek tüm feature'lar (numerik + indekslenmiş kategorik)\n",
    "feature_cols_for_assembler = numerical_cols_for_assembler + [c + \"_Idx\" for c in categorical_cols]\n",
    "\n",
    "\n",
    "# 2.7) NA at ve cache\n",
    "df_no_na = df_selected.dropna().cache()\n",
    "print(f\"✅ df_no_na hazır: {df_no_na.count():,} kayıt, {len(df_no_na.columns)} sütun\")\n",
    "\n",
    "# %% [python]\n",
    "# 3) Train/Test Bölme ve Küçük Alt Küme Oluşturma\n",
    "train, test = df_no_na.randomSplit([0.8,0.2], seed=42)\n",
    "print(f\"🔀 Bölme → train: {train.count():,}, test: {test.count():,}\")\n",
    "\n",
    "small = train.sample(fraction=0.2, seed=42)\n",
    "small_train, small_val = small.randomSplit([0.8,0.2], seed=42)\n",
    "print(f\"🔄 small_train: {small_train.count():,}, small_val: {small_val.count():,}\")\n",
    "\n",
    "# %% [python]\n",
    "# 4) Spark ML Modelleri Tanımlama ve Küçük Veride Deneme\n",
    "# Label indexer: Severity'yi 0'dan başlayan tamsayı indekslere dönüştürecek\n",
    "label_indexer = StringIndexer(inputCol=\"Severity\", outputCol=\"label\", handleInvalid=\"keep\")\n",
    "label_col_name_for_spark_models = \"label\" # Hedef sütunumuz artık \"label\" olacak\n",
    "\n",
    "# Kategorik özellikler için indexer'lar\n",
    "cat_indexers = [StringIndexer(inputCol=c, outputCol=c+\"_Idx\", handleInvalid=\"keep\")\n",
    "                for c in categorical_cols]\n",
    "# Tüm özellikleri tek bir vektörde birleştiren assembler\n",
    "assembler = VectorAssembler(inputCols=feature_cols_for_assembler, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "\n",
    "# Değerlendirme metrikleri\n",
    "f1_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol=label_col_name_for_spark_models, predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "acc_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol=label_col_name_for_spark_models, predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "# Spark ML Modellerinin Tanımlanması (çoklu sınıflandırma için)\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(labelCol=label_col_name_for_spark_models, featuresCol=\"features\", maxIter=20),\n",
    "    \"DT\": DecisionTreeClassifier(labelCol=label_col_name_for_spark_models, featuresCol=\"features\", maxBins=256),\n",
    "    \"RF\": RandomForestClassifier(labelCol=label_col_name_for_spark_models, featuresCol=\"features\", numTrees=50, maxBins=256)\n",
    "}\n",
    "\n",
    "# Hiperparametre ızgarası (basit örnek)\n",
    "grids = {\n",
    "    \"LR\": ParamGridBuilder().addGrid(models['LR'].regParam, [0.01,0.1]).addGrid(models['LR'].elasticNetParam, [0.0,0.5]).build(),\n",
    "    \"DT\": ParamGridBuilder().addGrid(models['DT'].maxDepth, [5,10]).build(),\n",
    "    \"RF\": ParamGridBuilder().addGrid(models['RF'].numTrees, [20,50]).addGrid(models['RF'].maxDepth, [5,10]).build()\n",
    "}\n",
    "\n",
    "# Her model için küçük veriyle prova (Spark ML modelleri)\n",
    "results_spark_ml = []\n",
    "for name, clf in models.items():\n",
    "    print(f\"\\n⚙️ **{name}** modeli eğitimi (küçük PySpark veri)...\")\n",
    "    \n",
    "    # Spark ML Pipeline'ı oluşturma: label_indexer, cat_indexers, assembler ve sınıflandırıcıyı içerir\n",
    "    pipe_stages = [label_indexer] + cat_indexers + [assembler, clf]\n",
    "    pipe = Pipeline(stages=pipe_stages)\n",
    "    \n",
    "    tvs = TrainValidationSplit(\n",
    "        estimator=pipe,\n",
    "        estimatorParamMaps=grids[name],\n",
    "        evaluator=f1_eval,\n",
    "        trainRatio=0.8,\n",
    "        parallelism=1 # Paralelleştirmeyi sınırlayabiliriz\n",
    "    )\n",
    "    tvsModel = tvs.fit(small_train)\n",
    "    best = tvsModel.bestModel\n",
    "    \n",
    "    # small_val üzerinde değerlendirme\n",
    "    preds = best.transform(small_val)\n",
    "    f1 = f1_eval.evaluate(preds)\n",
    "    acc = acc_eval.evaluate(preds)\n",
    "    \n",
    "    print(f\"🏆 **{name}** — small_val F1: {f1:.4f}, Acc: {acc:.4f}\")\n",
    "    \n",
    "    # Modeli kaydet (Spark ML modellerini kaydetme yolu bu şekildedir)\n",
    "    path = f\"models/us_accidents_{name.lower()}_small_spark\"\n",
    "    best.write().overwrite().save(path)\n",
    "    print(f\"💾 **{name}** modeli kaydedildi → {path}\")\n",
    "    results_spark_ml.append((name, f1, acc))\n",
    "\n",
    "# %% [python]\n",
    "# 5) Sonuçların Karşılaştırılması ve En İyi Modelin Tüm Veride Eğitimi\n",
    "print(\"\\n--- Spark ML (PySpark) Modellerinin Küçük Verideki Sonuçları ---\")\n",
    "for res in results_spark_ml:\n",
    "    print(f\"Model: {res[0]}, F1: {res[1]:.4f}, Acc: {res[2]:.4f}\")\n",
    "\n",
    "# En iyi performansı gösteren modeli bulalım\n",
    "if results_spark_ml:\n",
    "    best_model_name = max(results_spark_ml, key=lambda item: item[1])[0] # F1 skoruna göre en iyisi\n",
    "    print(f\"\\n✨ Küçük veride en iyi performans gösteren Spark ML modeli: {best_model_name}\")\n",
    "\n",
    "    # En iyi modelin orijinal (daha geniş) tanımlamasını al\n",
    "    best_clf_full = models[best_model_name]\n",
    "\n",
    "    print(f\"\\n⚙️ Final **{best_model_name}** eğitimi tüm `train` verisi üzerinde…\")\n",
    "    \n",
    "    # Tüm train veri seti üzerinde eğitmek için yeni bir pipeline oluştur (parametreler aynı kalabilir)\n",
    "    pipe_full_train = Pipeline(stages=[label_indexer] + cat_indexers + [assembler, best_clf_full])\n",
    "    \n",
    "    # Tüm train veri seti üzerinde eğit\n",
    "    model_full_train = pipe_full_train.fit(train) # df_no_na yerine 'train' kullanıyoruz, test için ayırdık\n",
    "    \n",
    "    # Modeli kaydet\n",
    "    full_path = f\"models/us_accidents_{best_model_name.lower()}_full_spark\"\n",
    "    model_full_train.write().overwrite().save(full_path)\n",
    "    print(f\"💾 Final {best_model_name} modeli kaydedildi → {full_path}\")\n",
    "\n",
    "    # Son olarak, eğitilmiş modeli 'test' veri seti üzerinde değerlendir\n",
    "    print(f\"\\n📊 Final {best_model_name} modelinin 'test' verisi üzerinde değerlendirilmesi...\")\n",
    "    final_preds = model_full_train.transform(test)\n",
    "    final_f1 = f1_eval.evaluate(final_preds)\n",
    "    final_acc = acc_eval.evaluate(final_preds)\n",
    "    print(f\"🎉 Final {best_model_name} — Test Seti F1: {final_f1:.4f}, Acc: {final_acc:.4f}\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Spark ML modelleri için sonuç bulunamadı.\")\n",
    "\n",
    "# Spark Session'ı durdur\n",
    "spark.stop()\n",
    "print(\"\\nSpark Session durduruldu.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e040d3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af75743e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ df_no_na hazır: 5,217,919 kayıt, 22 sütun\n",
      "🔀 Bölme → train: 4,175,117, test: 1,042,802\n",
      "🔄 small_train: 668,070, small_val: 167,401\n"
     ]
    }
   ],
   "source": [
    "# %% [python]\n",
    "# 1) Spark ve Kütüphaneleri Başlat (Aynı kalır)\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, unix_timestamp, when, rand,\n",
    "    hour, dayofweek, month, year # Yeni tarih/saat fonksiyonları\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression, DecisionTreeClassifier,\n",
    "    RandomForestClassifier, GBTClassifier # GBTClassifier'ı OneVsRest ile deneyeceğiz\n",
    ")\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder, CrossValidator # CrossValidator eklendi\n",
    "from pyspark.ml.classification import OneVsRest # OneVsRest eklendi\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"US Accidents Severity Experiments - Improved\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\",\"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\",\"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# %% [python]\n",
    "# 2) Veri Hazırlığı (Yeni Özellik Mühendisliği Dahil)\n",
    "# 2.1) CSV'yi oku\n",
    "df = spark.read.csv(\"US_Accidents_March23.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 2.2) İstenmeyen sütunları düş\n",
    "drop_cols = [\n",
    "    \"ID\",\"Source\",\"Zipcode\",\"Timezone\",\"Airport_Code\",\"Amenity\",\"Bump\",\n",
    "    \"Give_Way\",\"No_Exit\",\"Railway\",\"Description\",\"County\",\"Roundabout\",\n",
    "    \"Station\",\"Stop\",\"Nautical_Twilight\",\"Astronomical_Twilight\",\"Country\"\n",
    "]\n",
    "df2 = df.drop(*drop_cols)\n",
    "\n",
    "# 2.3) Start/End → Duration hesapla ve Yeni Tarih/Saat Özellikleri Oluştur\n",
    "df3 = (\n",
    "    df2\n",
    "    .withColumn(\"Start_TS\", to_timestamp(col(\"Start_Time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .withColumn(\"End_TS\",   to_timestamp(col(\"End_Time\"),   \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .withColumn(\"Duration\",\n",
    "        ((unix_timestamp(col(\"End_TS\")) - unix_timestamp(col(\"Start_TS\"))) / 60)\n",
    "        .cast(DoubleType())\n",
    "    )\n",
    "    # Yeni tarih/saat özellikleri\n",
    "    .withColumn(\"HourOfDay\", hour(col(\"Start_TS\")))\n",
    "    .withColumn(\"DayOfWeek\", dayofweek(col(\"Start_TS\")))\n",
    "    .withColumn(\"Month\", month(col(\"Start_TS\")))\n",
    "    .withColumn(\"Year\", year(col(\"Start_TS\")))\n",
    "    .drop(\"Start_TS\",\"End_TS\",\"Start_Time\",\"End_Time\")\n",
    ")\n",
    "\n",
    "# 2.4) City/Street cardinality düşür\n",
    "def clean_column(df, column_name, top_n=32):\n",
    "    top_vals = [\n",
    "        r[column_name] for r in\n",
    "        df.groupBy(column_name).count()\n",
    "          .orderBy(col(\"count\").desc())\n",
    "          .limit(top_n)\n",
    "          .collect()\n",
    "    ]\n",
    "    return df.withColumn(\n",
    "        f\"{column_name}_Cleaned\",\n",
    "        when(col(column_name).isin(top_vals), col(column_name)).otherwise(\"Other\")\n",
    "    )\n",
    "\n",
    "for c in [\"City\",\"Street\"]:\n",
    "    df3 = clean_column(df3, c, top_n=32)\n",
    "\n",
    "# 2.5) Sütunları seç (Yeni özellikler dahil)\n",
    "selected_cols = [\n",
    "    \"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\", \"Precipitation(in)\", \"Wind_Chill(F)\", \"Traffic_Signal\",\n",
    "    \"Weather_Condition\", \"Wind_Direction\", \"Junction\", \"Duration\", \"Severity\",\n",
    "    \"Civil_Twilight\", \"Sunrise_Sunset\", \"State\", \"City_Cleaned\", \"Street_Cleaned\",\n",
    "    \"HourOfDay\", \"DayOfWeek\", \"Month\", \"Year\" # Yeni eklenen özellikler\n",
    "]\n",
    "df_selected = df3.select(*selected_cols)\n",
    "\n",
    "# 2.6) Kategorik ve feature listeleri (Yeni özellikler dahil)\n",
    "# Unutmayın: Spark ML StringIndexer'lar sadece string veya numerik (integer, double) sütunları alır.\n",
    "# 'HourOfDay', 'DayOfWeek', 'Month', 'Year' gibi sütunlar numerik olarak zaten uygun.\n",
    "# Ancak bu sütunların kategorik olarak muamele görmesini istiyorsanız, onları da StringIndexer'dan geçirmelisiniz.\n",
    "# Bu örnekte, onları numerik olarak bırakıp doğrudan assembler'a vereceğim.\n",
    "categorical_cols = [\n",
    "    \"Weather_Condition\",\"Wind_Direction\",\"Civil_Twilight\",\n",
    "    \"Sunrise_Sunset\",\"State\",\"City_Cleaned\",\"Street_Cleaned\"\n",
    "]\n",
    "\n",
    "numerical_cols_for_assembler = [\n",
    "    \"Temperature(F)\",\"Humidity(%)\",\"Pressure(in)\",\"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\",\"Precipitation(in)\",\"Wind_Chill(F)\",\"Traffic_Signal\",\n",
    "    \"Junction\", \"Duration\", # Mevcut numerik özellikler\n",
    "    \"HourOfDay\", \"DayOfWeek\", \"Month\", \"Year\" # Yeni numerik özellikler\n",
    "]\n",
    "\n",
    "feature_cols_for_assembler = numerical_cols_for_assembler + [c + \"_Idx\" for c in categorical_cols]\n",
    "\n",
    "\n",
    "# 2.7) NA at ve cache\n",
    "df_no_na = df_selected.dropna().cache()\n",
    "print(f\"✅ df_no_na hazır: {df_no_na.count():,} kayıt, {len(df_no_na.columns)} sütun\")\n",
    "\n",
    "# %% [python]\n",
    "# 3) Train/Test Bölme ve Küçük Alt Küme Oluşturma (Aynı kalır)\n",
    "train, test = df_no_na.randomSplit([0.8,0.2], seed=42)\n",
    "print(f\"🔀 Bölme → train: {train.count():,}, test: {test.count():,}\")\n",
    "\n",
    "small = train.sample(fraction=0.2, seed=42)\n",
    "small_train, small_val = small.randomSplit([0.8,0.2], seed=42)\n",
    "print(f\"🔄 small_train: {small_train.count():,}, small_val: {small_val.count():,}\")\n",
    "\n",
    "# %% [python]\n",
    "# 4) Spark ML Modelleri Tanımlama ve Küçük Veride Deneme (Genişletilmiş Hiperparametreler)\n",
    "# Label indexer: Severity'yi 0'dan başlayan tamsayı indekslere dönüştürecek\n",
    "label_indexer = StringIndexer(inputCol=\"Severity\", outputCol=\"label\", handleInvalid=\"keep\")\n",
    "label_col_name_for_spark_models = \"label\"\n",
    "\n",
    "# Kategorik özellikler için indexer'lar\n",
    "cat_indexers = [StringIndexer(inputCol=c, outputCol=c+\"_Idx\", handleInvalid=\"keep\")\n",
    "                for c in categorical_cols]\n",
    "# Tüm özellikleri tek bir vektörde birleştiren assembler\n",
    "assembler = VectorAssembler(inputCols=feature_cols_for_assembler, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "\n",
    "# Değerlendirme metrikleri\n",
    "f1_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol=label_col_name_for_spark_models, predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "acc_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol=label_col_name_for_spark_models, predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01d29051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ **LR** modeli eğitimi (küçük PySpark veri, CrossValidation)...\n",
      "🏆 **LR** — small_val F1: 0.7944, Acc: 0.8519\n",
      "💾 **LR** modeli kaydedildi → models/us_accidents_lr_optimized_small_spark\n",
      "\n",
      "⚙️ **DT** modeli eğitimi (küçük PySpark veri, CrossValidation)...\n",
      "🏆 **DT** — small_val F1: 0.8515, Acc: 0.8715\n",
      "💾 **DT** modeli kaydedildi → models/us_accidents_dt_optimized_small_spark\n",
      "\n",
      "⚙️ **RF** modeli eğitimi (küçük PySpark veri, CrossValidation)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aslay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] Varolan bir bağlantı uzaktaki bir ana bilgisayar tarafından zorla kapatıldı\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] Hedef makine etkin olarak reddettiğinden bağlantı kurulamadı",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[3], line 45\u001b[0m\n\u001b[0;32m     37\u001b[0m cv \u001b[38;5;241m=\u001b[39m CrossValidator(\n\u001b[0;32m     38\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mpipe,\n\u001b[0;32m     39\u001b[0m     estimatorParamMaps\u001b[38;5;241m=\u001b[39mgrids[name],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     44\u001b[0m )\n\u001b[1;32m---> 45\u001b[0m cvModel \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmall_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m best \u001b[38;5;241m=\u001b[39m cvModel\u001b[38;5;241m.\u001b[39mbestModel\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[0;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[0;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[0;32m    846\u001b[0m )\n\u001b[1;32m--> 847\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubModel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap_unordered\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics_all\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[1;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[1;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit.<locals>.<lambda>\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[0;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[0;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[0;32m    846\u001b[0m )\n\u001b[1;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, tasks):\n\u001b[0;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\util.py:342\u001b[0m, in \u001b[0;36minheritable_thread_target.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39msetLocalProperties(properties)\n\u001b[1;32m--> 342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\tuning.py:113\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[1;34m()\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingleTask\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, Transformer]:\n\u001b[1;32m--> 113\u001b[0m     index, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodelIter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\base.py:98\u001b[0m, in \u001b[0;36m_FitMultipleIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfitSingleModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\base.py:156\u001b[0m, in \u001b[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[1;34m(index)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfitSingleModel\u001b[39m(index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m M:\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparamMaps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(10061, 'Hedef makine etkin olarak reddettiğinden bağlantı kurulamadı', None, 10061, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2179\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2176\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[0;32m   2177\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2179\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[0;32m   2181\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[0;32m   2182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\ipykernel\\zmqshell.py:559\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[1;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[0;32m    553\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    554\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    556\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    560\u001b[0m }\n\u001b[0;32m    562\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[0;32m    563\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[1;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[1;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] Hedef makine etkin olarak reddettiğinden bağlantı kurulamadı"
     ]
    }
   ],
   "source": [
    "# Spark ML Modellerinin Tanımlanması\n",
    "# GBTClassifier'ı OneVsRest ile çoklu sınıflandırma için ekleyelim\n",
    "gbt = GBTClassifier(labelCol=label_col_name_for_spark_models, featuresCol=\"features\", maxIter=20, maxBins=256)\n",
    "ovr = OneVsRest(classifier=gbt, labelCol=label_col_name_for_spark_models, featuresCol=\"features\")\n",
    "\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(labelCol=label_col_name_for_spark_models, featuresCol=\"features\", maxIter=50), # MaxIter artırıldı\n",
    "    \"DT\": DecisionTreeClassifier(labelCol=label_col_name_for_spark_models, featuresCol=\"features\", maxBins=256),\n",
    "    \"RF\": RandomForestClassifier(labelCol=label_col_name_for_spark_models, featuresCol=\"features\", numTrees=100, maxBins=256), # numTrees artırıldı\n",
    "}\n",
    "\n",
    "# Hiperparametre ızgarası (Daha geniş ve fazla kombinasyonlar)\n",
    "grids = {\n",
    "    \"LR\": ParamGridBuilder() \\\n",
    "        .addGrid(models['LR'].regParam, [0.001, 0.01, 0.1]) \\\n",
    "        .addGrid(models['LR'].elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "        .build(),\n",
    "    \"DT\": ParamGridBuilder() \\\n",
    "        .addGrid(models['DT'].maxDepth, [5, 10, 15]) \\\n",
    "        .addGrid(models['DT'].minInstancesPerNode, [1, 5]) \\\n",
    "        .build(),\n",
    "    \"RF\": ParamGridBuilder() \\\n",
    "        .addGrid(models['RF'].numTrees, [50, 100, 150]) \\\n",
    "        .addGrid(models['RF'].maxDepth, [10, 15, 20]) \\\n",
    "        .build()\n",
    "}\n",
    "\n",
    "# Her model için küçük veriyle prova (CrossValidator kullanıyoruz)\n",
    "results_spark_ml = []\n",
    "for name, clf in models.items():\n",
    "    print(f\"\\n⚙️ **{name}** modeli eğitimi (küçük PySpark veri, CrossValidation)...\")\n",
    "    \n",
    "    pipe_stages = [label_indexer] + cat_indexers + [assembler, clf]\n",
    "    pipe = Pipeline(stages=pipe_stages)\n",
    "    \n",
    "    # CrossValidator kullanımı: numFolds=3 veya 5 genellikle iyidir\n",
    "    cv = CrossValidator(\n",
    "        estimator=pipe,\n",
    "        estimatorParamMaps=grids[name],\n",
    "        evaluator=f1_eval, # Değerlendirme için F1 skoru\n",
    "        numFolds=3, # 3 katlı çapraz doğrulama\n",
    "        parallelism=1, # Spark 3.0+ ile paralel eğitim. CPU çekirdek sayınıza göre artırılabilir\n",
    "        seed=42\n",
    "    )\n",
    "    cvModel = cv.fit(small_train)\n",
    "    best = cvModel.bestModel\n",
    "    \n",
    "    # small_val üzerinde değerlendirme\n",
    "    preds = best.transform(small_val)\n",
    "    f1 = f1_eval.evaluate(preds)\n",
    "    acc = acc_eval.evaluate(preds)\n",
    "    \n",
    "    print(f\"🏆 **{name}** — small_val F1: {f1:.4f}, Acc: {acc:.4f}\")\n",
    "    \n",
    "    # Modeli kaydet\n",
    "    path = f\"models/us_accidents_{name.lower()}_optimized_small_spark\"\n",
    "    best.write().overwrite().save(path)\n",
    "    print(f\"💾 **{name}** modeli kaydedildi → {path}\")\n",
    "    results_spark_ml.append((name, f1, acc))\n",
    "\n",
    "# %% [python]\n",
    "# 5) Sonuçların Karşılaştırılması ve En İyi Modelin Tüm Veride Eğitimi\n",
    "print(\"\\n--- Spark ML (PySpark) Modellerinin Küçük Verideki Sonuçları ---\")\n",
    "for res in results_spark_ml:\n",
    "    print(f\"Model: {res[0]}, F1: {res[1]:.4f}, Acc: {res[2]:.4f}\")\n",
    "\n",
    "# En iyi performansı gösteren modeli bulalım\n",
    "if results_spark_ml:\n",
    "    best_model_name = max(results_spark_ml, key=lambda item: item[1])[0] # F1 skoruna göre en iyisi\n",
    "    print(f\"\\n✨ Küçük veride en iyi performans gösteren Spark ML modeli: {best_model_name}\")\n",
    "\n",
    "    # En iyi modelin orijinal (daha geniş) tanımlamasını al\n",
    "    best_clf_full = models[best_model_name]\n",
    "\n",
    "    print(f\"\\n⚙️ Final **{best_model_name}** eğitimi tüm `train` verisi üzerinde…\")\n",
    "    \n",
    "    # Tüm train veri seti üzerinde eğitmek için yeni bir pipeline oluştur (parametreler aynı kalabilir)\n",
    "    pipe_full_train = Pipeline(stages=[label_indexer] + cat_indexers + [assembler, best_clf_full])\n",
    "    \n",
    "    # Tüm train veri seti üzerinde eğit\n",
    "    model_full_train = pipe_full_train.fit(train) # df_no_na yerine 'train' kullanıyoruz, test için ayırdık\n",
    "    \n",
    "    # Modeli kaydet\n",
    "    full_path = f\"models/us_accidents_{best_model_name.lower()}_optimized_full_spark\"\n",
    "    model_full_train.write().overwrite().save(full_path)\n",
    "    print(f\"💾 Final {best_model_name} modeli kaydedildi → {full_path}\")\n",
    "\n",
    "    # Son olarak, eğitilmiş modeli 'test' veri seti üzerinde değerlendir\n",
    "    print(f\"\\n📊 Final {best_model_name} modelinin 'test' verisi üzerinde değerlendirilmesi...\")\n",
    "    final_preds = model_full_train.transform(test)\n",
    "    final_f1 = f1_eval.evaluate(final_preds)\n",
    "    final_acc = acc_eval.evaluate(final_preds)\n",
    "    print(f\"🎉 Final {best_model_name} — Test Seti F1: {final_f1:.4f}, Acc: {final_acc:.4f}\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Spark ML modelleri için sonuç bulunamadı.\")\n",
    "\n",
    "# Spark Session'ı durdur\n",
    "spark.stop()\n",
    "print(\"\\nSpark Session durduruldu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ddd624d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ df_no_na hazır: 5,217,919 kayıt, 22 sütun\n",
      "🔀 Bölme → train: 4,175,117, test: 1,042,802\n",
      "🔄 small_train: 334,092, small_val: 83,858\n",
      "\n",
      "⚙️ **RandomForestClassifier** modeli eğitimi (küçük PySpark veri, CrossValidation)...\n",
      "🏆 **RandomForestClassifier** — small_val F1: 0.8401, Acc: 0.8737\n",
      "💾 **RandomForestClassifier** modeli kaydedildi → models/us_accidents_rf_optimized_small_spark\n",
      "\n",
      "--- RandomForestClassifier Modelinin Küçük Verideki Sonuçları ---\n",
      "Model: RandomForestClassifier, F1: 0.8401, Acc: 0.8737\n",
      "\n",
      "⚙️ Final **RandomForestClassifier** eğitimi tüm `train` verisi üzerinde…\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "keywords must be strings",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 212\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;66;03m# Yeni bir RandomForestClassifier estimator'ı oluştur ve en iyi parametreleri ata\u001b[39;00m\n\u001b[0;32m    207\u001b[0m final_rf_estimator \u001b[38;5;241m=\u001b[39m RandomForestClassifier(\n\u001b[0;32m    208\u001b[0m     labelCol\u001b[38;5;241m=\u001b[39mlabel_col_name_for_spark_models,\n\u001b[0;32m    209\u001b[0m     featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    210\u001b[0m     maxBins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    211\u001b[0m )\n\u001b[1;32m--> 212\u001b[0m \u001b[43mfinal_rf_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetParams\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbest_rf_params\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# En iyi parametreleri set et\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;66;03m# Tüm train veri seti üzerinde eğitmek için yeni bir pipeline oluştur\u001b[39;00m\n\u001b[0;32m    215\u001b[0m pipe_full_train_rf \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39m[label_indexer] \u001b[38;5;241m+\u001b[39m cat_indexers \u001b[38;5;241m+\u001b[39m [assembler, final_rf_estimator])\n",
      "\u001b[1;31mTypeError\u001b[0m: keywords must be strings"
     ]
    }
   ],
   "source": [
    "# %% [python]\n",
    "# 1) Spark ve Kütüphaneleri Başlat\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, unix_timestamp, when, rand,\n",
    "    hour, dayofweek, month, year\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import os\n",
    "\n",
    "# Spark Session'ı başlat ve bellek ayarlarını artır\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"US Accidents RF Experiment - PySpark MLlib\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\",\"16g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\",\"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# %% [python]\n",
    "# 2) Veri Hazırlığı\n",
    "# 2.1) CSV'yi oku\n",
    "df = spark.read.csv(\"US_Accidents_March23.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 2.2) İstenmeyen sütunları düş\n",
    "drop_cols = [\n",
    "    \"ID\",\"Source\",\"Zipcode\",\"Timezone\",\"Airport_Code\",\"Amenity\",\"Bump\",\n",
    "    \"Give_Way\",\"No_Exit\",\"Railway\",\"Description\",\"County\",\"Roundabout\",\n",
    "    \"Station\",\"Stop\",\"Nautical_Twilight\",\"Astronomical_Twilight\",\"Country\"\n",
    "]\n",
    "df2 = df.drop(*drop_cols)\n",
    "\n",
    "# 2.3) Start/End → Duration hesapla ve Yeni Tarih/Saat Özellikleri Oluştur\n",
    "df3 = (\n",
    "    df2\n",
    "    .withColumn(\"Start_TS\", to_timestamp(col(\"Start_Time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .withColumn(\"End_TS\",   to_timestamp(col(\"End_Time\"),   \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .withColumn(\"Duration\",\n",
    "        ((unix_timestamp(col(\"End_TS\")) - unix_timestamp(col(\"Start_TS\"))) / 60)\n",
    "        .cast(DoubleType())\n",
    "    )\n",
    "    # Yeni tarih/saat özellikleri\n",
    "    .withColumn(\"HourOfDay\", hour(col(\"Start_TS\")))\n",
    "    .withColumn(\"DayOfWeek\", dayofweek(col(\"Start_TS\")))\n",
    "    .withColumn(\"Month\", month(col(\"Start_TS\")))\n",
    "    .withColumn(\"Year\", year(col(\"Start_TS\")))\n",
    "    .drop(\"Start_TS\",\"End_TS\",\"Start_Time\",\"End_Time\")\n",
    ")\n",
    "\n",
    "# 2.4) City/Street cardinality düşür\n",
    "def clean_column(df, column_name, top_n=32):\n",
    "    top_vals = [\n",
    "        r[column_name] for r in\n",
    "        df.groupBy(column_name).count()\n",
    "          .orderBy(col(\"count\").desc())\n",
    "          .limit(top_n)\n",
    "          .collect()\n",
    "    ]\n",
    "    return df.withColumn(\n",
    "        f\"{column_name}_Cleaned\",\n",
    "        when(col(column_name).isin(top_vals), col(column_name)).otherwise(\"Other\")\n",
    "    )\n",
    "\n",
    "for c in [\"City\",\"Street\"]:\n",
    "    df3 = clean_column(df3, c, top_n=32)\n",
    "\n",
    "# 2.5) Sütunları seç\n",
    "selected_cols = [\n",
    "    \"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\", \"Precipitation(in)\", \"Wind_Chill(F)\", \"Traffic_Signal\",\n",
    "    \"Weather_Condition\", \"Wind_Direction\", \"Junction\", \"Duration\", \"Severity\",\n",
    "    \"Civil_Twilight\", \"Sunrise_Sunset\", \"State\", \"City_Cleaned\", \"Street_Cleaned\",\n",
    "    \"HourOfDay\", \"DayOfWeek\", \"Month\", \"Year\"\n",
    "]\n",
    "df_selected = df3.select(*selected_cols)\n",
    "\n",
    "# 2.6) Kategorik ve feature listeleri\n",
    "categorical_cols = [\n",
    "    \"Weather_Condition\",\"Wind_Direction\",\"Civil_Twilight\",\n",
    "    \"Sunrise_Sunset\",\"State\",\"City_Cleaned\",\"Street_Cleaned\"\n",
    "]\n",
    "numerical_cols_for_assembler = [\n",
    "    \"Temperature(F)\",\"Humidity(%)\",\"Pressure(in)\",\"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\",\"Precipitation(in)\",\"Wind_Chill(F)\",\"Traffic_Signal\",\n",
    "    \"Junction\", \"Duration\",\n",
    "    \"HourOfDay\", \"DayOfWeek\", \"Month\", \"Year\"\n",
    "]\n",
    "\n",
    "feature_cols_for_assembler = numerical_cols_for_assembler + [c + \"_Idx\" for c in categorical_cols]\n",
    "\n",
    "# 2.7) NA at ve cache\n",
    "df_no_na = df_selected.dropna().cache()\n",
    "print(f\"✅ df_no_na hazır: {df_no_na.count():,} kayıt, {len(df_no_na.columns)} sütun\")\n",
    "\n",
    "# %% [python]\n",
    "# 3) Train/Test Bölme ve Küçük Alt Küme Oluşturma\n",
    "train, test = df_no_na.randomSplit([0.8,0.2], seed=42)\n",
    "print(f\"🔀 Bölme → train: {train.count():,}, test: {test.count():,}\")\n",
    "\n",
    "# Küçük alt küme (daha küçük bir örneklem deneyebiliriz)\n",
    "small = train.sample(fraction=0.1, seed=42) # Örneklem oranını düşürdük\n",
    "small_train, small_val = small.randomSplit([0.8,0.2], seed=42)\n",
    "print(f\"🔄 small_train: {small_train.count():,}, small_val: {small_val.count():,}\")\n",
    "\n",
    "# %% [python]\n",
    "# 4) RandomForestClassifier Model Tanımlama ve Küçük Veride Deneme\n",
    "\n",
    "# Label indexer: Severity'yi 0'dan başlayan tamsayı indekslere dönüştürecek\n",
    "label_indexer = StringIndexer(inputCol=\"Severity\", outputCol=\"label\", handleInvalid=\"keep\")\n",
    "label_col_name_for_spark_models = \"label\"\n",
    "\n",
    "# Kategorik özellikler için indexer'lar\n",
    "cat_indexers = [StringIndexer(inputCol=c, outputCol=c+\"_Idx\", handleInvalid=\"keep\")\n",
    "                for c in categorical_cols]\n",
    "# Tüm özellikleri tek bir vektörde birleştiren assembler\n",
    "assembler = VectorAssembler(inputCols=feature_cols_for_assembler, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "\n",
    "# Değerlendirme metrikleri\n",
    "f1_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol=label_col_name_for_spark_models, predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "acc_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol=label_col_name_for_spark_models, predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "# RandomForestClassifier Modelinin Tanımlanması\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    labelCol=label_col_name_for_spark_models,\n",
    "    featuresCol=\"features\",\n",
    "    maxBins=256\n",
    ")\n",
    "\n",
    "# Hiperparametre ızgarası (RandomForest için ayarlanmış)\n",
    "# numTrees ve maxDepth değerlerini daha düşük aralıklarla deniyoruz\n",
    "grids_rf = ParamGridBuilder() \\\n",
    "    .addGrid(rf_classifier.numTrees, [50, 75]) \\\n",
    "    .addGrid(rf_classifier.maxDepth, [10, 15]) \\\n",
    "    .build()\n",
    "\n",
    "print(f\"\\n⚙️ **RandomForestClassifier** modeli eğitimi (küçük PySpark veri, CrossValidation)...\")\n",
    "\n",
    "# Pipeline oluşturma\n",
    "pipe_rf = Pipeline(stages=[label_indexer] + cat_indexers + [assembler, rf_classifier])\n",
    "\n",
    "# CrossValidator kullanımı\n",
    "cv_rf = CrossValidator(\n",
    "    estimator=pipe_rf,\n",
    "    estimatorParamMaps=grids_rf,\n",
    "    evaluator=f1_eval,\n",
    "    numFolds=3, # 3 katlı çapraz doğrulama\n",
    "    parallelism=1,\n",
    "    seed=42\n",
    ")\n",
    "cvModel_rf = cv_rf.fit(small_train)\n",
    "best_rf_model = cvModel_rf.bestModel\n",
    "\n",
    "# small_val üzerinde değerlendirme\n",
    "preds_rf = best_rf_model.transform(small_val)\n",
    "f1_rf = f1_eval.evaluate(preds_rf)\n",
    "acc_rf = acc_eval.evaluate(preds_rf)\n",
    "\n",
    "print(f\"🏆 **RandomForestClassifier** — small_val F1: {f1_rf:.4f}, Acc: {acc_rf:.4f}\")\n",
    "\n",
    "# Modeli kaydet\n",
    "path_rf = f\"models/us_accidents_rf_optimized_small_spark\"\n",
    "best_rf_model.write().overwrite().save(path_rf)\n",
    "print(f\"💾 **RandomForestClassifier** modeli kaydedildi → {path_rf}\")\n",
    "\n",
    "\n",
    "# %% [python]\n",
    "# 5) RandomForestClassifier için Sonuçları Görüntüleme ve Tüm Veride Eğitme\n",
    "print(\"\\n--- RandomForestClassifier Modelinin Küçük Verideki Sonuçları ---\")\n",
    "print(f\"Model: RandomForestClassifier, F1: {f1_rf:.4f}, Acc: {acc_rf:.4f}\")\n",
    "\n",
    "print(f\"\\n⚙️ Final **RandomForestClassifier** eğitimi tüm `train` verisi üzerinde…\")\n",
    "\n",
    "# En iyi modelin orijinal tanımlamasını al (CrossValidator'dan gelen en iyi parametrelerle)\n",
    "# cvModel_rf.bestModel zaten en iyi parametrelerle eğitilmiş pipeline'dır.\n",
    "# Bu pipeline'ı doğrudan tüm train verisi üzerinde eğitebiliriz.\n",
    "# Ancak, RandomForestClassifier'ın kendisini alıp yeni bir pipeline oluşturmak daha temiz olabilir.\n",
    "# best_rf_model.stages[-1] bize RandomForestClassifier modelini verir.\n",
    "# Yeni bir RandomForestClassifier nesnesi oluşturup en iyi parametreleri set edebiliriz.\n",
    "# Veya daha basiti, best_rf_model'ın kendisi zaten bir PipelineModel olduğu için,\n",
    "# onu doğrudan yeni bir Pipeline'a koymak yerine, onun estimator'ını alıp yeni bir Pipeline oluşturabiliriz.\n",
    "\n",
    "# En iyi RandomForestClassifier estimator'ını almak için:\n",
    "# best_rf_estimator = best_rf_model.stages[-1] # Bu, eğitilmiş bir modeldir (RandomForestClassificationModel)\n",
    "\n",
    "# Aslında, CrossValidator'ın fit ettiği `bestModel` zaten bir `PipelineModel`'dır.\n",
    "# Bu `PipelineModel`'ın son aşaması, en iyi parametrelerle eğitilmiş `RandomForestClassificationModel`'dır.\n",
    "# `PipelineModel`'ı doğrudan `train` üzerinde tekrar `fit` edemeyiz.\n",
    "# Yapmamız gereken, `best_rf_model`'ın içindeki `RandomForestClassifier`'ın en iyi parametrelerini alıp,\n",
    "# yeni bir `RandomForestClassifier` (Estimator) oluşturmak ve onu tüm `train` üzerinde eğitmektir.\n",
    "\n",
    "# En iyi RF parametrelerini al\n",
    "best_rf_params = best_rf_model.stages[-1].extractParamMap()\n",
    "\n",
    "# Yeni bir RandomForestClassifier estimator'ı oluştur ve en iyi parametreleri ata\n",
    "final_rf_estimator = RandomForestClassifier(\n",
    "    labelCol=label_col_name_for_spark_models,\n",
    "    featuresCol=\"features\",\n",
    "    maxBins=256\n",
    ")\n",
    "final_rf_estimator.setParams(**best_rf_params) # En iyi parametreleri set et\n",
    "\n",
    "# Tüm train veri seti üzerinde eğitmek için yeni bir pipeline oluştur\n",
    "pipe_full_train_rf = Pipeline(stages=[label_indexer] + cat_indexers + [assembler, final_rf_estimator])\n",
    "\n",
    "# Tüm train veri seti üzerinde eğit\n",
    "model_full_train_rf = pipe_full_train_rf.fit(train)\n",
    "\n",
    "# Modeli kaydet\n",
    "full_path_rf = \"models/us_accidents_rf_optimized_full_spark\"\n",
    "model_full_train_rf.write().overwrite().save(full_path_rf)\n",
    "print(f\"💾 Final RandomForestClassifier modeli kaydedildi → {full_path_rf}\")\n",
    "\n",
    "# Son olarak, eğitilmiş modeli 'test' veri seti üzerinde değerlendir\n",
    "print(f\"\\n📊 Final RandomForestClassifier modelinin 'test' verisi üzerinde değerlendirilmesi...\")\n",
    "final_preds_rf = model_full_train_rf.transform(test)\n",
    "final_f1_rf = f1_eval.evaluate(final_preds_rf)\n",
    "final_acc_rf = acc_eval.evaluate(final_preds_rf)\n",
    "print(f\"🎉 Final RandomForestClassifier — Test Seti F1: {final_f1_rf:.4f}, Acc: {final_acc_rf:.4f}\")\n",
    "\n",
    "# Spark Session'ı durdur\n",
    "spark.stop()\n",
    "print(\"\\nSpark Session durduruldu.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9a984b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RandomForestClassifier Modelinin Küçük Verideki Sonuçları ---\n",
      "Model: RandomForestClassifier, F1: 0.8401, Acc: 0.8737\n",
      "\n",
      "⚙️ Final **RandomForestClassifier** eğitimi tüm `train` verisi üzerinde…\n",
      "💾 Final RandomForestClassifier modeli kaydedildi → models/us_accidents_rf_optimized_full_spark\n",
      "\n",
      "📊 Final RandomForestClassifier modelinin 'test' verisi üzerinde değerlendirilmesi...\n"
     ]
    }
   ],
   "source": [
    "# %% [python]\n",
    "# 5) RandomForestClassifier için Sonuçları Görüntüleme ve Tüm Veride Eğitme\n",
    "print(\"\\n--- RandomForestClassifier Modelinin Küçük Verideki Sonuçları ---\")\n",
    "print(f\"Model: RandomForestClassifier, F1: {f1_rf:.4f}, Acc: {acc_rf:.4f}\")\n",
    "\n",
    "print(f\"\\n⚙️ Final **RandomForestClassifier** eğitimi tüm `train` verisi üzerinde…\")\n",
    "\n",
    "# En iyi RF parametrelerini al\n",
    "# best_rf_model.stages[-1] bize eğitilmiş RandomForestClassificationModel'ı verir.\n",
    "# extractParamMap() methodu Param objeleri içeren bir dictionary döndürür.\n",
    "best_rf_params = best_rf_model.stages[-1].extractParamMap()\n",
    "\n",
    "# TypeError: keywords must be strings hatasını çözmek için:\n",
    "# Param objelerini string isimlerine dönüştürüyoruz.\n",
    "# Param objelerinin 'name' özelliği string ismini verir.\n",
    "string_keyed_rf_params = {param.name: value for param, value in best_rf_params.items()}\n",
    "\n",
    "\n",
    "# Yeni bir RandomForestClassifier estimator'ı oluştur ve en iyi parametreleri ata\n",
    "final_rf_estimator = RandomForestClassifier(\n",
    "    labelCol=label_col_name_for_spark_models,\n",
    "    featuresCol=\"features\",\n",
    "    maxBins=256\n",
    ")\n",
    "# Parametreleri set ederken artık string anahtarlı dictionary kullanıyoruz.\n",
    "final_rf_estimator.setParams(**string_keyed_rf_params)\n",
    "\n",
    "# Tüm train veri seti üzerinde eğitmek için yeni bir pipeline oluştur\n",
    "pipe_full_train_rf = Pipeline(stages=[label_indexer] + cat_indexers + [assembler, final_rf_estimator])\n",
    "\n",
    "# Tüm train veri seti üzerinde eğit\n",
    "model_full_train_rf = pipe_full_train_rf.fit(train)\n",
    "\n",
    "# Modeli kaydet\n",
    "full_path_rf = \"models/us_accidents_rf_optimized_full_spark\"\n",
    "model_full_train_rf.write().overwrite().save(full_path_rf)\n",
    "print(f\"💾 Final RandomForestClassifier modeli kaydedildi → {full_path_rf}\")\n",
    "\n",
    "# Son olarak, eğitilmiş modeli 'test' veri seti üzerinde değerlendir\n",
    "print(f\"\\n📊 Final RandomForestClassifier modelinin 'test' verisi üzerinde değerlendirilmesi...\")\n",
    "final_preds_rf = model_full_train_rf.transform(test)\n",
    "final_f1_rf = f1_eval.evaluate(final_preds_rf)\n",
    "final_acc_rf = acc_eval.evaluate(final_preds_rf)\n",
    "print(f\"🎉 Final RandomForestClassifier — Test Seti F1: {final_f1_rf:.4f}, Acc: {final_acc_rf:.4f}\")\n",
    "\n",
    "# Spark Session'ı durdur\n",
    "spark.stop()\n",
    "print(\"\\nSpark Session durduruldu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b55b00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\aslay\\bert_env\\lib\\site-packages (3.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\aslay\\bert_env\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\aslay\\bert_env\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44f829e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ MLP-TVSplit (küçük veri) eğitimi başlıyor…\n",
      "🏆 MLP — small_val F1: 0.7860, Acc: 0.8535\n",
      "✅ MLP small-model kaydedildi → models/us_accidents_mlp_small\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# MLP on small split – full standalone cell\n",
    "# ---------------------------------------------\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature     import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation    import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning        import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.ml                import Pipeline\n",
    "\n",
    "# assume you already have:\n",
    "#   spark       = SparkSession.builder.getOrCreate()\n",
    "#   df_no_na    = your cleaned-and-NA-dropped DataFrame\n",
    "#   small_train, small_val = your 80/20 split of a 20% sample of train\n",
    "\n",
    "# 1) re-define your feature / categorical lists\n",
    "numeric_cols = [\n",
    "    \"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\",\n",
    "    \"Visibility(mi)\", \"Wind_Speed(mph)\", \"Precipitation(in)\",\n",
    "    \"Wind_Chill(F)\", \"Traffic_Signal\"\n",
    "]\n",
    "\n",
    "categorical_cols = [\n",
    "    \"Weather_Condition\", \"Wind_Direction\",\n",
    "    \"Civil_Twilight\", \"Sunrise_Sunset\",\n",
    "    \"State\", \"City_Cleaned\", \"Street_Cleaned\"\n",
    "]\n",
    "\n",
    "# build feature_cols for assembler\n",
    "feature_cols = numeric_cols + [c + \"_Idx\" for c in categorical_cols]\n",
    "\n",
    "# 2) StringIndexers for label + categories\n",
    "label_indexer = StringIndexer(\n",
    "    inputCol=\"Severity\", outputCol=\"label\", handleInvalid=\"keep\"\n",
    ")\n",
    "cat_indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=c + \"_Idx\", handleInvalid=\"keep\")\n",
    "    for c in categorical_cols\n",
    "]\n",
    "\n",
    "# 3) Vector assembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "# 4) Layers: [input_dim, hidden1, hidden2, output_dim]\n",
    "num_classes = df_no_na.select(\"Severity\").distinct().count()\n",
    "layers = [ len(feature_cols), 64, 32, num_classes ]\n",
    "\n",
    "# 5) define the MLP\n",
    "mlp = MultilayerPerceptronClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    layers=layers,\n",
    "    maxIter=100,\n",
    "    blockSize=128,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# 6) param grid\n",
    "paramGrid_mlp = (ParamGridBuilder()\n",
    "    .addGrid(mlp.maxIter,  [50, 100])\n",
    "    .addGrid(mlp.stepSize, [0.03, 0.1])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# 7) pipeline\n",
    "pipe_mlp = Pipeline(stages=[\n",
    "    label_indexer,\n",
    "    *cat_indexers,\n",
    "    assembler,\n",
    "    mlp\n",
    "])\n",
    "\n",
    "# 8) evaluator & TVS\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "\n",
    "tvs_mlp = TrainValidationSplit(\n",
    "    estimator          = pipe_mlp,\n",
    "    estimatorParamMaps = paramGrid_mlp,\n",
    "    evaluator          = evaluator,\n",
    "    trainRatio         = 0.8,\n",
    "    parallelism        = 1\n",
    ")\n",
    "\n",
    "# 9) fit & eval\n",
    "print(\"⚙️ MLP-TVSplit (küçük veri) eğitimi başlıyor…\")\n",
    "tvsModel_mlp = tvs_mlp.fit(small_train)\n",
    "best_mlp    = tvsModel_mlp.bestModel\n",
    "\n",
    "preds_mlp = best_mlp.transform(small_val)\n",
    "f1_mlp  = evaluator.evaluate(preds_mlp)\n",
    "acc_mlp = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ").evaluate(preds_mlp)\n",
    "\n",
    "print(f\"🏆 MLP — small_val F1: {f1_mlp:.4f}, Acc: {acc_mlp:.4f}\")\n",
    "\n",
    "# 10) save\n",
    "best_mlp.write().overwrite().save(\"models/us_accidents_mlp_small\")\n",
    "print(\"✅ MLP small-model kaydedildi → models/us_accidents_mlp_small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7259ef94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d11440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Yüklenen küçük-model LR parametreleri → regParam=0.001, elasticNetParam=0.0\n",
      "🔀 Train/Test → 4,175,117/1,042,802\n",
      "🎉 Final LR — Test F1: 0.7921, Acc: 0.8505\n",
      "💾 Final LR modeli kaydedildi → models/us_accidents_lr_final_full_spark\n"
     ]
    }
   ],
   "source": [
    "# %% [python]\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, unix_timestamp, when, hour, dayofweek, month, year\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 1) SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Final LR Full Train\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\",\"16g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\",\"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2) En iyi küçük-model LR stage’ını yükle\n",
    "small_stage_path = \"models/us_accidents_lr_optimized_small_spark/stages/09_LogisticRegression_e69b6dea7a0b\"\n",
    "best_lr_small    = LogisticRegressionModel.load(small_stage_path)\n",
    "best_reg         = best_lr_small.getRegParam()\n",
    "best_enet        = best_lr_small.getElasticNetParam()\n",
    "print(f\"🔍 Yüklenen küçük-model LR parametreleri → regParam={best_reg}, elasticNetParam={best_enet}\")\n",
    "\n",
    "# 3) Veri Hazırlığı (notebook’unuzdaki adımları aynen buraya yapıştırın)\n",
    "df = spark.read.csv(\"US_Accidents_March23.csv\", header=True, inferSchema=True)\n",
    "# → drop, Duration, date‐time eklemeleri, clean_column, selected_cols vs.\n",
    "# … (aynı selected_cols, categorical_cols, numerical_cols tanımlarınız)\n",
    "df_no_na = df_selected.dropna().cache()\n",
    "\n",
    "# 4) Train/Test böl ve sayaç yazdır\n",
    "train, test = df_no_na.randomSplit([0.8,0.2], seed=42)\n",
    "print(f\"🔀 Train/Test → {train.count():,}/{test.count():,}\")\n",
    "\n",
    "# 5) Pipeline bileşenleri\n",
    "label_indexer = StringIndexer(inputCol=\"Severity\", outputCol=\"label\", handleInvalid=\"keep\")\n",
    "cat_indexers  = [StringIndexer(inputCol=c, outputCol=c+\"_Idx\", handleInvalid=\"keep\")\n",
    "                 for c in categorical_cols]\n",
    "assembler      = VectorAssembler(\n",
    "    inputCols=numerical_cols_for_assembler + [c+\"_Idx\" for c in categorical_cols],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "# 6) Final LogisticRegression Estimator (küçük modelden gelen parametrelerle)\n",
    "lr_final = LogisticRegression(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    maxIter=20,\n",
    "    regParam=best_reg,\n",
    "    elasticNetParam=best_enet\n",
    ")\n",
    "\n",
    "pipe_final = Pipeline(stages=[label_indexer] + cat_indexers + [assembler, lr_final])\n",
    "\n",
    "# 7) Tam train set ile eğit\n",
    "model_full_lr = pipe_final.fit(train)\n",
    "\n",
    "# 8) Test set üzerinde değerlendir\n",
    "f1_eval  = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "acc_eval = f1_eval.copy({f1_eval.metricName: \"accuracy\"})\n",
    "\n",
    "preds     = model_full_lr.transform(test)\n",
    "f1_test   = f1_eval.evaluate(preds)\n",
    "acc_test  = acc_eval.evaluate(preds)\n",
    "print(f\"🎉 Final LR — Test F1: {f1_test:.4f}, Acc: {acc_test:.4f}\")\n",
    "\n",
    "# 9) Modeli kaydet\n",
    "output_path = \"models/us_accidents_lr_final_full_spark\"\n",
    "model_full_lr.write().overwrite().save(output_path)\n",
    "print(f\"💾 Final LR modeli kaydedildi → {output_path}\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a322d9e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23f34985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hazır veri: 5,217,919 satır, 22 sütun\n",
      "🔀 Bölme → train: 4,175,117, test: 1,042,802\n"
     ]
    }
   ],
   "source": [
    "# %% [python]\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, unix_timestamp, when,\n",
    "    hour, dayofweek, month, year\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import (\n",
    "    DecisionTreeClassifier,\n",
    "    DecisionTreeClassificationModel\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# 1) SparkSession'ı başlat (daha büyük heap + maxResultSize)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"US Accidents DT Final Full Train\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\",        \"16g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2) Veri Hazırlığı\n",
    "# 2.1) CSV oku\n",
    "df = spark.read.csv(\"US_Accidents_March23.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 2.2) İstenmeyen sütunları düş\n",
    "drop_cols = [\n",
    "    \"ID\",\"Source\",\"Zipcode\",\"Timezone\",\"Airport_Code\",\"Amenity\",\"Bump\",\n",
    "    \"Give_Way\",\"No_Exit\",\"Railway\",\"Description\",\"County\",\"Roundabout\",\n",
    "    \"Station\",\"Stop\",\"Nautical_Twilight\",\"Astronomical_Twilight\",\"Country\"\n",
    "]\n",
    "df2 = df.drop(*drop_cols)\n",
    "\n",
    "# 2.3) Zaman sütunlarını kullanarak Duration ve ek tarih/saat özellikleri oluştur\n",
    "df3 = (\n",
    "    df2\n",
    "    .withColumn(\"Start_TS\", to_timestamp(col(\"Start_Time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .withColumn(\"End_TS\",   to_timestamp(col(\"End_Time\"),   \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .withColumn(\"Duration\",\n",
    "        ((unix_timestamp(col(\"End_TS\")) - unix_timestamp(col(\"Start_TS\"))) / 60)\n",
    "        .cast(DoubleType())\n",
    "    )\n",
    "    .withColumn(\"HourOfDay\", hour(col(\"Start_TS\")))\n",
    "    .withColumn(\"DayOfWeek\", dayofweek(col(\"Start_TS\")))\n",
    "    .withColumn(\"Month\",     month(col(\"Start_TS\")))\n",
    "    .withColumn(\"Year\",      year(col(\"Start_TS\")))\n",
    "    .drop(\"Start_TS\",\"End_TS\",\"Start_Time\",\"End_Time\")\n",
    ")\n",
    "\n",
    "# 2.4) City/Street cardinality düşür\n",
    "def clean_column(df, column_name, top_n=32):\n",
    "    top_vals = [\n",
    "        r[column_name] for r in\n",
    "        df.groupBy(column_name).count()\n",
    "          .orderBy(col(\"count\").desc())\n",
    "          .limit(top_n)\n",
    "          .collect()\n",
    "    ]\n",
    "    return df.withColumn(\n",
    "        f\"{column_name}_Cleaned\",\n",
    "        when(col(column_name).isin(top_vals), col(column_name)).otherwise(\"Other\")\n",
    "    )\n",
    "\n",
    "for c in [\"City\",\"Street\"]:\n",
    "    df3 = clean_column(df3, c, top_n=32)\n",
    "\n",
    "# 2.5) Modelde kullanacağımız sütunları seç\n",
    "selected_cols = [\n",
    "    \"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\", \"Precipitation(in)\", \"Wind_Chill(F)\", \"Traffic_Signal\",\n",
    "    \"Weather_Condition\", \"Wind_Direction\", \"Junction\", \"Duration\", \"Severity\",\n",
    "    \"Civil_Twilight\", \"Sunrise_Sunset\", \"State\",\n",
    "    \"City_Cleaned\", \"Street_Cleaned\",\n",
    "    \"HourOfDay\", \"DayOfWeek\", \"Month\", \"Year\"\n",
    "]\n",
    "df_selected = df3.select(*selected_cols)\n",
    "\n",
    "# 2.6) Kategorik ve sayısal sütun listelerini hazırla\n",
    "categorical_cols = [\n",
    "    \"Weather_Condition\",\"Wind_Direction\",\"Civil_Twilight\",\n",
    "    \"Sunrise_Sunset\",\"State\",\"City_Cleaned\",\"Street_Cleaned\"\n",
    "]\n",
    "numerical_cols = [\n",
    "    \"Temperature(F)\",\"Humidity(%)\",\"Pressure(in)\",\"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\",\"Precipitation(in)\",\"Wind_Chill(F)\",\"Traffic_Signal\",\n",
    "    \"Junction\",\"Duration\",\"HourOfDay\",\"DayOfWeek\",\"Month\",\"Year\"\n",
    "]\n",
    "feature_cols = numerical_cols + [c + \"_Idx\" for c in categorical_cols]\n",
    "\n",
    "# 2.7) Eksikleri at ve cache\n",
    "df_no_na = df_selected.dropna().cache()\n",
    "print(f\"✅ Hazır veri: {df_no_na.count():,} satır, {len(df_no_na.columns)} sütun\")\n",
    "\n",
    "# 3) Train/Test bölme\n",
    "train, test = df_no_na.randomSplit([0.8,0.2], seed=42)\n",
    "print(f\"🔀 Bölme → train: {train.count():,}, test: {test.count():,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69825a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Küçük modelden gelen parametreler → maxDepth=15, maxBins=256\n",
      "⚙️ Final DT eğitimi tüm train üzerinde başlıyor…\n",
      "🎉 Final DT — Test F1: 0.8586, Acc: 0.8781\n",
      "💾 Final DT modeli kaydedildi → models/us_accidents_dt_final_full_spark\n"
     ]
    }
   ],
   "source": [
    "# 4) Küçük-modelden en iyi DecisionTree parametrelerini yükle\n",
    "small_dt_stage = (\n",
    "    \"models/us_accidents_dt_optimized_small_spark\"\n",
    "    \"/stages/09_DecisionTreeClassifier_f9b588c68bd3\"\n",
    ")\n",
    "dt_small = DecisionTreeClassificationModel.load(small_dt_stage)\n",
    "best_depth = dt_small.getMaxDepth()\n",
    "# small-model'de de 32 kullanılmıştı; burada de 32'yi sabitliyoruz:\n",
    "best_bins  = 256\n",
    "print(f\"🔍 Küçük modelden gelen parametreler → maxDepth={best_depth}, maxBins={best_bins}\")\n",
    "\n",
    "# 5) Pipeline bileşenleri ve final DecisionTree tanımı\n",
    "label_indexer = StringIndexer(\n",
    "    inputCol=\"Severity\", outputCol=\"label\", handleInvalid=\"keep\"\n",
    ")\n",
    "cat_indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=c + \"_Idx\", handleInvalid=\"keep\")\n",
    "    for c in categorical_cols\n",
    "]\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"skip\"\n",
    ")\n",
    "dt_final = DecisionTreeClassifier(\n",
    "    labelCol=\"label\", featuresCol=\"features\",\n",
    "    maxDepth=best_depth, maxBins=best_bins\n",
    ")\n",
    "\n",
    "pipe_full = Pipeline(stages=[\n",
    "    label_indexer\n",
    "] + cat_indexers + [\n",
    "    assembler,\n",
    "    dt_final\n",
    "])\n",
    "\n",
    "# 6) Final modeli tüm train üzerinde eğit\n",
    "print(\"⚙️ Final DT eğitimi tüm train üzerinde başlıyor…\")\n",
    "model_full = pipe_full.fit(train)\n",
    "\n",
    "# 7) Test üzerinde değerlendir\n",
    "f1_eval  = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "acc_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "preds = model_full.transform(test)\n",
    "print(f\"🎉 Final DT — Test F1: {f1_eval.evaluate(preds):.4f}, \"\n",
    "      f\"Acc: {acc_eval.evaluate(preds):.4f}\")\n",
    "\n",
    "# 8) Modeli kaydet\n",
    "out_path = \"models/us_accidents_dt_final_full_spark\"\n",
    "model_full.write().overwrite().save(out_path)\n",
    "print(f\"💾 Final DT modeli kaydedildi → {out_path}\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e131160a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ Final DT eğitimi tüm train üzerinde başlıyor…\n",
      ">> model_full tipi: <class 'pyspark.ml.pipeline.PipelineModel'>\n",
      "🎉 Final DT — Test F1: 0.8589, Acc: 0.8781\n",
      "💾 Final DT modeli kaydedildi → models/us_accidents_dt_final_last_full_spark\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, DecisionTreeClassificationModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# 1) SparkSession başlat\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Final_DT_Train_And_Save\")\n",
    "         .config(\"spark.executor.memory\",\"8g\")\n",
    "         .config(\"spark.driver.memory\",\"4g\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# --- (Önceki adımlar: train/test, küçük-model’den depth/bins alımı) ---\n",
    "small_dt_stage = (\n",
    "    \"models/us_accidents_dt_optimized_small_spark\"\n",
    "    \"/stages/09_DecisionTreeClassifier_f9b588c68bd3\"\n",
    ")\n",
    "dt_small   = DecisionTreeClassificationModel.load(small_dt_stage)\n",
    "best_depth = dt_small.getMaxDepth()\n",
    "best_bins  = 256\n",
    "\n",
    "# label ve categorical indexer’lar\n",
    "label_indexer = StringIndexer(\n",
    "    inputCol=\"Severity\", outputCol=\"label\", handleInvalid=\"keep\"\n",
    ")\n",
    "cat_indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=c + \"_Idx\", handleInvalid=\"keep\")\n",
    "    for c in categorical_cols\n",
    "]\n",
    "\n",
    "# assembler & final DT\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "dt_final = DecisionTreeClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    maxDepth=best_depth,\n",
    "    maxBins=best_bins\n",
    ")\n",
    "\n",
    "pipe_full = Pipeline(stages=[\n",
    "    label_indexer,\n",
    "    *cat_indexers,\n",
    "    assembler,\n",
    "    dt_final\n",
    "])\n",
    "\n",
    "# 6) Final modeli tüm train üzerinde eğit\n",
    "print(\"⚙️ Final DT eğitimi tüm train üzerinde başlıyor…\")\n",
    "model_full = pipe_full.fit(train)\n",
    "\n",
    "# — Yeni ekleme: tip kontrolü —\n",
    "print(f\">> model_full tipi: {type(model_full)}\")\n",
    "assert isinstance(model_full, PipelineModel), \"model_full bir PipelineModel değil!\"\n",
    "\n",
    "# 7) Test üzerinde değerlendir\n",
    "f1_eval  = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "acc_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "preds = model_full.transform(test)\n",
    "print(f\"🎉 Final DT — Test F1: {f1_eval.evaluate(preds):.4f}, \"\n",
    "      f\"Acc: {acc_eval.evaluate(preds):.4f}\")\n",
    "\n",
    "# 8) PipelineModel olarak kaydet\n",
    "out_path = \"models/us_accidents_dt_final_last_full_spark\"\n",
    "model_full.write().overwrite().save(out_path)\n",
    "print(f\"💾 Final DT modeli kaydedildi → {out_path}\")\n",
    "\n",
    "# 9) (Kullanım örneği)\n",
    "# loaded = PipelineModel.load(out_path)\n",
    "# new_preds = loaded.transform(new_dataframe)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b0767fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Loading PipelineModel from: c:\\Users\\aslay\\Desktop\\BİL 401\\project\\models\\us_accidents_dt_final_last_full_spark\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (MSI executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:528)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:528)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipelineModel folder not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mabs_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[DEBUG] Loading PipelineModel from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mabs_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m pipeline_model \u001b[38;5;241m=\u001b[39m \u001b[43mPipelineModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/us_accidents_dt_final_last_full_spark\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# 3) Define helper for cardinality reduction\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_column\u001b[39m(df, col_name, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\pyspark\\ml\\util.py:369\u001b[0m, in \u001b[0;36mMLReadable.load\u001b[1;34m(cls, path)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RL:\n\u001b[0;32m    368\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\pyspark\\ml\\pipeline.py:282\u001b[0m, in \u001b[0;36mPipelineModelReader.load\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipelineModel\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 282\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mDefaultParamsReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadMetadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m JavaMLReader(cast(Type[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJavaMLReadable[PipelineModel]\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls))\u001b[38;5;241m.\u001b[39mload(path)\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\pyspark\\ml\\util.py:579\u001b[0m, in \u001b[0;36mDefaultParamsReader.loadMetadata\u001b[1;34m(path, sc, expectedClassName)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;124;03mLoad metadata saved using :py:meth:`DefaultParamsWriter.saveMetadata`\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;124;03m    If non empty, this is checked against the loaded metadata.\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    578\u001b[0m metadataPath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 579\u001b[0m metadataStr \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadataPath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    580\u001b[0m loadedVals \u001b[38;5;241m=\u001b[39m DefaultParamsReader\u001b[38;5;241m.\u001b[39m_parseMetaData(metadataStr, expectedClassName)\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loadedVals\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\pyspark\\rdd.py:2888\u001b[0m, in \u001b[0;36mRDD.first\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2862\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m   2863\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2864\u001b[0m \u001b[38;5;124;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[0;32m   2865\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2886\u001b[0m \u001b[38;5;124;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[0;32m   2887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2888\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2889\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[0;32m   2890\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\pyspark\\rdd.py:2855\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   2852\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2854\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[1;32m-> 2855\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2857\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[0;32m   2858\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\pyspark\\context.py:2510\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   2508\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[0;32m   2509\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2510\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (MSI executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:528)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:528)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, to_timestamp, unix_timestamp,\n",
    "    hour, dayofweek\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "# 0) Ensure Spark uses same Python interpreter (avoids worker connect errors)\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "# 1) Start SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Accident Severity Batch Predict\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .config(\"spark.pyspark.python\", sys.executable)\n",
    "    .config(\"spark.pyspark.driver.python\", sys.executable)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 2) Load the saved PipelineModel\n",
    "model_path = \"models/us_accidents_dt_final_last_full_spark\"\n",
    "abs_path = os.path.abspath(model_path)\n",
    "if not os.path.isdir(abs_path):\n",
    "    raise FileNotFoundError(f\"PipelineModel folder not found: {abs_path}\")\n",
    "print(f\"[DEBUG] Loading PipelineModel from: {abs_path}\")\n",
    "pipeline_model = PipelineModel.load(\"models/us_accidents_dt_final_last_full_spark\")\n",
    "\n",
    "# 3) Define helper for cardinality reduction\n",
    "def clean_column(df, col_name, top_n=32):\n",
    "    top_vals = [\n",
    "        row[col_name]\n",
    "        for row in (\n",
    "            df.groupBy(col_name)\n",
    "              .count()\n",
    "              .orderBy(col(\"count\").desc())\n",
    "              .limit(top_n)\n",
    "              .collect()\n",
    "        )\n",
    "    ]\n",
    "    out_col = f\"{col_name}_Cleaned\"\n",
    "    return df.withColumn(\n",
    "        out_col,\n",
    "        when(col(col_name).isin(top_vals), col(col_name)).otherwise(\"Other\")\n",
    "    )\n",
    "\n",
    "# 4) Define preprocessing function\n",
    "def prepare_data(df):\n",
    "    # parse timestamps & compute Duration (minutes)\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"Start_TS\", to_timestamp(col(\"Start_Time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "        .withColumn(\"End_TS\",   to_timestamp(col(\"End_Time\"),   \"yyyy-MM-dd HH:mm:ss\"))\n",
    "        .withColumn(\"Duration\",\n",
    "            ((unix_timestamp(col(\"End_TS\")) - unix_timestamp(col(\"Start_TS\"))) / 60)\n",
    "            .cast(DoubleType())\n",
    "        )\n",
    "        .drop(\"Start_Time\", \"End_Time\", \"Start_TS\", \"End_TS\")\n",
    "    )\n",
    "    # extract time features\n",
    "    df = df.withColumn(\"hour\", hour(col(\"Start_TS\")))\n",
    "    df = df.withColumn(\"weekday\", dayofweek(col(\"Start_TS\")))\n",
    "    # reduce cardinality\n",
    "    df = clean_column(df, \"City\", top_n=32)\n",
    "    df = clean_column(df, \"Street\", top_n=32)\n",
    "    # drop rows with missing numeric\n",
    "    numeric_cols = [\n",
    "        \"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\",\n",
    "        \"Duration\", \"Wind_Speed(mph)\", \"Precipitation(in)\", \"Wind_Chill(F)\"\n",
    "    ]\n",
    "    df = df.na.drop(subset=numeric_cols)\n",
    "    # fill categorical missing\n",
    "    df = df.na.fill({\n",
    "        \"Weather_Condition\": \"Other\",\n",
    "        \"Wind_Direction\":     \"Other\",\n",
    "        \"Civil_Twilight\":     \"Other\",\n",
    "        \"Sunrise_Sunset\":     \"Other\",\n",
    "        \"State\":              \"Other\",\n",
    "        \"Traffic_Signal\":     False,\n",
    "        \"Junction\":           False\n",
    "    })\n",
    "    # select exactly pipeline inputs\n",
    "    pipeline_cols = (\n",
    "        numeric_cols\n",
    "        + [\"Traffic_Signal\", \"Junction\", \"hour\", \"weekday\"]\n",
    "        + [\"City_Cleaned\", \"Street_Cleaned\"]\n",
    "        + [\"Weather_Condition\", \"Wind_Direction\",\n",
    "           \"Civil_Twilight\", \"Sunrise_Sunset\", \"State\"]\n",
    "    )\n",
    "    return df.select(*pipeline_cols)\n",
    "\n",
    "# 5) Your new data as list of dicts\n",
    "veri = [\n",
    "    {\n",
    "        \"Start_Time\": \"2023-12-10 07:00:00\",\n",
    "        \"End_Time\":   \"2023-12-10 07:45:00\",\n",
    "        \"Temperature(F)\": 15.0,\n",
    "        \"Humidity(%)\":    85.0,\n",
    "        \"Pressure(in)\":   29.5,\n",
    "        \"Visibility(mi)\": 2.0,\n",
    "        \"Wind_Speed(mph)\":20.0,\n",
    "        \"Precipitation(in)\":0.4,\n",
    "        \"Weather_Condition\":\"Snow\",\n",
    "        \"Wind_Direction\": \"N\",\n",
    "        \"Civil_Twilight\":\"Night\",\n",
    "        \"Sunrise_Sunset\":\"Night\",\n",
    "        \"State\":\"IL\",\n",
    "        \"Junction\": True,\n",
    "        \"Traffic_Signal\": False,\n",
    "        \"Crossing\": True,\n",
    "        \"City\":\"Chicago\",\n",
    "        \"Street\":\"W Adams St\",\n",
    "        \"Wind_Chill(F)\":10.0\n",
    "    },\n",
    "    {\n",
    "        \"Start_Time\": \"2024-03-20 18:30:00\",\n",
    "        \"End_Time\":   \"2024-03-20 18:40:00\",\n",
    "        \"Temperature(F)\": 85.0,\n",
    "        \"Humidity(%)\":    30.0,\n",
    "        \"Pressure(in)\":   30.2,\n",
    "        \"Visibility(mi)\": 10.0,\n",
    "        \"Wind_Speed(mph)\":3.0,\n",
    "        \"Precipitation(in)\":0.0,\n",
    "        \"Weather_Condition\":\"Clear\",\n",
    "        \"Wind_Direction\": \"SE\",\n",
    "        \"Civil_Twilight\":\"Day\",\n",
    "        \"Sunrise_Sunset\":\"Day\",\n",
    "        \"State\":\"TX\",\n",
    "        \"Junction\": False,\n",
    "        \"Traffic_Signal\": True,\n",
    "        \"Crossing\": False,\n",
    "        \"City\":\"Houston\",\n",
    "        \"Street\":\"Main St\",\n",
    "        \"Wind_Chill(F)\":84.0\n",
    "    },\n",
    "    # add more rows as needed...\n",
    "]\n",
    "\n",
    "# 6) Create DataFrame, preprocess, predict\n",
    "df_rows = spark.createDataFrame(veri)\n",
    "df_prep = prepare_data(df_rows)\n",
    "preds = pipeline_model.transform(df_prep)\n",
    "\n",
    "# 7) Show desired columns\n",
    "preds.select(\"City_Cleaned\", \"Street_Cleaned\", \"prediction\").show(truncate=False)\n",
    "\n",
    "# 8) Stop Spark\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c5c4c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hazır veri: 5,217,919 kayıt\n",
      "🔀 Bölme → train: 4,175,117, test: 1,042,802\n"
     ]
    }
   ],
   "source": [
    "# %% [python]\n",
    "# 1) Findspark ve SparkSession Başlat\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, unix_timestamp, when,\n",
    "    hour, dayofweek, month, year\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import (\n",
    "    RandomForestClassificationModel,\n",
    "    RandomForestClassifier\n",
    ")\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"US Accidents RF Final Training\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2) Veri Okuma ve Hazırlama\n",
    "df = spark.read.csv(\"US_Accidents_March23.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 2.1) İstenmeyen sütunları çıkar\n",
    "drop_cols = [\n",
    "    \"ID\",\"Source\",\"Zipcode\",\"Timezone\",\"Airport_Code\",\"Amenity\",\"Bump\",\n",
    "    \"Give_Way\",\"No_Exit\",\"Railway\",\"Description\",\"County\",\"Roundabout\",\n",
    "    \"Station\",\"Stop\",\"Nautical_Twilight\",\"Astronomical_Twilight\",\"Country\"\n",
    "]\n",
    "df2 = df.drop(*drop_cols)\n",
    "\n",
    "# 2.2) Tarih/Saat'ten Duration ve bölgesel feature'lar\n",
    "df3 = (\n",
    "    df2\n",
    "    .withColumn(\"Start_TS\", to_timestamp(col(\"Start_Time\"), \"yyyy‑MM‑dd HH:mm:ss\"))\n",
    "    .withColumn(\"End_TS\",   to_timestamp(col(\"End_Time\"),   \"yyyy‑MM‑dd HH:mm:ss\"))\n",
    "    .withColumn(\"Duration\",\n",
    "        ((unix_timestamp(col(\"End_TS\")) - unix_timestamp(col(\"Start_TS\"))) / 60)\n",
    "        .cast(DoubleType())\n",
    "    )\n",
    "    .withColumn(\"HourOfDay\",  hour(col(\"Start_TS\")))\n",
    "    .withColumn(\"DayOfWeek\",  dayofweek(col(\"Start_TS\")))\n",
    "    .withColumn(\"Month\",      month(col(\"Start_TS\")))\n",
    "    .withColumn(\"Year\",       year(col(\"Start_TS\")))\n",
    "    .drop(\"Start_TS\",\"End_TS\",\"Start_Time\",\"End_Time\")\n",
    ")\n",
    "\n",
    "# 2.3) City/Street cardinality düşürme fonksiyonu\n",
    "def clean_column(df, column_name, top_n=32):\n",
    "    top_vals = [\n",
    "        r[column_name] for r in\n",
    "        df.groupBy(column_name).count()\n",
    "          .orderBy(col(\"count\").desc())\n",
    "          .limit(top_n)\n",
    "          .collect()\n",
    "    ]\n",
    "    return df.withColumn(\n",
    "        f\"{column_name}_Cleaned\",\n",
    "        when(col(column_name).isin(top_vals), col(column_name)).otherwise(\"Other\")\n",
    "    )\n",
    "\n",
    "for c in [\"City\",\"Street\"]:\n",
    "    df3 = clean_column(df3, c, top_n=32)\n",
    "\n",
    "# 2.4) İstenen sütunları seç\n",
    "selected_cols = [\n",
    "    \"Temperature(F)\",\"Humidity(%)\",\"Pressure(in)\",\"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\",\"Precipitation(in)\",\"Wind_Chill(F)\",\n",
    "    \"Traffic_Signal\",\"Weather_Condition\",\"Wind_Direction\",\"Junction\",\n",
    "    \"Duration\",\"Severity\",\"Civil_Twilight\",\"Sunrise_Sunset\",\"State\",\n",
    "    \"City_Cleaned\",\"Street_Cleaned\",\"HourOfDay\",\"DayOfWeek\",\"Month\",\"Year\"\n",
    "]\n",
    "df_selected = df3.select(*selected_cols)\n",
    "\n",
    "# 2.5) Kategorik ve sayısal liste\n",
    "categorical_cols = [\n",
    "    \"Weather_Condition\",\"Wind_Direction\",\"Civil_Twilight\",\n",
    "    \"Sunrise_Sunset\",\"State\",\"City_Cleaned\",\"Street_Cleaned\"\n",
    "]\n",
    "numerical_cols = [\n",
    "    \"Temperature(F)\",\"Humidity(%)\",\"Pressure(in)\",\"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\",\"Precipitation(in)\",\"Wind_Chill(F)\",\n",
    "    \"Traffic_Signal\",\"Junction\",\"Duration\",\n",
    "    \"HourOfDay\",\"DayOfWeek\",\"Month\",\"Year\"\n",
    "]\n",
    "feature_cols = numerical_cols + [c + \"_Idx\" for c in categorical_cols]\n",
    "\n",
    "df_no_na = df_selected.dropna().cache()\n",
    "print(f\"✅ Hazır veri: {df_no_na.count():,} kayıt\")\n",
    "\n",
    "# 3) Train/Test Bölme\n",
    "train, test = df_no_na.randomSplit([0.8,0.2], seed=42)\n",
    "print(f\"🔀 Bölme → train: {train.count():,}, test: {test.count():,}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a543d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Small‑model RF params → numTrees=75, maxDepth=<bound method _DecisionTreeParams.getMaxDepth of RandomForestClassificationModel: uid=RandomForestClassifier_328cc66430e7, numTrees=75, numClasses=5, numFeatures=21>, maxBins=<bound method _DecisionTreeParams.getMaxBins of RandomForestClassificationModel: uid=RandomForestClassifier_328cc66430e7, numTrees=75, numClasses=5, numFeatures=21>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid param value given for param \"maxDepth\". Could not convert <bound method _DecisionTreeParams.getMaxDepth of RandomForestClassificationModel: uid=RandomForestClassifier_328cc66430e7, numTrees=75, numClasses=5, numFeatures=21> to int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\param\\__init__.py:503\u001b[0m, in \u001b[0;36mParams._set\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 503\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtypeConverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\param\\__init__.py:224\u001b[0m, in \u001b[0;36mTypeConverters.toInt\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not convert \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to int\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m value)\n",
      "\u001b[1;31mTypeError\u001b[0m: Could not convert <bound method _DecisionTreeParams.getMaxDepth of RandomForestClassificationModel: uid=RandomForestClassifier_328cc66430e7, numTrees=75, numClasses=5, numFeatures=21> to int",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 121\u001b[0m\n\u001b[0;32m    114\u001b[0m cat_indexers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    115\u001b[0m     StringIndexer(inputCol\u001b[38;5;241m=\u001b[39mc, outputCol\u001b[38;5;241m=\u001b[39mc \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_Idx\u001b[39m\u001b[38;5;124m\"\u001b[39m, handleInvalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m categorical_cols\n\u001b[0;32m    117\u001b[0m ]\n\u001b[0;32m    118\u001b[0m assembler \u001b[38;5;241m=\u001b[39m VectorAssembler(\n\u001b[0;32m    119\u001b[0m     inputCols\u001b[38;5;241m=\u001b[39mfeature_cols, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, handleInvalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m )\n\u001b[1;32m--> 121\u001b[0m rf_final \u001b[38;5;241m=\u001b[39m \u001b[43mRandomForestClassifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabelCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeaturesCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumTrees\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trees\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxDepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxBins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm_bins\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m pipe_full \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m    127\u001b[0m     label_indexer, \u001b[38;5;241m*\u001b[39mcat_indexers, assembler, rf_final\n\u001b[0;32m    128\u001b[0m ])\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# 6) Fit on full train\u001b[39;00m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\classification.py:2108\u001b[0m, in \u001b[0;36mRandomForestClassifier.__init__\u001b[1;34m(self, featuresCol, labelCol, predictionCol, probabilityCol, rawPredictionCol, maxDepth, maxBins, minInstancesPerNode, minInfoGain, maxMemoryInMB, cacheNodeIds, checkpointInterval, impurity, numTrees, featureSubsetStrategy, seed, subsamplingRate, leafCol, minWeightFractionPerNode, weightCol, bootstrap)\u001b[0m\n\u001b[0;32m   2104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_java_obj(\n\u001b[0;32m   2105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.ml.classification.RandomForestClassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muid\n\u001b[0;32m   2106\u001b[0m )\n\u001b[0;32m   2107\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs\n\u001b[1;32m-> 2108\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetParams\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\classification.py:2147\u001b[0m, in \u001b[0;36mRandomForestClassifier.setParams\u001b[1;34m(self, featuresCol, labelCol, predictionCol, probabilityCol, rawPredictionCol, maxDepth, maxBins, minInstancesPerNode, minInfoGain, maxMemoryInMB, cacheNodeIds, checkpointInterval, impurity, numTrees, featureSubsetStrategy, seed, subsamplingRate, leafCol, minWeightFractionPerNode, weightCol, bootstrap)\u001b[0m\n\u001b[0;32m   2137\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2138\u001b[0m \u001b[38;5;124;03msetParams(self, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\\u001b[39;00m\n\u001b[0;32m   2139\u001b[0m \u001b[38;5;124;03m         probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\", \\\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2144\u001b[0m \u001b[38;5;124;03mSets params for linear classification.\u001b[39;00m\n\u001b[0;32m   2145\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2146\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs\n\u001b[1;32m-> 2147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\param\\__init__.py:505\u001b[0m, in \u001b[0;36mParams._set\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    503\u001b[0m             value \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mtypeConverter(value)\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 505\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid param value given for param \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (p\u001b[38;5;241m.\u001b[39mname, e))\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paramMap[p] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid param value given for param \"maxDepth\". Could not convert <bound method _DecisionTreeParams.getMaxDepth of RandomForestClassificationModel: uid=RandomForestClassifier_328cc66430e7, numTrees=75, numClasses=5, numFeatures=21> to int"
     ]
    }
   ],
   "source": [
    "# %% [python]\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, unix_timestamp, when,\n",
    "    hour, dayofweek, month, year\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import (\n",
    "    RandomForestClassifier,\n",
    "    RandomForestClassificationModel\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# 1) Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"US Accidents RF Final Training\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\",\"16g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\",\"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2) Read & prep data\n",
    "df = spark.read.csv(\"US_Accidents_March23.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 2.1) Drop unneeded cols\n",
    "drop_cols = [\n",
    "    \"ID\",\"Source\",\"Zipcode\",\"Timezone\",\"Airport_Code\",\"Amenity\",\"Bump\",\n",
    "    \"Give_Way\",\"No_Exit\",\"Railway\",\"Description\",\"County\",\"Roundabout\",\n",
    "    \"Station\",\"Stop\",\"Nautical_Twilight\",\"Astronomical_Twilight\",\"Country\"\n",
    "]\n",
    "df2 = df.drop(*drop_cols)\n",
    "\n",
    "# 2.2) Timestamps → Duration + time features\n",
    "df3 = (\n",
    "    df2\n",
    "    .withColumn(\"Start_TS\", to_timestamp(col(\"Start_Time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .withColumn(\"End_TS\",   to_timestamp(col(\"End_Time\"),   \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .withColumn(\"Duration\",\n",
    "        ((unix_timestamp(col(\"End_TS\")) - unix_timestamp(col(\"Start_TS\"))) / 60)\n",
    "        .cast(DoubleType())\n",
    "    )\n",
    "    .withColumn(\"HourOfDay\",  hour(col(\"Start_TS\")))\n",
    "    .withColumn(\"DayOfWeek\",  dayofweek(col(\"Start_TS\")))\n",
    "    .withColumn(\"Month\",      month(col(\"Start_TS\")))\n",
    "    .withColumn(\"Year\",       year(col(\"Start_TS\")))\n",
    "    .drop(\"Start_TS\",\"End_TS\",\"Start_Time\",\"End_Time\")\n",
    ")\n",
    "\n",
    "# 2.3) Cardinality‑reduce City/Street\n",
    "def clean_column(df, column_name, top_n=32):\n",
    "    top_vals = [\n",
    "        r[column_name] for r in\n",
    "        df.groupBy(column_name).count()\n",
    "          .orderBy(col(\"count\").desc())\n",
    "          .limit(top_n)\n",
    "          .collect()\n",
    "    ]\n",
    "    return df.withColumn(\n",
    "        f\"{column_name}_Cleaned\",\n",
    "        when(col(column_name).isin(top_vals), col(column_name)).otherwise(\"Other\")\n",
    "    )\n",
    "\n",
    "for c in [\"City\",\"Street\"]:\n",
    "    df3 = clean_column(df3, c, top_n=32)\n",
    "\n",
    "# 2.4) Final select + drop NA\n",
    "selected_cols = [\n",
    "    \"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\", \"Precipitation(in)\", \"Wind_Chill(F)\", \"Traffic_Signal\",\n",
    "    \"Weather_Condition\", \"Wind_Direction\", \"Junction\", \"Duration\", \"Severity\",\n",
    "    \"Civil_Twilight\", \"Sunrise_Sunset\", \"State\", \"City_Cleaned\", \"Street_Cleaned\",\n",
    "    \"HourOfDay\", \"DayOfWeek\", \"Month\", \"Year\"\n",
    "]\n",
    "df_selected = df3.select(*selected_cols)\n",
    "df_no_na = df_selected.dropna().cache()\n",
    "\n",
    "# 3) Train/test split\n",
    "train, test = df_no_na.randomSplit([0.8,0.2], seed=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da67ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Small‑model RF params → numTrees=75, maxDepth=15, maxBins=256\n",
      "⚙️ Final RF training on all data…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aslay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] Varolan bir bağlantı uzaktaki bir ana bilgisayar tarafından zorla kapatıldı\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o3169.evaluate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 60\u001b[0m\n\u001b[0;32m     56\u001b[0m acc_eval \u001b[38;5;241m=\u001b[39m MulticlassClassificationEvaluator(\n\u001b[0;32m     57\u001b[0m     labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, predictionCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m, metricName\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     58\u001b[0m )\n\u001b[0;32m     59\u001b[0m preds \u001b[38;5;241m=\u001b[39m model_full\u001b[38;5;241m.\u001b[39mtransform(test)\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🎉 Final RF — Test F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mf1_eval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc_eval\u001b[38;5;241m.\u001b[39mevaluate(preds)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# 8) Save\u001b[39;00m\n\u001b[0;32m     63\u001b[0m out_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/us_accidents_rf_final_full_spark\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\evaluation.py:111\u001b[0m, in \u001b[0;36mEvaluator.evaluate\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_evaluate(dataset)\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\evaluation.py:148\u001b[0m, in \u001b[0;36mJavaEvaluator._evaluate\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mPy4JError\u001b[0m: An error occurred while calling o3169.evaluate"
     ]
    }
   ],
   "source": [
    "# 4) Load tuned small‑model RF stage\n",
    "stage_path = (\n",
    "    \"models/us_accidents_rf_optimized_small_spark/\"\n",
    "    \"stages/09_RandomForestClassifier_328cc66430e7\"\n",
    ")\n",
    "rf_small = RandomForestClassificationModel.load(stage_path)\n",
    "\n",
    "# **NOTE: getters are properties in Spark 3.x, no ()**\n",
    "n_trees = rf_small.getNumTrees\n",
    "m_depth = rf_small.getMaxDepth()\n",
    "m_bins  = rf_small.getMaxBins()\n",
    "\n",
    "print(f\"🔍 Small‑model RF params → numTrees={n_trees}, maxDepth={m_depth}, maxBins={m_bins}\")\n",
    "\n",
    "# 5) Build final‑train pipeline\n",
    "categorical_cols = [\n",
    "    \"Weather_Condition\",\"Wind_Direction\",\"Civil_Twilight\",\n",
    "    \"Sunrise_Sunset\",\"State\",\"City_Cleaned\",\"Street_Cleaned\"\n",
    "]\n",
    "numerical_cols = [\n",
    "    \"Temperature(F)\",\"Humidity(%)\",\"Pressure(in)\",\"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\",\"Precipitation(in)\",\"Wind_Chill(F)\",\"Traffic_Signal\",\n",
    "    \"Junction\",\"Duration\",\"HourOfDay\",\"DayOfWeek\",\"Month\",\"Year\"\n",
    "]\n",
    "feature_cols = numerical_cols + [c + \"_Idx\" for c in categorical_cols]\n",
    "\n",
    "label_indexer = StringIndexer(\n",
    "    inputCol=\"Severity\", outputCol=\"label\", handleInvalid=\"keep\"\n",
    ")\n",
    "cat_indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=c + \"_Idx\", handleInvalid=\"keep\")\n",
    "    for c in categorical_cols\n",
    "]\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "\n",
    "rf_final = RandomForestClassifier(\n",
    "    labelCol=\"label\", featuresCol=\"features\",\n",
    "    numTrees=n_trees, maxDepth=m_depth, maxBins=m_bins\n",
    ")\n",
    "\n",
    "pipe_full = Pipeline(stages=[\n",
    "    label_indexer, *cat_indexers, assembler, rf_final\n",
    "])\n",
    "\n",
    "# 6) Fit on full train\n",
    "print(\"⚙️ Final RF training on all data…\")\n",
    "model_full = pipe_full.fit(train)\n",
    "\n",
    "# 7) Evaluate on test\n",
    "f1_eval  = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "acc_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "preds = model_full.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45ce1c6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] Hedef makine etkin olarak reddettiğinden bağlantı kurulamadı",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# %% [python]\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfindspark\u001b[39;00m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\session.py:1796\u001b[0m, in \u001b[0;36mSparkSession.stop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;124;03mStop the underlying :class:`SparkContext`.\u001b[39;00m\n\u001b[0;32m   1784\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1792\u001b[0m \u001b[38;5;124;03m>>> spark.stop()  # doctest: +SKIP\u001b[39;00m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SQLContext\n\u001b[1;32m-> 1796\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1797\u001b[0m \u001b[38;5;66;03m# We should clean the default session up. See SPARK-23228.\u001b[39;00m\n\u001b[0;32m   1798\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\context.py:654\u001b[0m, in \u001b[0;36mSparkContext.stop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_jsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 654\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JError:\n\u001b[0;32m    656\u001b[0m         \u001b[38;5;66;03m# Case: SPARK-18523\u001b[39;00m\n\u001b[0;32m    657\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    658\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to cleanly shutdown Spark JVM process.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    659\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m It is possible that the process has crashed,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    660\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m been killed or may also be in a zombie state.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    661\u001b[0m             \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m,\n\u001b[0;32m    662\u001b[0m         )\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[1;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] Hedef makine etkin olarak reddettiğinden bağlantı kurulamadı"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "# %% [python]\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, unix_timestamp, when,\n",
    "    hour, dayofweek, month, year\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import (\n",
    "    RandomForestClassifier,\n",
    "    RandomForestClassificationModel\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# 1) Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"US Accidents RF Final Training\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\",\"16g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\",\"4g\") \\\n",
    "    .getOrCreate()# --- 1) Evaluate on test set\n",
    "\n",
    "f1 = f1_eval.evaluate(preds)\n",
    "acc = acc_eval.evaluate(preds)\n",
    "print(f\"🎉 Final RF — Test F1: {f1:.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "# --- 2) Save the final model\n",
    "out_path = \"models/us_accidents_rf_final_full_spark\"\n",
    "model_full.write().overwrite().save(out_path)\n",
    "print(f\"💾 Final RF modeli kaydedildi → {out_path}\")\n",
    "\n",
    "# --- 3) Stop Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1920596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [python]\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, unix_timestamp,\n",
    "    hour, dayofweek, month, year,\n",
    "    when, monotonically_increasing_id\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from functools import reduce\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1) SparkSession başlat\n",
    "# ------------------------------------------------------------------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"US Accidents Batch Predict 100 Rows\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87366956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+-----------------------------------------------------------------------------------------------------------+\n",
      "|_row_id|Severity|prediction|probability                                                                                                |\n",
      "+-------+--------+----------+-----------------------------------------------------------------------------------------------------------+\n",
      "|0      |3       |1.0       |[0.048272784018670564,0.9501515670254781,0.0014535609699508748,1.0368506671203633E-4,1.8402919188492117E-5]|\n",
      "|1      |3       |1.0       |[0.04940417982616065,0.9490594379549752,0.001432049193846695,8.52563833034802E-5,1.9076641713896903E-5]    |\n",
      "|2      |3       |1.0       |[0.060468464507702054,0.9374907376514571,0.001890167188593261,1.272876968319723E-4,2.334295541596523E-5]   |\n",
      "|3      |2       |1.0       |[0.13762110078704323,0.858517424614645,0.0029161651448025654,8.909143381787794E-4,5.439511533046569E-5]    |\n",
      "|4      |2       |1.0       |[0.05107693169261835,0.9474235719280876,0.0014243932229623185,5.5613876415341434E-5,1.948927991631835E-5]  |\n",
      "|5      |2       |1.0       |[0.13188858880474427,0.8648482219646014,0.0028806576466569216,3.332731108144601E-4,4.9258473182885705E-5]  |\n",
      "|6      |3       |1.0       |[0.04551555653974827,0.9530127556253072,0.0014163755549778988,3.813533908002647E-5,1.7176940886685196E-5]  |\n",
      "|7      |2       |1.0       |[0.04686594997175218,0.9517051039927454,0.0013524754498065508,5.8317903103433654E-5,1.81526825923433E-5]   |\n",
      "|8      |2       |1.0       |[0.13336818294262698,0.8634751716735053,0.0026951913706631174,4.113061110610042E-4,5.01479021435362E-5]    |\n",
      "|9      |2       |1.0       |[0.12495876815528316,0.871926262775141,0.0027001785709956402,3.7225666442023336E-4,4.253383416005052E-5]   |\n",
      "|10     |2       |1.0       |[0.12157618909972868,0.8752949800130337,0.002838924430342202,2.4569015629310743E-4,4.421630060218531E-5]   |\n",
      "|11     |2       |1.0       |[0.043053985783768206,0.9554824116778143,0.0014178579197954644,3.111073384175415E-5,1.4633884780040781E-5] |\n",
      "|12     |2       |1.0       |[0.05233942197231882,0.945254209717325,0.002379238338280096,1.0112801914362453E-5,1.7017170161773703E-5]   |\n",
      "|13     |2       |1.0       |[0.139172556417861,0.8557045870453751,0.00500126605114034,7.546865089612117E-5,4.612183472749972E-5]       |\n",
      "|14     |2       |1.0       |[0.07101937701814602,0.9263813577420322,0.0025515745215072636,2.2198375383933217E-5,2.5492342930686553E-5] |\n",
      "|15     |3       |1.0       |[0.07371682628749512,0.9237814881169081,0.0024434013294040184,3.035015863242443E-5,2.7934107560498476E-5]  |\n",
      "|16     |2       |1.0       |[0.07666797937091324,0.9206970294256533,0.0025870470461660075,2.0692644706401757E-5,2.7251512561032684E-5] |\n",
      "|17     |2       |1.0       |[0.19447862256689322,0.8002312684657892,0.004994184825778918,2.2491903662121584E-4,7.100510491745903E-5]   |\n",
      "|18     |2       |1.0       |[0.07351250742834212,0.9239791229205031,0.0024649900216796652,1.6814548081687838E-5,2.6565081393463086E-5] |\n",
      "|19     |3       |1.0       |[0.06252510099678321,0.9342080070632662,0.0032182318079670745,2.4417799659856805E-5,2.424233232372917E-5]  |\n",
      "|20     |3       |1.0       |[0.18963250432673845,0.8055761854124384,0.004542023263925225,1.766920435997201E-4,7.259495329816418E-5]    |\n",
      "|21     |3       |1.0       |[0.07160699735225581,0.9258444393904198,0.002498681286442176,2.389364199344999E-5,2.598832888870414E-5]    |\n",
      "|22     |2       |1.0       |[0.19026885234176064,0.8048530870411066,0.004584424041125846,2.207931332376272E-4,7.284344276915347E-5]    |\n",
      "|23     |3       |1.0       |[0.07330780342769073,0.924084085880633,0.0025573243971393532,2.4440171966082836E-5,2.6346122570624496E-5]  |\n",
      "|24     |2       |1.0       |[0.07061092734134077,0.9269048562004436,0.0024401485912930853,1.829636094813384E-5,2.5771505974358173E-5]  |\n",
      "|25     |2       |1.0       |[0.07061092734134077,0.9269048562004436,0.0024401485912930853,1.829636094813384E-5,2.5771505974358173E-5]  |\n",
      "|26     |3       |1.0       |[0.07061092734134077,0.9269048562004436,0.0024401485912930853,1.829636094813384E-5,2.5771505974358173E-5]  |\n",
      "|27     |3       |1.0       |[0.07524390923037948,0.9221606107861349,0.0025521581299209535,1.6867462259984366E-5,2.6454391304752497E-5] |\n",
      "|28     |2       |1.0       |[0.1585700233718715,0.8383319073290618,0.0028185055351022436,2.2023756712961505E-4,5.932619683467318E-5]   |\n",
      "|29     |2       |1.0       |[0.05979051353970178,0.9386799092021566,0.0014784161578781162,2.976058844146505E-5,2.1400511822098574E-5]  |\n",
      "|30     |2       |1.0       |[0.05979051353970178,0.9386799092021566,0.0014784161578781162,2.976058844146505E-5,2.1400511822098574E-5]  |\n",
      "|31     |2       |1.0       |[0.05979051353970178,0.9386799092021566,0.0014784161578781162,2.976058844146505E-5,2.1400511822098574E-5]  |\n",
      "|32     |3       |1.0       |[0.06046136668598997,0.9380572613390318,0.0014235903151569062,3.537330271167503E-5,2.2408357109562136E-5]  |\n",
      "|33     |2       |1.0       |[0.05870327412984007,0.9398686272752589,0.0013805523321657616,2.5600097671855666E-5,2.194616506348919E-5]  |\n",
      "|34     |2       |1.0       |[0.06215905363454235,0.9363301942660274,0.0014660027819854689,2.318336804408972E-5,2.1565949400684433E-5]  |\n",
      "|35     |2       |1.0       |[0.06215905363454235,0.9363301942660274,0.0014660027819854689,2.318336804408972E-5,2.1565949400684433E-5]  |\n",
      "|36     |2       |1.0       |[0.059662076681638865,0.9388547206445401,0.001439396369647546,2.2873122104518756E-5,2.0933182068975735E-5] |\n",
      "|37     |3       |1.0       |[0.10298522661025042,0.8939659443291214,0.002949097539092,6.402456443985241E-5,3.5706957096432896E-5]      |\n",
      "|38     |3       |1.0       |[0.06215905363454235,0.9363301942660274,0.0014660027819854689,2.318336804408972E-5,2.1565949400684433E-5]  |\n",
      "|39     |2       |1.0       |[0.14227089665909204,0.8549622844227237,0.0025418249402930477,1.7352937322521707E-4,5.146460466607633E-5]  |\n",
      "|40     |3       |1.0       |[0.07983390344093015,0.9175893413852442,0.0025390669813108425,1.1322122406280466E-5,2.6366070108296303E-5] |\n",
      "|41     |2       |1.0       |[0.2095589712453521,0.7853873935769751,0.00487100659191477,1.1170259320793226E-4,7.092599255008921E-5]     |\n",
      "|42     |2       |1.0       |[0.07319554298890667,0.9242527531890153,0.0025163339375312943,1.1057579899111092E-5,2.4312304647717298E-5] |\n",
      "|43     |3       |1.0       |[0.06735896765026778,0.9311048051590026,0.001496182463801036,1.7698827439722675E-5,2.2345899488713172E-5]  |\n",
      "|44     |2       |1.0       |[0.06256973699998766,0.9359995021437736,0.0013955965653541097,1.3919026528282795E-5,2.1245264356229833E-5] |\n",
      "|45     |2       |1.0       |[0.06610262834831372,0.9323996986544616,0.0014596740669209826,1.5951405970069644E-5,2.2047524333534325E-5] |\n",
      "|46     |3       |1.0       |[0.061848880844642115,0.9367989661747934,0.0013195858572774208,1.1527040554174749E-5,2.1040082732828524E-5]|\n",
      "|47     |3       |1.0       |[0.048612453902262986,0.9495744065836044,0.0017890462931229873,7.901089293465477E-6,1.619213171617671E-5]  |\n",
      "|48     |3       |1.0       |[0.05281336962572699,0.9458186177527783,0.001346627418613549,5.249928466443476E-6,1.6135274414855653E-5]   |\n",
      "|49     |3       |1.0       |[0.06427791989859317,0.9332460082030823,0.002454387167853746,2.515361878962155E-6,1.916936859187397E-5]    |\n",
      "|50     |3       |1.0       |[0.08173135097988099,0.9156792337648505,0.002555112860922835,9.541565729528892E-6,2.4760828615872504E-5]   |\n",
      "|51     |3       |1.0       |[0.08177576764294725,0.9156084675311841,0.002577763782942881,1.1358781720880788E-5,2.6642261204724927E-5]  |\n",
      "|52     |3       |1.0       |[0.06920125701006821,0.9293392497155454,0.0014237412807789668,1.2173445211454819E-5,2.357854839594488E-5]  |\n",
      "|53     |3       |1.0       |[0.06868862938350115,0.9288445438875133,0.0024438431682220177,4.274448436823605E-6,1.8709112326916418E-5]  |\n",
      "|54     |2       |1.0       |[0.05107185842060883,0.9477401521439219,0.0011634064164304134,8.220528470157157E-6,1.636249056857881E-5]   |\n",
      "|55     |3       |1.0       |[0.04613334047215184,0.9522786382963122,0.00156353431989928,8.199983774274702E-6,1.6286927862146592E-5]    |\n",
      "|56     |3       |1.0       |[0.05079659370840415,0.9466739530927597,0.0025115068007303698,1.883062302994297E-6,1.6063335802645353E-5]  |\n",
      "|57     |3       |1.0       |[0.05078607113328192,0.9466850315097874,0.0025109534898230712,1.8828638548143876E-6,1.6061003252594955E-5] |\n",
      "|58     |3       |1.0       |[0.07367588410353117,0.9234342487181274,0.002867288428345881,2.6502188321831118E-6,1.9928531163313406E-5]  |\n",
      "|59     |3       |1.0       |[0.04858374021428666,0.949618822349341,0.00177840343562189,2.9925777944983333E-6,1.604142295579232E-5]     |\n",
      "|60     |3       |1.0       |[0.06079813812799173,0.9354156989951906,0.003753181405836753,9.786066500307957E-6,2.3195404480509213E-5]   |\n",
      "|61     |2       |1.0       |[0.05714685390058642,0.9411931906330679,0.0016293584853861046,1.0218551546162304E-5,2.037842941366611E-5]  |\n",
      "|62     |3       |1.0       |[0.05656956169622058,0.9417619578577556,0.001636910827585798,1.0789931190715156E-5,2.077968724733103E-5]   |\n",
      "|63     |2       |1.0       |[0.05779049110410654,0.9406978254604353,0.0014595326134286912,3.0944218456905066E-5,2.1206603572406844E-5] |\n",
      "|64     |2       |1.0       |[0.056347458680926235,0.9419881926325474,0.0016311740745557828,1.2230775874132425E-5,2.0943836096317068E-5]|\n",
      "|65     |2       |1.0       |[0.0581437875115815,0.9403971772726519,0.0014159542347430142,2.296299728226091E-5,2.0117983741214994E-5]   |\n",
      "|66     |3       |1.0       |[0.08401477311507678,0.9130641256500243,0.002856756548396513,3.3245911711653734E-5,3.109877479065746E-5]   |\n",
      "|67     |2       |1.0       |[0.15353569064975764,0.8430639278651109,0.0032181100300772494,1.2533045427289572E-4,5.694100078128165E-5]  |\n",
      "|68     |2       |1.0       |[0.15672100760697333,0.8400781186973968,0.002883802600893388,2.563509098457162E-4,6.072018489072055E-5]    |\n",
      "|69     |2       |1.0       |[0.15353569064975764,0.8430639278651109,0.0032181100300772494,1.2533045427289572E-4,5.694100078128165E-5]  |\n",
      "|70     |2       |1.0       |[0.15179728970493114,0.8447878332338042,0.0032234864008253844,1.356143284305431E-4,5.577633200864432E-5]   |\n",
      "|71     |2       |1.0       |[0.052705548928979634,0.9455735995563155,0.0016845954168341055,1.713535461026729E-5,1.912074326053607E-5]  |\n",
      "|72     |2       |1.0       |[0.14559906929541203,0.8514329349414788,0.002596504827966687,3.168626137234543E-4,5.4628321419067136E-5]   |\n",
      "|73     |3       |1.0       |[0.05687618115703188,0.9399467580608158,0.0031032262439496154,4.9900673190067615E-5,2.393386501271561E-5]  |\n",
      "|74     |3       |1.0       |[0.05989292315896858,0.937019422559667,0.0030080323733683296,5.411057151779795E-5,2.5511336478315144E-5]   |\n",
      "|75     |3       |1.0       |[0.054333810441597505,0.9423416093791819,0.003270581894172248,3.171876635259107E-5,2.2279518695721463E-5]  |\n",
      "|76     |3       |1.0       |[0.0622814187185584,0.9351220722263056,0.0025415293187966244,3.107150338365676E-5,2.3908232955801593E-5]   |\n",
      "|77     |2       |1.0       |[0.06425570831166937,0.9332538393673704,0.002440583493430189,2.520125890977164E-5,2.466756862050403E-5]    |\n",
      "|78     |3       |1.0       |[0.0667933301392893,0.9308194985737538,0.0023235805313150037,3.75355353372998E-5,2.6055220304604956E-5]    |\n",
      "|79     |2       |1.0       |[0.05043610532603039,0.9480632318878561,0.0014459889438736035,3.533645895230483E-5,1.933738328757585E-5]   |\n",
      "|80     |3       |1.0       |[0.055137108645001834,0.9434090552639782,0.0013708180285534582,6.160323720665731E-5,2.1414825259818505E-5] |\n",
      "|81     |2       |1.0       |[0.14758132630947837,0.8491197619922334,0.002892251333034371,3.5290355421844604E-4,5.375681103549005E-5]   |\n",
      "|82     |3       |1.0       |[0.0413324497787187,0.9568254667939489,0.0018146354646835544,1.3286808995669836E-5,1.4161153653271544E-5]  |\n",
      "|83     |2       |1.0       |[0.06792165829208807,0.9294084460517887,0.002624883065272326,2.140761995636144E-5,2.3604970894540694E-5]   |\n",
      "|84     |3       |1.0       |[0.055258689093175925,0.9431834017878133,0.00150825382571019,3.047666384319854E-5,1.917862945751035E-5]    |\n",
      "|85     |3       |1.0       |[0.05309655213814502,0.9455123151320362,0.0013377754452802374,3.451461102404179E-5,1.8842673514811824E-5]  |\n",
      "|86     |2       |1.0       |[0.15295535773825902,0.8423797563366402,0.004083708650587914,5.15651426692863E-4,6.552584781980871E-5]     |\n",
      "|87     |2       |1.0       |[0.15303812439831746,0.8423439985367214,0.00401465136398096,5.37736127332403E-4,6.54895736478469E-5]       |\n",
      "|88     |3       |1.0       |[0.08274983588324507,0.9136351314151462,0.003480988273998572,9.800256950516048E-5,3.6041858104966535E-5]   |\n",
      "|89     |3       |1.0       |[0.0598869824814298,0.9378211935097035,0.0022534480606612446,1.5147359958380357E-5,2.3228588246951602E-5]  |\n",
      "|90     |3       |1.0       |[0.04914231822173683,0.9490092411599909,0.0017806503607605178,4.686914066666761E-5,2.09211168451587E-5]    |\n",
      "|91     |2       |1.0       |[0.1501315707828402,0.8451738845593937,0.004063641442846777,5.669560042412625E-4,6.39472106779828E-5]      |\n",
      "|92     |2       |1.0       |[0.14608190699061563,0.8488432883822904,0.0043320301654542126,6.811750840393123E-4,6.159937760040897E-5]   |\n",
      "|93     |2       |1.0       |[0.1555833721653366,0.8392084761662003,0.004374576657545819,7.69509540537639E-4,6.406547037963061E-5]      |\n",
      "|94     |3       |1.0       |[0.04633104248103773,0.9524797759497053,0.0011278413422339804,4.2114607429224775E-5,1.9225619593625494E-5] |\n",
      "|95     |3       |1.0       |[0.04328693143180994,0.9553910424086288,0.001219824825036479,8.417312558348827E-5,1.802820894151297E-5]    |\n",
      "|96     |2       |1.0       |[0.04274624856734728,0.9559476786515378,0.0012125366053482459,7.58865835606506E-5,1.7649592205882904E-5]   |\n",
      "|97     |2       |1.0       |[0.04258677293564354,0.9563261818793354,0.0010053704351082092,6.417235032666294E-5,1.7502399586022473E-5]  |\n",
      "|98     |2       |1.0       |[0.023613944167803227,0.9743581630510738,0.0018406960807955258,1.7677130520367276E-4,1.0425395123711794E-5]|\n",
      "|99     |2       |1.0       |[0.04282430534311253,0.9558379852129417,0.0012269438908761806,9.314760287464701E-5,1.7617950194985293E-5]  |\n",
      "|100    |2       |1.0       |[0.043924942129797626,0.9548012849206401,0.0011750215764588282,8.053373134968005E-5,1.821764175380289E-5]  |\n",
      "|101    |2       |1.0       |[0.03836458208970678,0.9601912741875828,0.0013589409311882583,6.813380713572826E-5,1.7068984386461172E-5]  |\n",
      "|102    |3       |1.0       |[0.04393184797824334,0.9548291141825052,0.0011462027997944603,7.409421229414578E-5,1.8740827162672118E-5]  |\n",
      "|103    |3       |1.0       |[0.04408381342346132,0.9547723092874073,0.0010586240197351232,6.689383669103412E-5,1.835943270511107E-5]   |\n",
      "|104    |3       |1.0       |[0.04324972210392344,0.9556930650736336,9.973066333211732E-4,4.206243557730095E-5,1.7843753544320294E-5]   |\n",
      "|105    |3       |1.0       |[0.04408381342346132,0.9547723092874073,0.0010586240197351232,6.689383669103412E-5,1.835943270511107E-5]   |\n",
      "|106    |2       |1.0       |[0.045053658304537135,0.9536482567348382,0.0012097998241505292,6.9402283810931E-5,1.8882852663123545E-5]   |\n",
      "|107    |1       |1.0       |[0.043541844684074224,0.9551953988590313,0.001135882817376026,1.088184981058491E-4,1.8055141412602032E-5]  |\n",
      "|108    |2       |1.0       |[0.045053658304537135,0.9536482567348382,0.0012097998241505292,6.9402283810931E-5,1.8882852663123545E-5]   |\n",
      "|109    |2       |1.0       |[0.04338341186814415,0.9553920779971088,0.0011393623940521322,6.680036841094697E-5,1.8347372284103287E-5]  |\n",
      "|110    |2       |1.0       |[0.04611496770062271,0.9526082596678466,0.0011994291898848983,5.831422996177765E-5,1.9029211684202302E-5]  |\n",
      "|111    |2       |1.0       |[0.04338341186814415,0.9553920779971088,0.0011393623940521322,6.680036841094697E-5,1.8347372284103287E-5]  |\n",
      "|112    |2       |1.0       |[0.12365851334291472,0.8731214656958161,0.0023032293992944868,8.635780131369059E-4,5.321354883797189E-5]   |\n",
      "|113    |3       |1.0       |[0.04898640185575307,0.9497260249260465,0.0012331513780497135,3.567879523275653E-5,1.874304491777454E-5]   |\n",
      "|114    |2       |1.0       |[0.041553009789519035,0.9570183710556991,0.0013522972238638633,5.962318874597578E-5,1.6698742171947907E-5] |\n",
      "|115    |2       |1.0       |[0.06588996640144508,0.9317894133731623,0.002264501834622247,3.0179544285742545E-5,2.5938846484458345E-5]  |\n",
      "|116    |2       |1.0       |[0.04842461039798967,0.950278023733077,0.001257173371540015,2.127886939805116E-5,1.8913627995287598E-5]    |\n",
      "|117    |2       |1.0       |[0.05008046324771978,0.9485750720714756,0.0012748416177917618,5.0171674287417024E-5,1.9451388725420705E-5] |\n",
      "|118    |3       |1.0       |[0.04810202370191028,0.9505263052637251,0.0013244090113275144,2.8979790009142473E-5,1.828223302815049E-5]  |\n",
      "|119    |2       |1.0       |[0.13327755347369308,0.8637947435773708,0.002550356596280694,3.269223020683434E-4,5.042405058709971E-5]    |\n",
      "|120    |2       |1.0       |[0.05157229807246186,0.9471121048315077,0.0012618657962567776,3.4422306839765446E-5,1.9308992934075163E-5] |\n",
      "|121    |2       |1.0       |[0.04833586812436507,0.9504984675086809,0.0011251607685294296,2.2604157427214987E-5,1.789944099743684E-5]  |\n",
      "|122    |2       |1.0       |[0.11893495707588894,0.8785117716177243,0.0023683556384107456,1.427673512786982E-4,4.214831669726124E-5]   |\n",
      "|123    |2       |1.0       |[0.04207956588123992,0.9567461363733539,0.0011462689327359406,1.3462324121142163E-5,1.4566488549318348E-5] |\n",
      "|124    |3       |1.0       |[0.05121369324416615,0.9475188434348251,0.0012403252796492667,1.1124516816743971E-5,1.601352454274954E-5]  |\n",
      "|125    |3       |1.0       |[0.06865671419202919,0.9289774980907927,0.002242438296734008,9.739543949543794E-5,2.5953980948792427E-5]   |\n",
      "|126    |3       |1.0       |[0.046260688601765125,0.9525030077157091,0.0011630924763326588,5.485558448153603E-5,1.835562171165316E-5]  |\n",
      "|127    |2       |1.0       |[0.10834008730549552,0.8891377199806938,0.0024016020198181108,8.329628470794854E-5,3.7294409284624745E-5]  |\n",
      "|128    |2       |1.0       |[0.05403375373179537,0.9445782501670283,0.0013632435910762388,9.162677436228917E-6,1.5589832663867424E-5]  |\n",
      "|129    |2       |1.0       |[0.06282063512988091,0.9356709653183151,0.0014501157095008824,3.693238986333906E-5,2.1351452439781074E-5]  |\n",
      "|130    |2       |1.0       |[0.056571394622499695,0.9419584212830138,0.0014158801256455393,3.519705472819112E-5,1.9106914112844226E-5] |\n",
      "|131    |3       |1.0       |[0.08602487886983734,0.9115190309833061,0.0023615424380416376,6.384922254978426E-5,3.069848626515995E-5]   |\n",
      "|132    |2       |1.0       |[0.05530463773092385,0.9433512145008889,0.0012981646345898267,2.6846073809682816E-5,1.9137059787535803E-5] |\n",
      "|133    |2       |1.0       |[0.05425906067714294,0.9443423617385502,0.0013459203654908169,3.2779886731091744E-5,1.9877332085027886E-5] |\n",
      "|134    |2       |1.0       |[0.05211385277091928,0.9465401038292098,0.0013040628024385827,2.351546505042652E-5,1.846513238182684E-5]   |\n",
      "|135    |2       |1.0       |[0.052983239042972835,0.9456542740069935,0.0013196282895244944,2.4236781344907614E-5,1.862187916440214E-5] |\n",
      "|136    |2       |1.0       |[0.0536283355236085,0.9450162547845573,0.001315785178004141,2.1141397227593435E-5,1.848311660259003E-5]    |\n",
      "|137    |2       |1.0       |[0.054382859182685095,0.9442855536843751,0.0012941048945012727,1.901573979047089E-5,1.846649864796598E-5]  |\n",
      "|138    |3       |1.0       |[0.050708024876733725,0.9480156936667171,0.001237158820685506,2.075660435738121E-5,1.836603150608572E-5]   |\n",
      "|139    |2       |1.0       |[0.051355526438864846,0.947305379197012,0.0013004717702240491,2.0590000951319577E-5,1.8032592947852947E-5] |\n",
      "|140    |3       |1.0       |[0.04668254348829094,0.9516453285809592,0.0016420297845235515,1.2841733879243894E-5,1.7256412347070794E-5] |\n",
      "|141    |3       |1.0       |[0.0849431066447568,0.912337191989787,0.0026518370686255807,3.843641523795745E-5,2.942788159260697E-5]     |\n",
      "|142    |3       |1.0       |[0.06094393549776393,0.9367912885123345,0.0022368030690782287,7.881442391196756E-6,2.0091478432037E-5]     |\n",
      "|143    |3       |1.0       |[0.060903341665328924,0.9368334798454124,0.002235217551939299,7.878798470364356E-6,2.008213884908214E-5]   |\n",
      "|144    |2       |1.0       |[0.039485230158322664,0.9590597697994759,0.0012944713878768885,1.4610687166092745E-4,1.4421782663719358E-5]|\n",
      "|145    |3       |1.0       |[0.050854404898903915,0.9477424188730581,0.0013027199476270664,8.026862171444582E-5,2.0187658696417972E-5] |\n",
      "|146    |2       |1.0       |[0.18596194507732225,0.8110657316738452,0.0029074728096479424,1.3501976418155832E-5,5.134846276630693E-5]  |\n",
      "|147    |2       |1.0       |[0.15313003650956808,0.8455971294850718,0.0012112545911442442,1.9602342617347565E-5,4.1977071598450904E-5] |\n",
      "|148    |2       |1.0       |[0.25860232221756085,0.7350139135189129,0.006299707514198485,1.5025284891236112E-5,6.903146443651028E-5]   |\n",
      "|149    |2       |1.0       |[0.2126710082211269,0.7824796845177053,0.004784020212689803,8.989315412581156E-6,5.629773306518995E-5]     |\n",
      "|150    |2       |1.0       |[0.17134779835581687,0.8265487668196196,0.002052401914030995,6.220445795827564E-6,4.481246473688922E-5]    |\n",
      "|151    |2       |1.0       |[0.16971391158778112,0.8284257896271751,0.0018042246745263875,7.935609092017037E-6,4.813850142546617E-5]   |\n",
      "|152    |2       |1.0       |[0.15415632426218026,0.8433447174336173,0.002443568323283276,8.679147442085098E-6,4.6710833477209415E-5]   |\n",
      "|153    |2       |1.0       |[0.16531185243485774,0.8328501052722199,0.001787242634866293,4.3828382237712185E-6,4.641681983223988E-5]   |\n",
      "|154    |2       |1.0       |[0.20660215325842096,0.7885297057406541,0.004808202262759247,4.9919584933144E-6,5.4946779672465125E-5]     |\n",
      "|155    |2       |1.0       |[0.3941895922954569,0.6023510483924888,0.0033021162291842544,5.296095376788487E-5,1.0428212910208466E-4]   |\n",
      "|156    |3       |1.0       |[0.18364874340450854,0.8099894916644832,0.006304392892050568,5.19337944948395E-6,5.217865950836918E-5]     |\n",
      "|157    |2       |1.0       |[0.1635088985666423,0.8346743636929244,0.001763032309775672,7.6071255106076775E-6,4.6098305147276965E-5]   |\n",
      "|158    |2       |1.0       |[0.3962342081481593,0.6004586188571437,0.0031347864807961616,6.328451287181651E-5,1.0910200102908011E-4]   |\n",
      "|159    |3       |1.0       |[0.30526619816924866,0.6902074939224851,0.004417784178912657,2.6789834720329627E-5,8.173389463324167E-5]   |\n",
      "|160    |2       |1.0       |[0.16564059602520614,0.8325587274022292,0.0017481136010259561,5.851258376272194E-6,4.6711713162274136E-5]  |\n",
      "|161    |3       |1.0       |[0.3732460203552167,0.617129206839831,0.009508413152055577,1.708116888423112E-5,9.927848401259673E-5]      |\n",
      "|162    |2       |1.0       |[0.20914139453865677,0.7864220279995375,0.004374893274195613,4.395141868557821E-6,5.7289045741361036E-5]   |\n",
      "|163    |2       |0.0       |[0.5223342908051533,0.47045552799106233,0.006668444291933847,3.617059195198101E-4,1.8003099233081097E-4]   |\n",
      "|164    |3       |1.0       |[0.34374657558350546,0.6511428985124361,0.0048504364255409355,1.5101335738515557E-4,1.0907612113230194E-4] |\n",
      "|165    |3       |1.0       |[0.35060988513557567,0.644119493178755,0.005000907664818342,1.5846390951167376E-4,1.1125011133913517E-4]   |\n",
      "|166    |2       |1.0       |[0.21852925463657782,0.7793236011928898,0.0020347299643294456,4.308227958679407E-5,6.933192661587506E-5]   |\n",
      "|167    |3       |1.0       |[0.20287801543253547,0.7951295621147156,0.0018701960496610383,5.4627801417232534E-5,6.759860167073319E-5]  |\n",
      "|168    |2       |1.0       |[0.2095953744056132,0.7880871526535572,0.002197165969191807,5.3038227727156945E-5,6.726874391064637E-5]    |\n",
      "|169    |3       |1.0       |[0.20381387397623071,0.7941454889124814,0.001924295810078035,5.001852503517181E-5,6.632277617451925E-5]    |\n",
      "|170    |2       |1.0       |[0.21882423230367595,0.7789803820377613,0.00207821108187326,4.75740683446328E-5,6.960050834492668E-5]      |\n",
      "|171    |2       |1.0       |[0.19292382747116787,0.8050507928422421,0.001923339726412628,4.085286831166552E-5,6.118709186581333E-5]    |\n",
      "|172    |3       |1.0       |[0.34996159760407514,0.6453449769959322,0.004463680090162628,1.1717539400093175E-4,1.125699158290992E-4]   |\n",
      "|173    |3       |1.0       |[0.20097690560159845,0.7969675495809795,0.0019577391026605277,3.309732078447358E-5,6.47083939770019E-5]    |\n",
      "|174    |2       |1.0       |[0.1734195633269149,0.8252563884487227,0.0012189576682573402,5.0260075357872086E-5,5.483048074735074E-5]   |\n",
      "|175    |3       |1.0       |[0.3991544556259664,0.5929982276104564,0.007580447373193607,1.4568202761910032E-4,1.2118736276454338E-4]   |\n",
      "|176    |2       |1.0       |[0.377247986860254,0.6201755489363564,0.002031865821714997,4.245286265864824E-4,1.2006975508804318E-4]     |\n",
      "|177    |3       |1.0       |[0.278727318801217,0.717193292808014,0.003805273408806398,1.8016868751555789E-4,9.394629444720972E-5]      |\n",
      "|178    |3       |1.0       |[0.1586175344933263,0.8400007779214913,0.0013172111680563605,1.8858855587350644E-5,4.561756153865655E-5]   |\n",
      "|179    |3       |1.0       |[0.37146782449224397,0.6205185359870434,0.007818215548588072,8.408904957064489E-5,1.1133492255400861E-4]   |\n",
      "|180    |2       |1.0       |[0.4469202159925611,0.5458641708227966,0.007027678435991868,6.022405373820537E-5,1.2771069491228603E-4]    |\n",
      "|181    |2       |1.0       |[0.14615714106547711,0.8513437049306589,0.002447887338171877,8.035303841039031E-6,4.323136185103933E-5]    |\n",
      "|182    |2       |1.0       |[0.34104724743307996,0.6559183391879934,0.0028873951577766135,4.6142544287891145E-5,1.0087567686222412E-4] |\n",
      "|183    |2       |1.0       |[0.16234218908559206,0.836205902556918,0.001400133938381399,8.047099319867224E-6,4.372731978873086E-5]     |\n",
      "|184    |2       |1.0       |[0.16743314468558385,0.8312728664004755,0.0011729823959358986,6.6534002114396E-5,5.447251589047687E-5]     |\n",
      "|185    |2       |1.0       |[0.16173999787266913,0.8369400361971501,0.0012169099702909188,5.242336775385891E-5,5.063259213624454E-5]   |\n",
      "|186    |2       |1.0       |[0.1571320927578278,0.8416097099915404,0.0011813890156960358,3.070103469714407E-5,4.610720023858857E-5]    |\n",
      "|187    |3       |1.0       |[0.3428343845409243,0.6524976522304485,0.00456122870748457,1.0378639213488358E-5,9.635588192919532E-5]     |\n",
      "|188    |3       |1.0       |[0.16209458361130516,0.8356187266027482,0.002238964862603476,3.067433406074821E-6,4.4657489937024445E-5]   |\n",
      "|189    |3       |1.0       |[0.32682291449936507,0.6676795353242521,0.005403356259788342,2.60977853447964E-6,9.158413806003901E-5]     |\n",
      "|190    |3       |1.0       |[0.33351704403711646,0.6608129596953682,0.005573799531397472,2.7399251107499046E-6,9.345681100698846E-5]   |\n",
      "|191    |3       |1.0       |[0.34529661760157565,0.6501946822506015,0.0044024931798431035,1.2540983059749051E-5,9.366598491999286E-5]  |\n",
      "|192    |3       |1.0       |[0.33759654807677425,0.6571894522754691,0.005119395573373386,9.11235223868919E-6,8.54917221446099E-5]      |\n",
      "|193    |2       |1.0       |[0.15992934718041094,0.83775207288423,0.002273691860102606,2.309540204098092E-6,4.2578535052250964E-5]     |\n",
      "|194    |3       |1.0       |[0.17156294856520354,0.82599205170719,0.0024009762282660475,1.6197247182146674E-6,4.240377462207047E-5]    |\n",
      "|195    |2       |1.0       |[0.1531685971685636,0.8436357626522768,0.0031536739579365006,2.0044714570685056E-6,3.996174976583137E-5]   |\n",
      "|196    |3       |1.0       |[0.2121036393410506,0.785227426595766,0.0025930850312843325,1.2861477339841659E-5,6.298755455911514E-5]    |\n",
      "|197    |3       |1.0       |[0.16390810656963176,0.834825757336893,0.0011547166972695404,5.906317605770745E-5,5.235622014798051E-5]    |\n",
      "|198    |2       |1.0       |[0.16820160472648513,0.8304981476948503,0.0011946995281468756,5.428543902831825E-5,5.126261148939037E-5]   |\n",
      "|199    |3       |1.0       |[0.3697060810770716,0.6218162558231878,0.008252427798288255,1.1917251282441732E-4,1.060627886280179E-4]    |\n",
      "|200    |3       |1.0       |[0.24040579137080437,0.7552210814979842,0.004182185375317931,9.953153215587393E-5,9.141022373762562E-5]    |\n",
      "|201    |2       |1.0       |[0.13667842935340002,0.8618036408006446,0.0013964227391689962,7.329368880065969E-5,4.821341798571333E-5]   |\n",
      "|202    |2       |1.0       |[0.12904794396753663,0.8693947067732657,0.0014243174239360066,8.920405433382839E-5,4.3827780927919755E-5]  |\n",
      "|203    |2       |1.0       |[0.160173813790229,0.8373128896849192,0.002437965618340901,2.2323383300726546E-5,5.3007523210031936E-5]    |\n",
      "|204    |2       |1.0       |[0.1601741195228563,0.8373125788786446,0.0024379706060755945,2.232340224285011E-5,5.300759018055101E-5]    |\n",
      "|205    |2       |1.0       |[0.1536304700897944,0.8440097491193048,0.0022873220742598344,2.1308498157070306E-5,5.115021848371258E-5]   |\n",
      "|206    |3       |1.0       |[0.13721107717984987,0.8602864448049427,0.0024103402006330227,4.7689362719318594E-5,4.4448451855239015E-5] |\n",
      "|207    |2       |1.0       |[0.1536299971443295,0.8440102297420615,0.0022873145310683893,2.130846870635022E-5,5.115011383437578E-5]    |\n",
      "|208    |2       |1.0       |[0.1519168660365836,0.845856052813861,0.002151115027947614,2.2141131438765244E-5,5.382499016911948E-5]     |\n",
      "|209    |2       |1.0       |[0.1298689969342095,0.8669467116092607,0.0030694923555441567,6.784429969858164E-5,4.6954801287169596E-5]   |\n",
      "|210    |2       |1.0       |[0.1627300644753657,0.8349792631491748,0.002224882584610364,1.1005272427011573E-5,5.47845184220958E-5]     |\n",
      "|211    |2       |1.0       |[0.1664420904124228,0.8310931844211124,0.0023846733994966436,2.3299631596411403E-5,5.6752135371902966E-5]  |\n",
      "|212    |2       |1.0       |[0.1664417120065423,0.8310935687492859,0.002384667585608584,2.32996082679911E-5,5.675205029515213E-5]      |\n",
      "|213    |2       |1.0       |[0.16644284722624317,0.8310924157626702,0.002384685027307677,2.3299678253248397E-5,5.6752305525610984E-5]  |\n",
      "|214    |2       |1.0       |[0.16462932902372257,0.8329197154801199,0.0023742654944776316,2.103983531513073E-5,5.5650166364494776E-5]  |\n",
      "|215    |2       |1.0       |[0.15670863499342438,0.8407757303879712,0.002446695370663701,1.8831806528561933E-5,5.010744141225334E-5]   |\n",
      "|216    |2       |1.0       |[0.12973622118900613,0.867774119684175,0.002406367638516789,4.0531357138826435E-5,4.276013116319498E-5]    |\n",
      "|217    |2       |1.0       |[0.15370043576808126,0.8440507516050034,0.0021825082932612878,1.4676528017585452E-5,5.162780563644253E-5]  |\n",
      "|218    |3       |1.0       |[0.2453534321181225,0.7484083101129356,0.006116593182568409,3.5407816959631016E-5,8.625676941383964E-5]    |\n",
      "|219    |2       |1.0       |[0.16746684364684275,0.829832201463874,0.0023863531554250477,2.472225532775857E-4,6.737918058039473E-5]    |\n",
      "|220    |2       |1.0       |[0.18596162245798264,0.8111548429696567,0.002474157325121673,3.3504141736800957E-4,7.433582987118377E-5]   |\n",
      "|221    |3       |1.0       |[0.17122868521946427,0.8259411697446417,0.0022854914023834254,4.732417378231468E-4,7.141189568747E-5]      |\n",
      "|222    |3       |1.0       |[0.16735452449738286,0.8298259717466114,0.0023352946754103382,4.1686654544908185E-4,6.734253514623125E-5]  |\n",
      "|223    |3       |1.0       |[0.31517110088336436,0.6752609669952724,0.00854872220764259,8.993177676549758E-4,1.1989214606584148E-4]    |\n",
      "|224    |3       |1.0       |[0.12765301890472774,0.8706990186498443,0.0013386417091229804,2.61935368023705E-4,4.738536828141316E-5]    |\n",
      "|225    |2       |1.0       |[0.32204373565575667,0.6728842980534423,0.0023402727006090053,0.0026059436848103012,1.2574990538170017E-4] |\n",
      "|226    |2       |1.0       |[0.14566462316318599,0.852796817834601,0.0013980255914650794,8.74415099451131E-5,5.309190080301055E-5]     |\n",
      "|227    |2       |1.0       |[0.13636667015831688,0.8621045228862034,0.0013964160084580803,8.318514025068116E-5,4.920580677088422E-5]   |\n",
      "|228    |3       |1.0       |[0.2734360080663065,0.7228854554192946,0.0033132058920491863,2.695627430494214E-4,9.576787930028796E-5]    |\n",
      "|229    |3       |1.0       |[0.23234051088010105,0.7642252639657156,0.003060913809713769,2.920194188708649E-4,8.129192559877562E-5]    |\n",
      "|230    |2       |1.0       |[0.12582429532812225,0.8725792688093492,0.0015128200421362964,3.922305467050342E-5,4.4392765721536265E-5]  |\n",
      "|231    |2       |1.0       |[0.11695304720066861,0.8816993811238016,0.0012403466718728118,6.594615039212719E-5,4.127885326486418E-5]   |\n",
      "|232    |3       |1.0       |[0.22902500355627015,0.7663334521009951,0.004425448868221294,1.312243679433708E-4,8.487110657005822E-5]    |\n",
      "|233    |2       |1.0       |[0.12307543424184987,0.8754830728571668,0.0012418123051428445,1.5457834511896926E-4,4.510225072149057E-5]  |\n",
      "|234    |2       |1.0       |[0.15180059970507562,0.8461222321001576,0.002002592897553356,2.2825095237622746E-5,5.175020197593343E-5]   |\n",
      "|235    |2       |1.0       |[0.12141009710693848,0.8765479236415209,0.0019791642230087125,2.1308641663364868E-5,4.150638686849223E-5]  |\n",
      "|236    |2       |1.0       |[0.1323147118809713,0.8654095074630502,0.0021939668213069663,3.677628672611341E-5,4.5037547945326776E-5]   |\n",
      "|237    |2       |1.0       |[0.12024910252718644,0.877850882004147,0.0018369454038350628,1.970681549440055E-5,4.336324933700673E-5]    |\n",
      "|238    |2       |1.0       |[0.10191149088511275,0.8958593695721466,0.002174155453247353,1.5053670585679594E-5,3.993041890755754E-5]   |\n",
      "|239    |2       |1.0       |[0.11845687566847592,0.8801829572066999,0.0011969474880292555,1.1738148581892032E-4,4.583815097596058E-5]  |\n",
      "|240    |2       |1.0       |[0.12634552355296622,0.8721357637536835,0.0013428020610226378,1.283976472491285E-4,4.7512985078473336E-5]  |\n",
      "|241    |2       |1.0       |[0.11378500852922455,0.8850244719151391,0.001054879330905178,9.10228741939803E-5,4.461735053701437E-5]     |\n",
      "|242    |2       |1.0       |[0.11585751453209518,0.8826673227150504,0.0014000691271797566,3.274281593065154E-5,4.235080974390212E-5]   |\n",
      "|243    |3       |1.0       |[0.24109416540855266,0.7556288984453293,0.0030553140986562402,1.3670094737550342E-4,8.492110008637294E-5]  |\n",
      "|244    |2       |1.0       |[0.15047572559346872,0.8471035484695868,0.0023255541842386518,4.2736869932534434E-5,5.2434882773384374E-5] |\n",
      "|245    |3       |1.0       |[0.1433524541835754,0.8544066660136855,0.0021366591909990763,5.573964323652339E-5,4.848096850336699E-5]    |\n",
      "|246    |3       |1.0       |[0.1355695873170327,0.8620995805730978,0.0022387299900600843,4.809589428335438E-5,4.40062255263593E-5]     |\n",
      "|247    |2       |1.0       |[0.18176513693492521,0.8153723125892488,0.0024412352237448993,3.4590103061964416E-4,7.541422146141596E-5]  |\n",
      "|248    |2       |1.0       |[0.13759358987315207,0.860555413124175,0.0017149037326508816,8.84709368046921E-5,4.7622333217238537E-5]    |\n",
      "|249    |2       |1.0       |[0.15017964087480562,0.8474959243648851,0.0022523028774012414,2.6365717569790536E-5,4.57661653382133E-5]   |\n",
      "|250    |3       |1.0       |[0.14251086184247416,0.855235627254134,0.002171145134073849,3.841005112750666E-5,4.395571819033987E-5]     |\n",
      "|251    |3       |1.0       |[0.16470680471152074,0.8329382594637029,0.0020542393475822198,2.3547094450748166E-4,6.522553268654108E-5]  |\n",
      "|252    |3       |1.0       |[0.16291147932185251,0.8347666759467509,0.002045254464185247,2.1263169236212113E-4,6.395857484925438E-5]   |\n",
      "|253    |2       |1.0       |[0.18577483859172686,0.8116114795157028,0.0023283322646444277,2.1679340362321136E-4,6.855622430260522E-5]  |\n",
      "|254    |3       |1.0       |[0.34439253297095923,0.647787772094235,0.006448750929458914,0.0012386688175705586,1.3227518777627714E-4]   |\n",
      "|255    |2       |1.0       |[0.19077309262157732,0.8017206662946176,0.007205854695147859,2.2214664616517461E-4,7.823974249195127E-5]   |\n",
      "|256    |3       |1.0       |[0.16165744829309522,0.8359509259317032,0.0021112074015389518,2.1747945427509258E-4,6.29389193875401E-5]   |\n",
      "|257    |2       |1.0       |[0.21420160379257147,0.7821810701256696,0.0031844644997344324,3.507442068210125E-4,8.211737520352347E-5]   |\n",
      "|258    |2       |1.0       |[0.2142000758358516,0.7821826239326969,0.0031844400377122025,3.5074319232788624E-4,8.211700141149303E-5]   |\n",
      "|259    |2       |1.0       |[0.16304914819894703,0.8336959847952724,0.0029940989401584205,1.9433092486078774E-4,6.643714076135834E-5]  |\n",
      "|260    |2       |1.0       |[0.2140216266056788,0.7806003079377676,0.0051313499398044655,1.6227667233804926E-4,8.443884441112431E-5]   |\n",
      "|261    |2       |1.0       |[0.21204969117809988,0.7846226329773168,0.0029326428418530144,3.1126402029927453E-4,8.376898243120056E-5]  |\n",
      "|262    |2       |1.0       |[0.38031196075705703,0.6143445431002699,0.0034516395730698835,0.0017377233239254902,1.5413324567777742E-4] |\n",
      "|263    |2       |1.0       |[0.18271855390977912,0.8147830747886464,0.002213173784597628,2.1468638249896852E-4,7.051113447806436E-5]   |\n",
      "|264    |3       |1.0       |[0.40212386515653525,0.5923113729929422,0.0036491895890878134,0.0017586078808990242,1.5696438053560907E-4] |\n",
      "|265    |3       |1.0       |[0.21556840293021215,0.7791067911913621,0.005106811082292469,1.3289478262784683E-4,8.510001350539863E-5]   |\n",
      "|266    |2       |1.0       |[0.11601609955931605,0.8827606381760545,0.0010979344805555967,7.954999669294333E-5,4.5777787380980836E-5]  |\n",
      "|267    |2       |1.0       |[0.2931209957728139,0.7034660934119136,0.0030421889550157717,2.5466918984341704E-4,1.1605267041316977E-4]  |\n",
      "|268    |2       |1.0       |[0.13169493914223868,0.8668166853987527,0.001318173843260698,1.1985059724185794E-4,5.035101850600873E-5]   |\n",
      "|269    |2       |1.0       |[0.1694703629805727,0.8279185921944572,0.002190328758205005,3.505577842858708E-4,7.01582824793162E-5]      |\n",
      "|270    |3       |1.0       |[0.26554226380581974,0.7293733084713284,0.004044718971684244,9.386331033313658E-4,1.0107564783617921E-4]   |\n",
      "|271    |3       |1.0       |[0.30277064224982164,0.6846119431891857,0.01123566285379968,0.0012544430465816529,1.2730866061121235E-4]   |\n",
      "|272    |3       |1.0       |[0.29243161699517445,0.6968364282286247,0.009604633138952287,0.0010088999971302958,1.1842164011816753E-4]  |\n",
      "|273    |2       |1.0       |[0.14771065120882895,0.8479964459100863,0.003992835321556145,2.3842693065629253E-4,6.164062887240695E-5]   |\n",
      "|274    |3       |1.0       |[0.14346914992455878,0.8548496928361119,0.0013982335032729275,2.3273135303264505E-4,5.019238302368403E-5]  |\n",
      "|275    |3       |1.0       |[0.2962774508886288,0.6912045884209592,0.011361955719741081,0.0010369261606488016,1.1907881002203317E-4]   |\n",
      "|276    |3       |1.0       |[0.2582165221325744,0.737293590178514,0.0034395940630783895,9.58331483314744E-4,9.196214251853613E-5]      |\n",
      "|277    |3       |1.0       |[0.2582173905648194,0.7372927078535252,0.00343960657426576,9.58332674278338E-4,9.196233311122332E-5]       |\n",
      "|278    |2       |1.0       |[0.12710543129030283,0.8712931781873157,0.001429659847121172,1.2844461820063618E-4,4.328605705960411E-5]   |\n",
      "|279    |2       |1.0       |[0.1218310465941682,0.8767559592402709,0.0012567642410809582,1.1337158053313683E-4,4.285834394700992E-5]   |\n",
      "|280    |3       |1.0       |[0.22816097983738035,0.7661088045237584,0.005422883136681265,2.209818556634883E-4,8.635064651652536E-5]    |\n",
      "|281    |2       |1.0       |[0.16917500318174267,0.8250949610526157,0.005671364303808812,7.890767074908093E-6,5.078069475772776E-5]    |\n",
      "|282    |2       |1.0       |[0.18969639311667147,0.8075900093792422,0.0024790128654699818,1.6582122297905096E-4,6.876341563710053E-5]  |\n",
      "|283    |2       |1.0       |[0.18975856886088102,0.8075269228965536,0.002479885917419406,1.658442723140369E-4,6.87780528318591E-5]     |\n",
      "|284    |3       |1.0       |[0.15079682409939002,0.8469466177339827,0.0021709930533592124,3.623631980352562E-5,4.932879346451875E-5]   |\n",
      "|285    |2       |1.0       |[0.17808379210869618,0.819011516264488,0.002358134322929265,4.749533224060906E-4,7.160398148037676E-5]     |\n",
      "|286    |2       |1.0       |[0.14312504116817262,0.8552439188817449,0.001325117352539708,2.527751807215166E-4,5.314741682129367E-5]    |\n",
      "|287    |2       |1.0       |[0.13331934019823574,0.8651450820888203,0.0012104010275959622,2.7204989912244686E-4,5.312678622546337E-5]  |\n",
      "|288    |3       |1.0       |[0.2432380059022323,0.7529554704509875,0.0032290515479181515,4.8733034308687397E-4,9.01417557750834E-5]    |\n",
      "|289    |2       |1.0       |[0.16784407467125018,0.8289876762623124,0.002920325140293677,1.8166087154457224E-4,6.626305459921336E-5]   |\n",
      "|290    |2       |1.0       |[0.13591512713619888,0.8625974382624951,0.001224729536472324,2.1175096038134492E-4,5.0954104452221016E-5]  |\n",
      "|291    |3       |1.0       |[0.2796245802285768,0.7155179789625472,0.003650427059716744,0.0010976309884103903,1.093827607490482E-4]    |\n",
      "|292    |2       |1.0       |[0.12373266520931896,0.8748520794547945,0.0012375752432690107,1.319482067857359E-4,4.573188583190954E-5]   |\n",
      "|293    |2       |1.0       |[0.11362202062835393,0.8849792658738459,0.0012767015119075837,8.204934400835427E-5,3.9962641884307846E-5]  |\n",
      "|294    |3       |1.0       |[0.21770205033064524,0.7781408404741138,0.003970300661000931,1.0477465122675114E-4,8.203388301338768E-5]   |\n",
      "|295    |2       |1.0       |[0.12110184342021084,0.8770536620595288,0.0017819282951259169,1.8806091614565085E-5,4.376013351974329E-5]  |\n",
      "|296    |2       |1.0       |[0.3037495078937724,0.6924430929507769,0.003523773072676116,1.768730569483838E-4,1.067530258261762E-4]     |\n",
      "|297    |3       |1.0       |[0.24438300934994137,0.751655747180437,0.003266026964899522,6.029088625033012E-4,9.230764221899928E-5]     |\n",
      "|298    |2       |1.0       |[0.14320667581872928,0.8545445892374773,0.002179212454167081,2.0945571848913864E-5,4.857691777724948E-5]   |\n",
      "|299    |2       |1.0       |[0.156616594264324,0.8410457976981167,0.002256824904730209,2.603895099443004E-5,5.47441818347193E-5]       |\n",
      "|300    |2       |1.0       |[0.17636055566129105,0.8210046807682442,0.002403313911057574,1.5970608951615183E-4,7.17435698908952E-5]    |\n",
      "|301    |3       |1.0       |[0.15477766716443705,0.8428246422381539,0.002159005267496922,1.7476129108463632E-4,6.3924038827322E-5]     |\n",
      "|302    |3       |1.0       |[0.15483671502131968,0.8427196096875164,0.0021842128871138,1.9586788865863416E-4,6.359451539151592E-5]     |\n",
      "|303    |3       |1.0       |[0.1359340545941103,0.862377007011075,0.0012823858739219622,3.5283792286586624E-4,5.3714598026767424E-5]   |\n",
      "|304    |3       |1.0       |[0.12401457637503897,0.8743717760170948,0.0012336760120617003,3.321586087909547E-4,4.781298701357194E-5]   |\n",
      "|305    |2       |1.0       |[0.13029009778122103,0.8673012598890177,0.002012743169228113,3.4001570940740743E-4,5.588345112565753E-5]   |\n",
      "|306    |2       |1.0       |[0.13807481776877334,0.8601923905547862,0.001388765603456849,2.914348346980945E-4,5.2591238285450946E-5]   |\n",
      "|307    |3       |1.0       |[0.14281262797854177,0.8547628186697158,0.0023069509482155676,7.027029476555005E-5,4.733210876124945E-5]   |\n",
      "|308    |3       |1.0       |[0.12592447950426824,0.870957947004593,0.003000470425554784,7.251723032172534E-5,4.458583526254075E-5]     |\n",
      "|309    |3       |1.0       |[0.14121305481183472,0.8563809483209827,0.0022961629146218627,6.343529992270031E-5,4.63986526378325E-5]    |\n",
      "|310    |3       |1.0       |[0.12448836009887083,0.8724166678357533,0.0029858248661817524,6.544982248677754E-5,4.369737670729905E-5]   |\n",
      "|311    |2       |1.0       |[0.33640292492427937,0.6592998123340267,0.003917272932119105,2.624251122511143E-4,1.1756469732389264E-4]   |\n",
      "|312    |2       |1.0       |[0.13725165892739352,0.8603479278094788,0.002293324846660313,6.155266110164443E-5,4.5535755365602204E-5]   |\n",
      "|313    |3       |1.0       |[0.13558504225559603,0.8620991821435188,0.002220492526334488,5.019667665505759E-5,4.5086397895666734E-5]   |\n",
      "|314    |3       |1.0       |[0.14037026351809603,0.8572880935300998,0.0022428635575413806,5.237554992388281E-5,4.6403844338880974E-5]  |\n",
      "|315    |2       |1.0       |[0.3606673733479661,0.6348969642412512,0.00410708108161292,2.0546803216428075E-4,1.2311329700542993E-4]    |\n",
      "|316    |2       |1.0       |[0.15888138023866352,0.8387535137587141,0.0022875924701823967,2.1760825561729803E-5,5.5752706878219354E-5] |\n",
      "|317    |2       |1.0       |[0.13306211559542708,0.8642042586065153,0.0026735943438638663,1.6138781680892507E-5,4.389267251289476E-5]  |\n",
      "|318    |2       |1.0       |[0.176006512454711,0.8210086253436292,0.002459419250109484,4.540570968887385E-4,7.13858546618202E-5]       |\n",
      "|319    |2       |1.0       |[0.1685144729130521,0.828784930840627,0.0023484080069362014,2.8732550049518767E-4,6.486273888948768E-5]    |\n",
      "|320    |2       |1.0       |[0.17013039651391307,0.8270185422180902,0.0024986865599378226,2.874725970948233E-4,6.490211096386992E-5]   |\n",
      "|321    |2       |1.0       |[0.17013026819729254,0.8270186726834173,0.0024986845383340663,2.874725021894933E-4,6.490207876659985E-5]   |\n",
      "|322    |3       |1.0       |[0.24558639056104895,0.7473992448731555,0.005254240220517648,0.0016603266681667662,9.979767711118373E-5]   |\n",
      "|323    |2       |1.0       |[0.1392935828177372,0.8589472052281255,0.0013773212894943395,3.2985370238347507E-4,5.203696225958666E-5]   |\n",
      "|324    |2       |1.0       |[0.15412199782597802,0.844300431168867,0.0013983948647549058,1.193840867471864E-4,5.97920536528611E-5]     |\n",
      "|325    |2       |1.0       |[0.13937505167158112,0.8584058872708265,0.002149010855385812,2.3563117647237917E-5,4.6487084559591104E-5]  |\n",
      "|326    |2       |1.0       |[0.3022725144476399,0.6941955948400759,0.0024326672085849556,9.829038239635964E-4,1.1631967973566344E-4]   |\n",
      "|327    |2       |1.0       |[0.1355127126284775,0.8620880339518899,0.0023178711017517806,3.969751352500922E-5,4.1684804355965476E-5]   |\n",
      "|328    |3       |1.0       |[0.13551292568359072,0.8620878169255946,0.002317875000174693,3.969754226748398E-5,4.168484837274717E-5]    |\n",
      "|329    |3       |0.0       |[0.5710262261573666,0.4136220889923826,0.008330805857343922,0.006804266180551343,2.166128123555865E-4]     |\n",
      "|330    |2       |1.0       |[0.17202250608494726,0.825023009271099,0.002621939753878657,2.6873712535135816E-4,6.38077647239893E-5]     |\n",
      "|331    |2       |1.0       |[0.16376974520999113,0.833469628425627,0.002381987656706047,3.1372466445392735E-4,6.491404322194127E-5]    |\n",
      "|332    |2       |1.0       |[0.38571183423968153,0.6072542644367556,0.004215890875294619,0.0026689513522536626,1.4905909601472243E-4]  |\n",
      "|333    |2       |1.0       |[0.1370474023204983,0.8609808239966659,0.0015509429505585102,3.7065829500410773E-4,5.0172437273329816E-5]  |\n",
      "|334    |2       |1.0       |[0.13736519396950914,0.8607006525259224,0.0015267684655877788,3.5752119039980526E-4,4.986384858078657E-5]  |\n",
      "|335    |2       |1.0       |[0.12110686914808276,0.8764917743110874,0.001985568986616533,3.688286968418308E-4,4.695885737153456E-5]    |\n",
      "|336    |2       |1.0       |[0.10968020618890373,0.8879741401212934,0.002215550649564491,8.687857545638391E-5,4.322446478194572E-5]    |\n",
      "|337    |2       |1.0       |[0.13642971162640707,0.8618495855004331,0.0014094160901945503,2.606111812303466E-4,5.067560173483739E-5]   |\n",
      "|338    |2       |1.0       |[0.3121894365776521,0.6845132065471838,0.0022706941143358565,9.159418145135082E-4,1.1072094631465737E-4]   |\n",
      "|339    |2       |1.0       |[0.3167689572223094,0.6802212585361688,0.0025484952612603974,3.533180259638181E-4,1.0797095429760808E-4]   |\n",
      "|340    |2       |1.0       |[0.13749813566733707,0.860195620885565,0.00220337110796426,5.774091285409895E-5,4.5131426279717524E-5]     |\n",
      "|341    |2       |1.0       |[0.14034635030110004,0.8572369098146615,0.0023289591121554306,4.131834127254561E-5,4.646243081044923E-5]   |\n",
      "|342    |3       |1.0       |[0.13983055229990413,0.8577528446151231,0.0023122235063855953,5.871074842844E-5,4.566883015870959E-5]      |\n",
      "|343    |3       |1.0       |[0.14556853210135068,0.8519813188165515,0.0023673423775205002,3.585769847727164E-5,4.694900610003178E-5]   |\n",
      "|344    |2       |1.0       |[0.1398805966565551,0.8577018941273801,0.002313109069793664,5.872036652900969E-5,4.567977974199166E-5]     |\n",
      "|345    |2       |1.0       |[0.14169588965436636,0.8558205601053569,0.0023867630888371066,5.1346447690888484E-5,4.5440703748686874E-5] |\n",
      "|346    |2       |1.0       |[0.12195825791717238,0.8750077195151641,0.0029442238405992865,4.837254327941456E-5,4.142618378462359E-5]   |\n",
      "|347    |2       |1.0       |[0.13780968161583568,0.8603736591904152,0.0017130830346890732,5.6097054565681334E-5,4.747910449428035E-5]  |\n",
      "|348    |2       |1.0       |[0.1382990557735015,0.8599034091338424,0.0016919477526898567,5.837617451359581E-5,4.7211165452540875E-5]   |\n",
      "|349    |3       |0.0       |[0.6360989713910242,0.34490494606655975,0.017274884101959986,0.001476155713713321,2.450427267427471E-4]    |\n",
      "|350    |3       |1.0       |[0.45052697357260946,0.5394129491315882,0.009461727931067015,4.27663880540627E-4,1.706854841946965E-4]     |\n",
      "|351    |2       |1.0       |[0.16888239093084878,0.8282403000359364,0.002777169364838176,4.111173853418285E-5,5.902792984227813E-5]    |\n",
      "|352    |2       |1.0       |[0.14002554148052487,0.8582665954756444,0.0016101253837666808,4.8683609679424E-5,4.905405038461298E-5]     |\n",
      "|353    |2       |1.0       |[0.13526994683851215,0.8635238882467156,0.001122942351749524,3.908249907014376E-5,4.4140063952596874E-5]   |\n",
      "|354    |2       |1.0       |[0.1539174292449185,0.8448829968079659,0.0011089046851571588,4.013443095631249E-5,5.053483100207554E-5]    |\n",
      "|355    |2       |1.0       |[0.15670234786324505,0.8420497387516283,0.0011808895469038352,2.0368593971822833E-5,4.6655244251165456E-5] |\n",
      "|356    |2       |1.0       |[0.14885028420607097,0.8499245902937729,0.001163433448928347,1.8827792983603374E-5,4.286425824413029E-5]   |\n",
      "|357    |3       |1.0       |[0.3184156740750642,0.6761408927582757,0.005346836103811182,3.6115892617154546E-6,9.29854735870947E-5]     |\n",
      "|358    |2       |1.0       |[0.16556999919724796,0.8324943191074224,0.0018809358101155224,5.396200074468878E-6,4.934968513991365E-5]   |\n",
      "|359    |2       |1.0       |[0.13073104438028432,0.8678793475444357,0.0013251271617516632,2.6375735813132972E-5,3.810517771531561E-5]  |\n",
      "|360    |2       |1.0       |[0.18356047590470131,0.8134471182591078,0.0029011089199820154,3.315031792407881E-5,5.8146598285029566E-5]  |\n",
      "|361    |2       |1.0       |[0.22155997505536557,0.7759220793438381,0.0018631327749576404,5.825682040822972E-4,7.224462175643743E-5]   |\n",
      "|362    |2       |1.0       |[0.09173852549719026,0.9071129568328641,0.0010331904037092963,8.222044814916949E-5,3.310681808697098E-5]   |\n",
      "|363    |2       |1.0       |[0.0868387388887603,0.912080771816342,9.922417225667736E-4,5.6637396483072576E-5,3.161017584775717E-5]     |\n",
      "|364    |2       |1.0       |[0.0833313869993592,0.9156829207792399,9.187901295644302E-4,3.7089331142472394E-5,2.9812760694150203E-5]   |\n",
      "|365    |2       |1.0       |[0.0765659998928884,0.9223950658848467,9.808300507342463E-4,3.2590735494293525E-5,2.5513436036235623E-5]   |\n",
      "|366    |2       |1.0       |[0.08645262196218242,0.9120797543452216,9.286848900481554E-4,5.090917221124158E-4,2.984708043536291E-5]    |\n",
      "|367    |3       |1.0       |[0.08646510194416805,0.9120670924643088,9.288277893275591E-4,5.091277500556137E-4,2.9850052139866132E-5]   |\n",
      "|368    |2       |1.0       |[0.08032560782399792,0.9181465911776997,9.319437431961489E-4,5.679580284926615E-4,2.7899226613562725E-5]   |\n",
      "|369    |2       |1.0       |[0.077899557947457,0.9207555784608229,8.694438002275336E-4,4.4882704593623654E-4,2.6592745556354733E-5]    |\n",
      "|370    |3       |1.0       |[0.173751382671298,0.8220295869502053,0.002723314330977799,0.0014376885018203707,5.802754569842964E-5]     |\n",
      "|371    |2       |1.0       |[0.0770913471165997,0.9217016357136516,9.254388117154868E-4,2.565052351340281E-4,2.507312289934032E-5]     |\n",
      "|372    |2       |1.0       |[0.0770913471165997,0.9217016357136516,9.254388117154868E-4,2.565052351340281E-4,2.507312289934032E-5]     |\n",
      "|373    |2       |1.0       |[0.07712051480552175,0.9216720403781901,9.258118011389966E-4,2.5655331922817896E-4,2.5079695920989114E-5]  |\n",
      "|374    |2       |1.0       |[0.07616324795135371,0.9226605049805884,9.203302880116844E-4,2.3135899821357194E-4,2.4557781832562506E-5]  |\n",
      "|375    |2       |1.0       |[0.07703151191254548,0.9218105598937085,9.263820484404449E-4,2.0725578574937861E-4,2.4290359556344272E-5]  |\n",
      "|376    |2       |1.0       |[0.07620719354666784,0.9226822348734832,9.188040003588874E-4,1.6797705288525478E-4,2.3790526604944385E-5]  |\n",
      "|377    |2       |1.0       |[0.07695114612765082,0.9220046702232876,9.092484400283706E-4,1.0973347149845469E-4,2.520173753477672E-5]   |\n",
      "|378    |2       |1.0       |[0.08052332503734727,0.9183705866917354,9.680409757762427E-4,1.127218435547628E-4,2.5325451586131315E-5]   |\n",
      "|379    |2       |1.0       |[0.23937121608695236,0.7538180215107745,0.0018597362135965977,0.004868938149603679,8.208803907289657E-5]   |\n",
      "|380    |2       |1.0       |[0.09089533544905705,0.9075257011784568,0.001002741912326277,5.458033022401447E-4,3.041815791988979E-5]    |\n",
      "|381    |2       |1.0       |[0.07815512401207728,0.9207571916844371,8.240853993223139E-4,2.3811644746759493E-4,2.5482456695663977E-5]  |\n",
      "|382    |2       |1.0       |[0.08088032159449675,0.9180451699985906,8.775436356720227E-4,1.7087885557334022E-4,2.6085915667357454E-5]  |\n",
      "|383    |2       |1.0       |[0.21399906281441552,0.7832601183721383,0.0017947590711691085,8.754674072068103E-4,7.059233507028755E-5]   |\n",
      "|384    |3       |1.0       |[0.208328414312541,0.7859632101510942,0.005361244833390564,2.8550978393484253E-4,6.162091903941069E-5]     |\n",
      "|385    |3       |1.0       |[0.12497391322985543,0.8726253894563001,0.0019418893463251487,4.0958886562394655E-4,4.921910189536172E-5]  |\n",
      "|386    |2       |1.0       |[0.0906782351872815,0.9078848970837242,9.974019109899347E-4,4.079116396459079E-4,3.155417835835316E-5]     |\n",
      "|387    |2       |1.0       |[0.08960800949054881,0.9090009394828187,9.921366046596073E-4,3.6800183784950225E-4,3.0912584123563244E-5]  |\n",
      "|388    |2       |1.0       |[0.08958501815982184,0.9090242537982286,9.918652099829917E-4,3.6795571013004797E-4,3.09071218365942E-5]    |\n",
      "|389    |2       |1.0       |[0.08669597413166685,0.9119089079712132,9.386094846720943E-4,4.265941588265295E-4,2.9914253621376022E-5]   |\n",
      "|390    |3       |1.0       |[0.1777830954048224,0.817371352313159,0.002706205556705522,0.0020794301184509163,5.991660686219699E-5]     |\n",
      "|391    |2       |1.0       |[0.0783074176802647,0.9203605173934211,9.093065942197965E-4,3.961846696492002E-4,2.6573662445224307E-5]    |\n",
      "|392    |3       |1.0       |[0.07702774256791117,0.92173566098628,9.098767919894947E-4,3.0103282934716995E-4,2.568682447220107E-5]     |\n",
      "|393    |2       |1.0       |[0.081215659601833,0.9177287337908969,9.331252449878487E-4,9.604561110564027E-5,2.6435751176496026E-5]     |\n",
      "|394    |2       |1.0       |[0.081215659601833,0.9177287337908969,9.331252449878487E-4,9.604561110564027E-5,2.6435751176496026E-5]     |\n",
      "|395    |2       |1.0       |[0.081215659601833,0.9177287337908969,9.331252449878487E-4,9.604561110564027E-5,2.6435751176496026E-5]     |\n",
      "|396    |3       |1.0       |[0.09355914759663383,0.9046948327916047,0.00165576273875031,6.126361234977814E-5,2.899326066155947E-5]     |\n",
      "|397    |2       |1.0       |[0.1254722744335705,0.8720780588243463,0.001886987505199278,5.139519798163291E-4,4.872725706762257E-5]     |\n",
      "|398    |2       |1.0       |[0.08744449882868308,0.9109740132304909,9.874916895585163E-4,5.627715828318047E-4,3.12246684356111E-5]     |\n",
      "|399    |2       |1.0       |[0.08080490730510044,0.917644088571373,0.0012045753940051437,3.147537221088898E-4,3.1675007412642376E-5]   |\n",
      "|400    |3       |1.0       |[0.08668235101834533,0.9120316887320054,9.383029458738239E-4,3.1701753102236134E-4,3.0639772753110615E-5]  |\n",
      "|401    |3       |1.0       |[0.0840820120912618,0.914654065385912,9.071163350287761E-4,3.2747007901792004E-4,2.933610877947381E-5]     |\n",
      "|402    |3       |1.0       |[0.08408467763100566,0.9146513634524428,9.071469825971016E-4,3.2747518294159593E-4,2.9336751012876127E-5]  |\n",
      "|403    |2       |1.0       |[0.0789413210227033,0.9198986357723824,8.546030962864044E-4,2.781118818206852E-4,2.732822680711485E-5]     |\n",
      "|404    |2       |1.0       |[0.07862961488981507,0.9201218773710221,8.946645619510819E-4,3.268002564797268E-4,2.7042920732204305E-5]   |\n",
      "|405    |2       |1.0       |[0.07768508822275189,0.9211039034458915,8.89749871557948E-4,2.9477065325186636E-4,2.6487806546933877E-5]   |\n",
      "|406    |3       |1.0       |[0.16156489989781797,0.8347035044600168,0.0025184101638459494,0.0011593471234144336,5.383835490499507E-5]  |\n",
      "|407    |3       |1.0       |[0.07819987020622873,0.9207199286901744,8.764770601462377E-4,1.773735948622044E-4,2.635044858855595E-5]    |\n",
      "|408    |3       |1.0       |[0.07819987020622873,0.9207199286901744,8.764770601462377E-4,1.773735948622044E-4,2.635044858855595E-5]    |\n",
      "|409    |2       |1.0       |[0.07819987020622873,0.9207199286901744,8.764770601462377E-4,1.773735948622044E-4,2.635044858855595E-5]    |\n",
      "|410    |3       |1.0       |[0.07909346779513274,0.9198359562059277,8.976528368323156E-4,1.4657442783010723E-4,2.6348734277498467E-5]  |\n",
      "|411    |3       |1.0       |[0.0705056906170538,0.9281789795929527,0.001190120513833725,1.003827008399069E-4,2.4826575320002226E-5]    |\n",
      "|412    |2       |1.0       |[0.12409412352551594,0.8736487615286826,0.001886233690391419,3.220285800389386E-4,4.885267537096336E-5]    |\n",
      "|413    |3       |1.0       |[0.12437984665485988,0.8733757434125679,0.0019095850451252988,2.874857236221181E-4,4.733916382485016E-5]   |\n",
      "|414    |2       |1.0       |[0.1021419720826187,0.8963853340727596,0.001096548587278535,3.371808366626325E-4,3.896442068060043E-5]     |\n",
      "|415    |3       |1.0       |[0.10747312710780944,0.8908349378415024,0.0011909760566762707,4.6096471102322333E-4,3.9994282988751095E-5] |\n",
      "|416    |3       |1.0       |[0.10363071747487622,0.8949009210076999,0.0010480187794734445,3.799841836988129E-4,4.035855425139668E-5]   |\n",
      "|417    |2       |1.0       |[0.10125399153006037,0.8970091597134576,0.0012453303145078865,4.550039360375886E-4,3.651450593652755E-5]   |\n",
      "|418    |2       |1.0       |[0.0754002710790131,0.9232632122105972,8.519828902289308E-4,4.587503057585781E-4,2.5783514402203474E-5]    |\n",
      "|419    |2       |1.0       |[0.08073594628117539,0.9169934399096892,0.002172271481311058,7.036508180221409E-5,2.7977246021959166E-5]   |\n",
      "|420    |2       |1.0       |[0.09754399125457908,0.9004308939534573,0.0011373454329545144,8.516575162715208E-4,3.6111842737708875E-5]  |\n",
      "|421    |2       |1.0       |[0.09062623346948412,0.9075555117052364,0.001105466117473032,6.820207767729785E-4,3.0767931033384345E-5]   |\n",
      "|422    |2       |1.0       |[0.09062056884656575,0.9075612721197015,0.0011053924430236405,6.819999832235638E-4,3.076660748547583E-5]   |\n",
      "|423    |2       |1.0       |[0.08955964822986746,0.9086952294782701,0.001099667984076721,6.153110276829585E-4,3.0143280102886844E-5]   |\n",
      "|424    |3       |1.0       |[0.16592493336559966,0.8276293478560499,0.0025416704796259347,0.003844392320230088,5.965597849425063E-5]   |\n",
      "|425    |3       |1.0       |[0.0787261217713525,0.91980472187017,8.850882918545009E-4,5.570461596403119E-4,2.7021906982658866E-5]      |\n",
      "|426    |2       |1.0       |[0.0787261217713525,0.91980472187017,8.850882918545009E-4,5.570461596403119E-4,2.7021906982658866E-5]      |\n",
      "|427    |2       |1.0       |[0.0787261217713525,0.91980472187017,8.850882918545009E-4,5.570461596403119E-4,2.7021906982658866E-5]      |\n",
      "|428    |2       |1.0       |[0.07403064736688943,0.9248053276491759,8.72025693476505E-4,2.671063317898306E-4,2.4892958668365864E-5]    |\n",
      "|429    |2       |1.0       |[0.1995174245994076,0.7965672123546947,0.0018293285080793697,0.002021698303338891,6.433623447932038E-5]    |\n",
      "|430    |2       |1.0       |[0.07677202643967965,0.9220830742767606,9.439379029495345E-4,1.7639667963258176E-4,2.4564700977629384E-5]  |\n",
      "|431    |2       |1.0       |[0.07515095146463385,0.923758096202156,9.168788825775159E-4,1.4901533541026506E-4,2.5058115222252786E-5]   |\n",
      "|432    |2       |1.0       |[0.1187578486926216,0.8788410139102855,0.001847177280877211,5.086038356019221E-4,4.535628061385974E-5]     |\n",
      "|433    |2       |1.0       |[0.1187578486926216,0.8788410139102855,0.001847177280877211,5.086038356019221E-4,4.535628061385974E-5]     |\n",
      "|434    |3       |1.0       |[0.08667439283790528,0.9118107288913088,9.586377367464924E-4,5.24654918477111E-4,3.1585615562344705E-5]    |\n",
      "|435    |3       |1.0       |[0.191220416814692,0.803233380093898,0.0028822643798919775,0.0025973792877543806,6.655942376364892E-5]     |\n",
      "|436    |2       |1.0       |[0.07714968339127809,0.9215148487799335,8.765926427481144E-4,4.323665481732384E-4,2.6508637867084918E-5]   |\n",
      "|437    |3       |1.0       |[0.07475455803190634,0.9240739480460671,9.079470032223559E-4,2.3924172575558742E-4,2.4305193048651116E-5]  |\n",
      "|438    |3       |1.0       |[0.07871082347971721,0.9200576138903352,9.989908228917159E-4,2.0747788740800327E-4,2.509391964785729E-5]   |\n",
      "|439    |2       |1.0       |[0.08456626046823082,0.9138438520209247,9.604268623838244E-4,6.006533388018371E-4,2.8807309658870357E-5]   |\n",
      "|440    |2       |1.0       |[0.07365389368441289,0.9250773952673196,8.466310001720768E-4,3.97252803948602E-4,2.4827244146767342E-5]    |\n",
      "|441    |2       |1.0       |[0.09424952528302531,0.9042627654525924,0.0010151372649281017,4.3897633835527847E-4,3.359566109899963E-5]  |\n",
      "|442    |2       |1.0       |[0.0768752028142104,0.9218575040568203,8.329105309399493E-4,4.068624786383754E-4,2.7520119390964506E-5]    |\n",
      "|443    |3       |1.0       |[0.07883699610229983,0.9199877374043739,8.4742554252534E-4,3.01114470866242E-4,2.6726479934618636E-5]      |\n",
      "|444    |2       |1.0       |[0.21040410654727165,0.7859333872179037,0.0017356292937064214,0.0018565240622375103,7.035287888068998E-5]  |\n",
      "|445    |2       |1.0       |[0.21040410654727165,0.7859333872179037,0.0017356292937064214,0.0018565240622375103,7.035287888068998E-5]  |\n",
      "|446    |2       |1.0       |[0.07801784265849267,0.9208551017081049,9.637822398928707E-4,1.3779570686763131E-4,2.5477686642009978E-5]  |\n",
      "|447    |3       |1.0       |[0.19615625403051568,0.7986107248850993,0.0047780248801694925,3.925800018648127E-4,6.241620235055017E-5]   |\n",
      "|448    |2       |1.0       |[0.0955827442185775,0.9025297371864933,0.001787506851344938,6.975319428278316E-5,3.025854930152204E-5]     |\n",
      "|449    |2       |1.0       |[0.09049772386256727,0.9081345880356769,9.699352262382031E-4,3.678715512607321E-4,2.988132425701589E-5]    |\n",
      "|450    |3       |1.0       |[0.08742933758032181,0.911486427122871,9.21719089281167E-4,1.3556566989079283E-4,2.695053763503846E-5]     |\n",
      "|451    |3       |1.0       |[0.10598625843071745,0.8920881367965692,0.0018446610194276516,4.805744890243984E-5,3.288630438342796E-5]   |\n",
      "|452    |3       |1.0       |[0.1327044638591769,0.8646660364366625,0.0022474468273053043,3.3527069529297107E-4,4.6782181562162406E-5]  |\n",
      "|453    |3       |1.0       |[0.2130482102174187,0.7814897516318936,0.00353792403377137,0.0018489859613509787,7.512815556517339E-5]     |\n",
      "|454    |2       |1.0       |[0.09587884655205534,0.9027147042832077,0.0010525751549955234,3.2174607734387705E-4,3.212793239759042E-5]  |\n",
      "|455    |2       |1.0       |[0.2338661403467896,0.7619927573823098,0.0018065485959603788,0.0022586328266497448,7.592084829066984E-5]   |\n",
      "|456    |3       |1.0       |[0.08956066888700538,0.9091979122447525,0.0010010495271598524,2.1317776084576415E-4,2.719158023636375E-5]  |\n",
      "|457    |2       |1.0       |[0.08852962000817466,0.9102553124181102,9.960788953974526E-4,1.9234487881244236E-4,2.6643799505205572E-5]  |\n",
      "|458    |3       |1.0       |[0.0890842453148496,0.9097619175307862,0.001002911104092134,1.228776678459729E-4,2.8048382426213902E-5]    |\n",
      "|459    |3       |1.0       |[0.08708443761912694,0.9117853947882921,0.0010256278537026826,7.807468546388028E-5,2.6465053414215537E-5]  |\n",
      "|460    |3       |1.0       |[0.10477541612724656,0.8933713706380704,0.0017683949384934739,5.1996891883661354E-5,3.28214043058229E-5]   |\n",
      "|461    |2       |1.0       |[0.09065137313650153,0.9069637828799834,0.002305006610687263,4.93535474935108E-5,3.0483825334397718E-5]    |\n",
      "|462    |3       |1.0       |[0.13341727122428124,0.8641039139348091,0.0019932331982710685,4.342524343319116E-4,5.1329208306560595E-5]  |\n",
      "|463    |3       |1.0       |[0.11749889033789444,0.8794158880044407,0.0025893296429851946,4.4759910368321975E-4,4.829291099631679E-5]  |\n",
      "|464    |3       |1.0       |[0.2714491846284339,0.7203396660348274,0.006012708426161765,0.0020987558154642585,9.968509511262292E-5]    |\n",
      "|465    |3       |1.0       |[0.2681576322275303,0.7240776955626282,0.005997478319594929,0.0016677331374578422,9.946075278883985E-5]    |\n",
      "|466    |2       |1.0       |[0.12844166104852803,0.8690909667721338,0.002078793862967974,3.4088641702100924E-4,4.7691899349214324E-5]  |\n",
      "|467    |2       |1.0       |[0.12844166104852803,0.8690909667721338,0.002078793862967974,3.4088641702100924E-4,4.7691899349214324E-5]  |\n",
      "|468    |2       |1.0       |[0.3143350682514477,0.6794025802172317,0.0036351872182884696,0.0025109459732376846,1.1621833979443081E-4]  |\n",
      "|469    |3       |1.0       |[0.2286784027126037,0.7658816905159532,0.003603072336193141,0.0017496891274370369,8.714530781276508E-5]    |\n",
      "|470    |2       |1.0       |[0.10365353657934563,0.8948844093109454,0.0011349642220535247,2.917980503210764E-4,3.529183733450862E-5]   |\n",
      "|471    |2       |1.0       |[0.10078490519161784,0.8977300243614754,0.001114615672340684,3.3665586384785E-4,3.379891071821828E-5]      |\n",
      "|472    |3       |1.0       |[0.19885937615049631,0.7967301023058712,0.003001096863058096,0.001345181988719829,6.424269185456347E-5]    |\n",
      "|473    |3       |1.0       |[0.08603562232463946,0.9128766208690798,8.878897848261908E-4,1.7243515055491272E-4,2.7431870899674722E-5]  |\n",
      "|474    |2       |1.0       |[0.08530469817350818,0.9135747458362481,0.0010029995202905022,9.055402811419004E-5,2.700244183900202E-5]   |\n",
      "|475    |3       |1.0       |[0.08901430328044259,0.9098650753365682,0.001007460306862834,8.501494640451836E-5,2.8146129721922126E-5]   |\n",
      "|476    |2       |1.0       |[0.08894785067962692,0.9099323752462659,0.001006658511799363,8.498390991056677E-5,2.8131652397215595E-5]   |\n",
      "|477    |2       |1.0       |[0.10784535008495579,0.8901094504259679,0.0019572242569752887,5.6439806885109116E-5,3.153542521581504E-5]  |\n",
      "|478    |3       |1.0       |[0.10784535008495579,0.8901094504259679,0.0019572242569752887,5.6439806885109116E-5,3.153542521581504E-5]  |\n",
      "|479    |3       |1.0       |[0.23222072055653564,0.7626749900767329,0.003283588039630118,0.0017338626160459838,8.683871105536396E-5]   |\n",
      "|480    |2       |1.0       |[0.08730605691191008,0.9114609980101132,9.691090930621792E-4,2.362228171275411E-4,2.761316778677452E-5]    |\n",
      "|481    |2       |1.0       |[0.18013311286163725,0.8156143045097844,0.002808189039505483,0.0013853987143202882,5.899487475250424E-5]   |\n",
      "|482    |3       |1.0       |[0.19712882854107946,0.7989827107273253,0.0028865130361614365,9.388935755889207E-4,6.305411984466528E-5]   |\n",
      "|483    |2       |1.0       |[0.07734616745883019,0.9212851811813005,0.0012325964339582824,1.0976492848675492E-4,2.628999742411831E-5]  |\n",
      "|484    |2       |1.0       |[0.08834138141314946,0.910568974651623,9.544183355305112E-4,1.0711831949229114E-4,2.8107280204829124E-5]   |\n",
      "|485    |3       |1.0       |[0.08785812651295888,0.9109616239674807,0.0010485648618017219,1.0411296144608211E-4,2.7571696312582223E-5] |\n",
      "|486    |3       |1.0       |[0.21383259485901696,0.7808504295017621,0.004942237499405308,3.074882367052575E-4,6.724990311037065E-5]    |\n",
      "|487    |3       |1.0       |[0.10475106260194461,0.8933095061842404,0.0018554643755308937,5.223488998907743E-5,3.173194829495588E-5]   |\n",
      "|488    |2       |1.0       |[0.10324516784056477,0.8949382859656589,0.001740359396173284,4.392434712853814E-5,3.226245047431666E-5]    |\n",
      "|489    |3       |1.0       |[0.2527228591126748,0.7403242550783774,0.005735225666658063,0.0011232358511296854,9.442429116010274E-5]    |\n",
      "|490    |2       |1.0       |[0.22099627903657726,0.7753653415901706,0.0016902431587612705,0.0018781286984908168,7.000751600005044E-5]  |\n",
      "|491    |2       |1.0       |[0.09053680954203039,0.9083675237280111,9.521005246696249E-4,1.158519355015373E-4,2.7714269787331438E-5]   |\n",
      "|492    |3       |1.0       |[0.08930624434551303,0.9096454414579787,9.359617582533028E-4,8.515220777935316E-5,2.7200230475663477E-5]   |\n",
      "|493    |2       |1.0       |[0.26999163440360563,0.7257738866622218,0.0034342413012760317,7.150191801512711E-4,8.52184527453736E-5]    |\n",
      "|494    |2       |1.0       |[0.1350478122031635,0.8623673951187532,0.0020485432262147946,4.860595958574509E-4,5.018985601092839E-5]    |\n",
      "|495    |3       |1.0       |[0.13115457305645464,0.8665421203899139,0.001966137265887483,2.8811142033834716E-4,4.9057867405631366E-5]  |\n",
      "|496    |2       |1.0       |[0.09029911423703194,0.9085779072976601,9.518006790790343E-4,1.4256447385358026E-4,2.861331237541896E-5]   |\n",
      "|497    |2       |1.0       |[0.1287654446779225,0.868849184632579,0.001959011388027691,3.7865364265935754E-4,4.770565881157697E-5]     |\n",
      "|498    |3       |1.0       |[0.10638375439312756,0.8919232874131224,0.001206549981216914,4.4902190686423954E-4,3.738630566882274E-5]   |\n",
      "|499    |2       |1.0       |[0.2601153368523911,0.7361705619013784,0.0030974631284175535,5.363902419369373E-4,8.024787587615363E-5]    |\n",
      "|500    |3       |1.0       |[0.18914759459164113,0.8059116000976374,0.003021016736690642,0.0018592993415388302,6.04892324919763E-5]    |\n",
      "|501    |2       |1.0       |[0.21540021510893553,0.780188672932454,0.0017067186012226956,0.0026316136104884663,7.277974689952988E-5]   |\n",
      "|502    |3       |1.0       |[0.08218189411112621,0.9166325738053284,8.907566874178535E-4,2.676482623242645E-4,2.7127133803409724E-5]   |\n",
      "|503    |3       |1.0       |[0.08625572518147062,0.9126153572362501,9.755056675915081E-4,1.2610645528021998E-4,2.73054594075268E-5]    |\n",
      "|504    |2       |1.0       |[0.10366871026147098,0.894776606951785,0.0011461628396462113,3.6980467472199404E-4,3.871527237563228E-5]   |\n",
      "|505    |2       |1.0       |[0.10366667972802847,0.8947786654412497,0.0011461388814938823,3.69801195311608E-4,3.871475391614715E-5]    |\n",
      "|506    |3       |1.0       |[0.22211727279074672,0.7721846709841951,0.003711381018330718,0.001907763298988549,7.891190773894897E-5]    |\n",
      "|507    |2       |1.0       |[0.0998811174219757,0.8986063548934892,0.0010978411326913991,3.811471302539732E-4,3.3539421589593306E-5]   |\n",
      "|508    |2       |1.0       |[0.08584045323696163,0.912844718896219,9.887508219331884E-4,2.9882990824977115E-4,2.724713663655414E-5]    |\n",
      "|509    |2       |1.0       |[0.08523760866733006,0.9135697537641372,9.233661753139383E-4,2.4122782520549457E-4,2.8043568013457615E-5]  |\n",
      "|510    |2       |1.0       |[0.21654872287963328,0.7799519354401264,0.00172789809460115,0.0016996661096473449,7.177747599172622E-5]    |\n",
      "|511    |2       |1.0       |[0.08274374520351502,0.9160904147326074,9.607173330056895E-4,1.7843481288741856E-4,2.6687917984479085E-5]  |\n",
      "|512    |2       |1.0       |[0.08509653746342663,0.9137674719981854,9.802968805093478E-4,1.2866428008632112E-4,2.702937779237313E-5]   |\n",
      "|513    |2       |1.0       |[0.10890379667818725,0.8891278340525165,0.0018694314006367217,6.566494885923745E-5,3.3272919800515595E-5]  |\n",
      "|514    |2       |1.0       |[0.09744732712916186,0.9011364644367761,0.0010610406899529512,3.2306134899776876E-4,3.21063951112891E-5]   |\n",
      "|515    |2       |1.0       |[0.09572415557104723,0.9028771863009702,0.0010284487334841492,3.39206308796283E-4,3.100308570193681E-5]    |\n",
      "|516    |2       |1.0       |[0.08702549939208666,0.9117356900388446,9.2486770841851E-4,2.8459219942699886E-4,2.93506612231182E-5]      |\n",
      "|517    |2       |1.0       |[0.21976827804704158,0.7760257392597975,0.001738052990256935,0.0023931153292150766,7.481437368880572E-5]   |\n",
      "|518    |2       |1.0       |[0.12760533590656986,0.8699227638047581,0.0020260890597065487,3.987374605595414E-4,4.707376840593125E-5]   |\n",
      "|519    |2       |1.0       |[0.09830527213020535,0.9002773163029313,0.001031157561605074,3.518212971322715E-4,3.4432708125967374E-5]   |\n",
      "|520    |2       |1.0       |[0.09066419297140627,0.9078938313223075,9.78229356101907E-4,4.3422673265701016E-4,2.9519617527094516E-5]   |\n",
      "|521    |3       |1.0       |[0.17657590296664605,0.8188121441816363,0.002621018908988666,0.001932632643021281,5.830129970765747E-5]    |\n",
      "|522    |2       |1.0       |[0.08474849407028025,0.914125038497975,8.532874337570991E-4,2.456992431225228E-4,2.7480754865299105E-5]    |\n",
      "|523    |3       |1.0       |[0.08217982349980985,0.9167540988168762,8.637089383969662E-4,1.759404547474445E-4,2.642829016955539E-5]    |\n",
      "|524    |3       |1.0       |[0.17852448372047147,0.8179709725903518,0.0027224654205236587,7.26021344864751E-4,5.6056923788499486E-5]   |\n",
      "|525    |3       |1.0       |[0.0889705061785814,0.9099474007214164,9.40622340033216E-4,1.1403732074562625E-4,2.7433439223306654E-5]    |\n",
      "|526    |3       |1.0       |[0.09286645474021749,0.9057477718085049,9.319565089289188E-4,4.218430360369751E-4,3.197390631180329E-5]    |\n",
      "|527    |3       |1.0       |[0.0928669917990329,0.9057472276878819,9.319622563261398E-4,4.2184422331366985E-4,3.197403344556763E-5]    |\n",
      "|528    |3       |1.0       |[0.0771180317097149,0.9214623055056707,0.0010918404490476673,3.009776163172299E-4,2.6844719249541794E-5]   |\n",
      "|529    |3       |1.0       |[0.08666150858037437,0.9122709648773991,8.460776856758633E-4,1.9394875900581015E-4,2.7500097544934784E-5]  |\n",
      "|530    |3       |1.0       |[0.1914492641249463,0.8049028310892058,0.0025827910566015196,0.0010044141806504277,6.069954859580501E-5]   |\n",
      "|531    |3       |1.0       |[0.08556417045774396,0.9133931790460559,8.408348221382805E-4,1.7489154176193863E-4,2.6924132299707876E-5]  |\n",
      "|532    |2       |1.0       |[0.08556417045774396,0.9133931790460559,8.408348221382805E-4,1.7489154176193863E-4,2.6924132299707876E-5]  |\n",
      "|533    |2       |1.0       |[0.08556417045774396,0.9133931790460559,8.408348221382805E-4,1.7489154176193863E-4,2.6924132299707876E-5]  |\n",
      "|534    |2       |1.0       |[0.18584087138158387,0.8105669017789461,0.0027507442736709685,7.843960872041597E-4,5.7086478594775324E-5]  |\n",
      "|535    |2       |1.0       |[0.08882185804486147,0.910097975219993,9.400522448609599E-4,1.139905698608862E-4,2.6123920423699325E-5]    |\n",
      "|536    |2       |1.0       |[0.257137425053307,0.7390586236880576,0.003245921578826618,4.7884782154157643E-4,7.918185826707017E-5]     |\n",
      "|537    |2       |1.0       |[0.13361845522363058,0.8637537214526831,0.002135558515626103,4.4342285581849746E-4,4.8841952241682196E-5]  |\n",
      "|538    |2       |1.0       |[0.09795442065655528,0.900671634034243,0.0010244306509343516,3.166337639110393E-4,3.2880894356430396E-5]   |\n",
      "|539    |3       |1.0       |[0.19734702328875617,0.7981451901436515,0.0026426021739048844,0.001799047875482279,6.613651820533274E-5]   |\n",
      "|540    |2       |1.0       |[0.08526932424447327,0.9136013012529735,8.301286995237821E-4,2.7112570737079675E-4,2.8120095658572143E-5]  |\n",
      "|541    |2       |1.0       |[0.08652970852805957,0.9123664434853056,8.352567119730106E-4,2.4076566042269525E-4,2.7825614238996118E-5]  |\n",
      "|542    |2       |1.0       |[0.08652970852805957,0.9123664434853056,8.352567119730106E-4,2.4076566042269525E-4,2.7825614238996118E-5]  |\n",
      "|543    |2       |1.0       |[0.08659453348277638,0.9123008487245461,8.359237142906107E-4,2.4085408679047892E-4,2.783999159650119E-5]   |\n",
      "|544    |2       |1.0       |[0.09083254933804921,0.9080325756092612,9.706332015864244E-4,1.3711162852472888E-4,2.7130222578439115E-5]  |\n",
      "|545    |3       |1.0       |[0.10072310588987615,0.8978931459523343,0.0010183350432352248,3.304633243776185E-4,3.494979017679049E-5]   |\n",
      "|546    |2       |1.0       |[0.23063736233166957,0.7660951156793612,0.0016464641738626728,0.0015465689989849955,7.448881612163208E-5]  |\n",
      "|547    |3       |1.0       |[0.12708848395391203,0.8704303031657934,0.0020421359143610596,3.904069490768844E-4,4.867001685663947E-5]   |\n",
      "|548    |2       |1.0       |[0.25090937726177964,0.7430833479234902,0.0021248886917514457,0.0038001304701561402,8.225565282265056E-5]  |\n",
      "|549    |2       |1.0       |[0.09555311870255816,0.9030112102528266,9.824171752070411E-4,4.212343889031216E-4,3.201948050498751E-5]    |\n",
      "|550    |2       |1.0       |[0.08982780903990653,0.9088952734772818,9.424506848669648E-4,3.072010404172882E-4,2.7265757527275853E-5]   |\n",
      "|551    |2       |1.0       |[0.0891150238100552,0.9097523572317928,9.042760540173538E-4,2.008193746336235E-4,2.752352950098251E-5]     |\n",
      "|552    |2       |1.0       |[0.11003260571006486,0.8880253353505245,0.0018413011016616143,6.848044885259448E-5,3.2277388896193605E-5]  |\n",
      "|553    |2       |1.0       |[0.10386974340926482,0.8941051492462938,0.001938109398942315,5.662108145520683E-5,3.037686404388835E-5]    |\n",
      "|554    |2       |1.0       |[0.10386974340926482,0.8941051492462938,0.001938109398942315,5.662108145520683E-5,3.037686404388835E-5]    |\n",
      "|555    |2       |1.0       |[0.13589983047005236,0.8613906717308261,0.002166616591868834,4.948602768206983E-4,4.80209304320869E-5]     |\n",
      "|556    |3       |1.0       |[0.10546272319653419,0.8929314384792251,0.001158099998350639,4.0982567971425824E-4,3.7912646175947716E-5]  |\n",
      "|557    |2       |1.0       |[0.10023818369045162,0.8982295553889821,0.0010126827180803,4.8707265337023277E-4,3.25055491156915E-5]      |\n",
      "|558    |2       |1.0       |[0.23639422566438545,0.757244124350928,0.0017217888311617236,0.004558648667282464,8.12124862423047E-5]     |\n",
      "|559    |2       |1.0       |[0.09191126700678415,0.9066696485616718,9.15752241468353E-4,4.724845121329159E-4,3.084767794285987E-5]     |\n",
      "|560    |2       |1.0       |[0.09429320816918363,0.9043681538323387,8.985412846812301E-4,4.098679905376069E-4,3.022872325876888E-5]    |\n",
      "|561    |2       |1.0       |[0.08606007415031372,0.9127784889801901,8.621985329378192E-4,2.7230469945702503E-4,2.6933637101283802E-5]  |\n",
      "|562    |2       |1.0       |[0.0851877612474176,0.9136768337921042,9.131186921354378E-4,1.9684549349017704E-4,2.5440774852547736E-5]   |\n",
      "|563    |2       |1.0       |[0.11328031172955134,0.8847165279389283,0.0019060507391086717,6.453228065330329E-5,3.257731175833262E-5]   |\n",
      "|564    |2       |1.0       |[0.09809524020338127,0.9001166657065102,0.001705379373139446,5.3328815828858904E-5,2.938590114024791E-5]   |\n",
      "|565    |2       |1.0       |[0.11610288434925306,0.8818597128823257,0.001948114205440699,5.6771749817251626E-5,3.251681316356848E-5]   |\n",
      "|566    |2       |1.0       |[0.0990918040947314,0.8992664121090266,0.001015636748848184,5.937587850229094E-4,3.238826237100052E-5]     |\n",
      "|567    |2       |1.0       |[0.09208780269914181,0.9064142455433684,9.296163385562356E-4,5.383158317052861E-4,3.001958722830155E-5]    |\n",
      "|568    |2       |1.0       |[0.09208529012035717,0.9064167928862611,9.295892920040221E-4,5.38308677506552E-4,3.001902387110017E-5]     |\n",
      "|569    |2       |1.0       |[0.09184754232571421,0.9066212403807401,9.336405005427395E-4,5.680323210581449E-4,2.954447194465492E-5]    |\n",
      "|570    |3       |1.0       |[0.17454249114305156,0.8203297397855793,0.0025336947118559643,0.0025363736476687825,5.770071184446331E-5]  |\n",
      "|571    |2       |1.0       |[0.09014729837929404,0.9087125585312608,9.851230514343486E-4,1.2887189319781925E-4,2.6148144812985907E-5]  |\n",
      "|572    |2       |1.0       |[0.27005844841099297,0.725891695190633,0.003126200375024706,8.450382963010609E-4,7.861772704832279E-5]     |\n",
      "|573    |2       |1.0       |[0.10970239845563644,0.8883844389801491,0.0018059942021952519,7.651633573584681E-5,3.0652026283269497E-5]  |\n",
      "|574    |3       |1.0       |[0.23380270541712292,0.7607108979531562,0.003108737223979129,0.00230037689009976,7.728251564202738E-5]     |\n",
      "|575    |2       |1.0       |[0.08963206602544638,0.9084941945972894,0.0012560001648409093,5.857303974462441E-4,3.200881497685823E-5]   |\n",
      "|576    |2       |1.0       |[0.10218292802718906,0.8962416482317228,9.707277258022755E-4,5.705385283175278E-4,3.415748696838629E-5]    |\n",
      "|577    |3       |1.0       |[0.2088211095746367,0.7852125836648328,0.0028841342884309324,0.0030143154497361355,6.785702236334823E-5]   |\n",
      "|578    |2       |1.0       |[0.22230652765868342,0.7702059345951601,0.0016803980949347572,0.005733315597270348,7.382405395142578E-5]   |\n",
      "|579    |2       |1.0       |[0.08556547827961483,0.9129337378232278,8.847579562597797E-4,5.882663292926253E-4,2.7759611604791876E-5]   |\n",
      "|580    |2       |1.0       |[0.08556305555109067,0.9129361959694451,8.847312552575735E-4,5.882581549669785E-4,2.775906923966494E-5]    |\n",
      "|581    |2       |1.0       |[0.15739041977417126,0.839826398420352,0.002182475002324705,5.489449124785233E-4,5.176189067344486E-5]     |\n",
      "|582    |2       |1.0       |[0.11497930608081305,0.88327922712152,0.0011968632993976647,5.082977643194428E-4,3.630573394976094E-5]     |\n",
      "|583    |3       |1.0       |[0.212707058345052,0.78127069342101,0.002720400339577481,0.0032293236947371594,7.252419962328009E-5]       |\n",
      "|584    |3       |1.0       |[0.08979554151034225,0.9084533242741575,9.332381957871019E-4,7.876874513465273E-4,3.020856836652052E-5]    |\n",
      "|585    |2       |1.0       |[0.2240406216826477,0.7667015710128644,0.0017244384032931378,0.007455985937941255,7.73829632535756E-5]     |\n",
      "|586    |3       |1.0       |[0.07951457497939804,0.9190977903020321,7.836832673264734E-4,5.766374627272214E-4,2.73139885159901E-5]     |\n",
      "|587    |3       |1.0       |[0.08552614263159956,0.9128462202824419,9.064708856802389E-4,6.925592695108538E-4,2.8606930767510724E-5]   |\n",
      "|588    |2       |1.0       |[0.2098073049609422,0.7834328923517172,0.001546444132355908,0.005141799635976468,7.155891900834463E-5]     |\n",
      "|589    |3       |1.0       |[0.1635559873364558,0.8317730240469751,0.002386306853414423,0.0022300378217237197,5.464394143080912E-5]    |\n",
      "|590    |2       |1.0       |[0.08421603697777788,0.9144670095605926,8.953592702716073E-4,3.947698190185933E-4,2.682437233945408E-5]    |\n",
      "|591    |2       |1.0       |[0.21968904667425582,0.7746687096097872,0.0017074334382200236,0.0038631827737169167,7.162750402018862E-5]  |\n",
      "|592    |2       |1.0       |[0.07935891273833622,0.9195725088261709,8.147180872085289E-4,2.2969101999965525E-4,2.4169328284753733E-5]  |\n",
      "|593    |2       |1.0       |[0.24406732274279988,0.752228453873995,0.0025896533055722237,0.0010401841387017562,7.438593893111289E-5]   |\n",
      "|594    |3       |1.0       |[0.10222448904370017,0.8961737980268697,0.0014712403364666947,1.0080775562165673E-4,2.9664837341802875E-5] |\n",
      "|595    |2       |1.0       |[0.10222448904370017,0.8961737980268697,0.0014712403364666947,1.0080775562165673E-4,2.9664837341802875E-5] |\n",
      "|596    |3       |1.0       |[0.235487147243374,0.7585076000275819,0.0034360170326423905,0.0024886584963333446,8.057720006841816E-5]    |\n",
      "|597    |2       |1.0       |[0.10280583671628175,0.8956186971850338,9.755018374368989E-4,5.652369748965124E-4,3.472728635103013E-5]    |\n",
      "|598    |2       |1.0       |[0.08239122604682862,0.9163880885576956,8.886227143579542E-4,3.036342674507581E-4,2.8428413667064527E-5]   |\n",
      "|599    |3       |1.0       |[0.08277934870415511,0.915978136711832,9.765967820975945E-4,2.3682874715440265E-4,2.908905476101391E-5]    |\n",
      "|600    |3       |1.0       |[0.22276902162901246,0.7677521295132157,0.007984285798793086,0.001400647072892643,9.391598608605565E-5]    |\n",
      "|601    |3       |1.0       |[0.18309553559417072,0.811697450072905,0.002678626668839879,0.002467549864368646,6.083779971607557E-5]     |\n",
      "|602    |3       |1.0       |[0.17524246617437028,0.821104887434538,0.002812381235448334,7.836059544330659E-4,5.6659201210256384E-5]    |\n",
      "|603    |2       |1.0       |[0.08158648679251436,0.9173427409485282,9.504339861941078E-4,9.487734731935483E-5,2.5460925443943826E-5]   |\n",
      "|604    |3       |1.0       |[0.21615689015587686,0.778564953048052,0.004801411321628922,4.1292040355349926E-4,6.382507088896685E-5]    |\n",
      "|605    |2       |1.0       |[0.26506148868448626,0.7310787323910877,0.0033957239677850047,3.8141483952397806E-4,8.264011711709087E-5]  |\n",
      "|606    |2       |1.0       |[0.10478276997828571,0.8933307456773089,0.0011150971297587927,7.359876678296217E-4,3.539954681687944E-5]   |\n",
      "|607    |2       |1.0       |[0.08362817039695421,0.9151315784201126,9.509069771609006E-4,2.6078739681671236E-4,2.8556808955697096E-5]  |\n",
      "|608    |2       |1.0       |[0.21861026054395816,0.7769386937787053,0.001817129019130427,0.0025575016483349758,7.641500987118191E-5]   |\n",
      "|609    |2       |1.0       |[0.21861375068255515,0.7769351551841409,0.0018171602720624778,0.0025575180752236463,7.641578601776476E-5]  |\n",
      "|610    |2       |1.0       |[0.082656846618951,0.9161248162698281,9.839288053388965E-4,2.0792456839350222E-4,2.6483737488660314E-5]    |\n",
      "|611    |3       |1.0       |[0.08484872539278585,0.9140388959736864,9.087785828222941E-4,1.7685469853302005E-4,2.674535217239636E-5]   |\n",
      "|612    |2       |1.0       |[0.22247446672960774,0.7718897988985794,0.00525180477641777,3.202747860852237E-4,6.365480930995808E-5]     |\n",
      "|613    |2       |1.0       |[0.10964392575531258,0.8884685766359627,0.00120196948115983,6.501574352880451E-4,3.537069227676094E-5]     |\n",
      "|614    |2       |1.0       |[0.0960673523569749,0.9025950135478082,0.0010663126421708445,2.39856604735985E-4,3.146484831007212E-5]     |\n",
      "|615    |2       |1.0       |[0.09286378795240433,0.9058055806082025,0.0010105135642098741,2.8886979954345177E-4,3.12480756397289E-5]   |\n",
      "|616    |2       |1.0       |[0.08378470280803434,0.9149625604366961,9.57776654868401E-4,2.670146721575425E-4,2.7945428243500873E-5]    |\n",
      "|617    |3       |1.0       |[0.07904726641492404,0.9196669697388684,8.560033814039496E-4,4.030849096749807E-4,2.667555512853002E-5]    |\n",
      "|618    |3       |1.0       |[0.1838796759156219,0.8121659229008928,0.0029235782031899714,9.708347800433128E-4,5.9988200251878246E-5]   |\n",
      "|619    |2       |1.0       |[0.2148520582675143,0.781620338155484,0.001898404389110315,0.001560598923037861,6.860026485330449E-5]      |\n",
      "|620    |3       |1.0       |[0.08185304822963592,0.9169735779779766,9.893528834707974E-4,1.584889294858433E-4,2.5531979430701886E-5]   |\n",
      "|621    |2       |1.0       |[0.10800491045944768,0.8901809261062155,0.001689121747343751,9.455479423068003E-5,3.0486892762298637E-5]   |\n",
      "|622    |2       |1.0       |[0.10342575509113881,0.8948011872205635,0.0016565961190348948,8.484401565051889E-5,3.1617553612164486E-5]  |\n",
      "|623    |2       |1.0       |[0.09132036610959199,0.9072254482645042,9.449432882859423E-4,4.797179909128495E-4,2.9524346705111557E-5]   |\n",
      "|624    |2       |1.0       |[0.1928381342820399,0.8032932080194289,0.00270616140252316,0.0011014744681196,6.102182788819623E-5]        |\n",
      "|625    |2       |1.0       |[0.0889225382239483,0.9099988306400594,9.207747761661002E-4,1.3045985647749955E-4,2.739650334872035E-5]    |\n",
      "|626    |3       |1.0       |[0.10083006863044904,0.8973985964493695,0.0016688089982953348,7.230407418593253E-5,3.022184770044602E-5]   |\n",
      "|627    |3       |1.0       |[0.1929259291561761,0.8012806203564596,0.00267620898210914,0.0030493620557426396,6.787944951244916E-5]     |\n",
      "|628    |2       |1.0       |[0.22191027465979848,0.769760556005925,0.0018003059998614227,0.00645046614629743,7.839718811757023E-5]     |\n",
      "|629    |2       |1.0       |[0.08018247896975461,0.91859086588369,8.236357913193207E-4,3.7588828441977463E-4,2.713107081641745E-5]     |\n",
      "|630    |3       |1.0       |[0.16025662947482203,0.8351669151655357,0.0023638955147823396,0.0021566730049820773,5.588683987786052E-5]  |\n",
      "|631    |3       |1.0       |[0.18323486940461833,0.8122496782330249,0.0025682947732829473,0.0018856676800610463,6.148990901276242E-5]  |\n",
      "|632    |3       |1.0       |[0.19028423333844421,0.8056255893692834,0.0028334835019202587,0.0011977973258312483,5.889646452106119E-5]  |\n",
      "|633    |3       |1.0       |[0.08549233886132068,0.9132934000236548,9.791800492019004E-4,2.075001189852492E-4,2.7580946837482422E-5]   |\n",
      "|634    |2       |1.0       |[0.10414172477251174,0.8940703037093392,0.001663014464701419,9.333908061422672E-5,3.161797283335499E-5]    |\n",
      "|635    |2       |1.0       |[0.10650947366731026,0.8915599475705023,0.001780131462228412,1.1691940395394956E-4,3.3527896005167305E-5]  |\n",
      "|636    |2       |1.0       |[0.10512973604706964,0.8931536038977915,0.0016078429628663753,7.608925140620344E-5,3.27278408662907E-5]    |\n",
      "|637    |2       |1.0       |[0.10512973604706964,0.8931536038977915,0.0016078429628663753,7.608925140620344E-5,3.27278408662907E-5]    |\n",
      "|638    |3       |1.0       |[0.1883158102599595,0.8047185677245375,0.006569999571640618,3.342499194580844E-4,6.137252440411466E-5]     |\n",
      "|639    |3       |1.0       |[0.1285422518315614,0.869028486408142,0.0019746347224539568,4.060033441868476E-4,4.862369365578091E-5]     |\n",
      "|640    |2       |1.0       |[0.1060040829557494,0.8923000300311134,0.0011842013464132069,4.7524496369753306E-4,3.644070302641512E-5]   |\n",
      "|641    |2       |1.0       |[0.23946082997159207,0.7546638874202406,0.0017475599149388195,0.004041721653939504,8.600103928898051E-5]   |\n",
      "|642    |2       |1.0       |[0.09430568686933778,0.9043853887437612,9.637552147746155E-4,3.122015604570079E-4,3.2967611669434596E-5]   |\n",
      "|643    |3       |1.0       |[0.08555070711450341,0.9124596294865945,0.0014407678296731938,5.151712885616063E-4,3.3724280667089034E-5]  |\n",
      "|644    |3       |1.0       |[0.1768316953881348,0.8168516536040252,0.004050856603914037,0.0022012231384775237,6.457126544843433E-5]    |\n",
      "|645    |3       |1.0       |[0.13335296263349758,0.8641894127148425,0.002021270495494001,3.8716780405737174E-4,4.91863521086452E-5]    |\n",
      "|646    |2       |1.0       |[0.278233318891927,0.713558090252504,0.0025224456036999636,0.005585372286374632,1.0077296549450575E-4]     |\n",
      "|647    |2       |1.0       |[0.11188058989633816,0.8862203763933356,0.0012793517383434866,5.79690007627322E-4,3.9991964355242075E-5]   |\n",
      "|648    |2       |1.0       |[0.0892091905430678,0.9096370895348715,8.969691845333774E-4,2.286618107798854E-4,2.8088926747457342E-5]    |\n",
      "|649    |3       |1.0       |[0.15109857514546907,0.8475454287097771,0.0012557768076268652,6.0861100955058155E-5,3.935823617167644E-5]  |\n",
      "|650    |2       |1.0       |[0.18200058015909046,0.8158590150644438,0.002066765451523865,2.753263991294275E-5,4.6106685029107246E-5]   |\n",
      "|651    |2       |1.0       |[0.16837966928996953,0.829279044158021,0.0022713079403670258,2.4288699332799286E-5,4.568991230948356E-5]   |\n",
      "|652    |2       |1.0       |[0.3967119341694937,0.5995903224467141,0.003410812031083505,1.843707794063748E-4,1.0256057330222409E-4]    |\n",
      "|653    |2       |1.0       |[0.4015947811666065,0.5943898503035546,0.0037374130685136992,1.7822291794216178E-4,9.973254338299511E-5]   |\n",
      "|654    |2       |1.0       |[0.17097987517473498,0.8266363288033252,0.0023277950131850157,1.437047518915566E-5,4.163053356564975E-5]   |\n",
      "|655    |3       |1.0       |[0.3226742028869298,0.672729421091333,0.004054303657627087,4.5018071961000467E-4,9.189164450028554E-5]     |\n",
      "|656    |2       |1.0       |[0.3829324687593566,0.6140408799443122,0.002125417642941507,7.914449045901026E-4,1.0978874879951182E-4]    |\n",
      "|657    |2       |1.0       |[0.1591723164789425,0.8395658671120275,0.0012061380036174343,1.086835526838402E-5,4.481005014407672E-5]    |\n",
      "|658    |2       |1.0       |[0.146706576919596,0.8521479216838226,0.0010764370970259183,2.9976930145081666E-5,3.9087369410531746E-5]   |\n",
      "|659    |3       |1.0       |[0.3320254052703803,0.6641862586566575,0.0034059180476498792,3.0144618599478476E-4,8.097183931756125E-5]   |\n",
      "|660    |2       |1.0       |[0.23745602787341036,0.7596048258634298,0.002803048092073592,6.827205329744899E-5,6.782611778885641E-5]    |\n",
      "|661    |3       |1.0       |[0.18658478908364393,0.811907325277357,0.001391354120191435,6.656399716078547E-5,4.996752164672196E-5]     |\n",
      "|662    |3       |1.0       |[0.36656910130704295,0.6270153866492338,0.006198060274700464,1.3113416988966422E-4,8.631759913308411E-5]   |\n",
      "|663    |3       |1.0       |[0.22484974157976437,0.7723326192233092,0.002656607340105492,9.730914374830147E-5,6.372271307255039E-5]    |\n",
      "|664    |2       |1.0       |[0.4061029066640132,0.5904862995454729,0.0025028205342270894,7.95517585590655E-4,1.1245567069607811E-4]    |\n",
      "|665    |2       |1.0       |[0.1843086893362461,0.8138744817316745,0.001671033800689953,9.497307021292014E-5,5.082206117673928E-5]     |\n",
      "|666    |2       |1.0       |[0.16916815790182246,0.8293925389979339,0.0012946629351950213,9.730111373063297E-5,4.7339051318031674E-5]  |\n",
      "|667    |2       |1.0       |[0.18324555224463354,0.8150651636455932,0.0015412649800057418,1.0080065234237269E-4,4.72184774250204E-5]   |\n",
      "|668    |3       |1.0       |[0.15899000079239473,0.8395314367400842,0.0013215436269665933,1.105840394406264E-4,4.6434801113988253E-5]  |\n",
      "|669    |2       |1.0       |[0.3671560939621549,0.6295451935070839,0.002230985782184327,9.579642487129434E-4,1.0976249986399953E-4]    |\n",
      "|670    |2       |1.0       |[0.36714227941270555,0.6295591117216487,0.0022308938258678167,9.57954814020839E-4,1.097602257570291E-4]    |\n",
      "|671    |2       |1.0       |[0.14338372020490942,0.8552549616360949,0.0012405968028025157,8.04648478689417E-5,4.0256508324284604E-5]   |\n",
      "|672    |2       |1.0       |[0.16751754280090655,0.8299683424191442,0.0024340157985754886,3.445652437358032E-5,4.564245700036739E-5]   |\n",
      "|673    |2       |1.0       |[0.16615602977067537,0.8315670415149661,0.002198539341019729,3.08139718014718E-5,4.7575401537235114E-5]    |\n",
      "|674    |2       |1.0       |[0.1614940626663546,0.8360346681501687,0.002395910212828259,2.7828816650499403E-5,4.7530153997794674E-5]   |\n",
      "|675    |2       |1.0       |[0.1711331483858859,0.8263984158335492,0.0024006335446542957,2.1531450168790704E-5,4.627078574166461E-5]   |\n",
      "|676    |2       |1.0       |[0.3877235105911523,0.6080101790920418,0.0039760036106738115,1.8299891985800378E-4,1.0730778627411154E-4]  |\n",
      "|677    |2       |1.0       |[0.14713016048407607,0.8497286659114782,0.003081821355118364,1.8772330271508278E-5,4.057991905573995E-5]   |\n",
      "|678    |2       |1.0       |[0.15984731981195402,0.83855454219401,0.0014346130431246334,1.176355894297737E-4,4.588936148146955E-5]     |\n",
      "|679    |2       |1.0       |[0.15704107823363322,0.8413699338892445,0.0014359436433431009,1.0920971856497608E-4,4.3834515214089484E-5] |\n",
      "|680    |2       |1.0       |[0.14680607586030725,0.8517664489363792,0.0012960071539845287,9.042285387951306E-5,4.1045195449534945E-5]  |\n",
      "|681    |2       |1.0       |[0.20814541622432,0.788806462358633,0.0028820814350603505,1.0141737526817231E-4,6.462260671850944E-5]      |\n",
      "|682    |2       |1.0       |[0.16942165038061052,0.8289449646451315,0.0014826390019271171,1.028714929977685E-4,4.787447933319494E-5]   |\n",
      "|683    |2       |1.0       |[0.16465009868068964,0.833795201024513,0.0014048809336494644,1.050189788740781E-4,4.480038227374545E-5]    |\n",
      "|684    |2       |1.0       |[0.3683288559058611,0.6277813089512181,0.002634097103864702,0.0011442164267430825,1.1152161231297522E-4]   |\n",
      "|685    |2       |1.0       |[0.14075716498869215,0.8579107788773145,0.0012348806393401974,5.85837951774848E-5,3.859169947572385E-5]    |\n",
      "|686    |2       |1.0       |[0.14070580027066953,0.8579626450335176,0.0012343984293947483,5.857400391328113E-5,3.8582262504901216E-5]  |\n",
      "|687    |3       |1.0       |[0.14387786222525992,0.8548088606531925,0.0012341930460124772,3.983462070857256E-5,3.9249454826588254E-5]  |\n",
      "|688    |2       |1.0       |[0.17387396111602235,0.8237226867070915,0.0023332168286800373,2.5287633092754877E-5,4.484771511320398E-5]  |\n",
      "|689    |3       |1.0       |[0.16868139886112887,0.8286144633090732,0.0026336845061240774,2.403920632997151E-5,4.641411734397469E-5]   |\n",
      "|690    |2       |1.0       |[0.15118248808914073,0.8456819331105883,0.0030788690784856517,1.613012070141414E-5,4.0579601083923255E-5]  |\n",
      "|691    |3       |1.0       |[0.211956718570476,0.7853962536028568,0.002514955791800319,7.19000867780638E-5,6.017194808891623E-5]       |\n",
      "|692    |2       |1.0       |[0.1712765437209723,0.8269196656382256,0.0016320091052248902,1.199301771984387E-4,5.1851358378799254E-5]   |\n",
      "|693    |2       |1.0       |[0.17299731383677058,0.8253217174431132,0.0015171331768212624,1.1533949053281702E-4,4.849605276199711E-5]  |\n",
      "|694    |2       |1.0       |[0.36317744631905,0.6338791926594608,0.0020213549545127205,8.222637724264273E-4,9.974229454993005E-5]      |\n",
      "|695    |2       |1.0       |[0.1458715074962155,0.8528032559537169,0.0012030357963481889,8.30389337416903E-5,3.916181997793986E-5]     |\n",
      "|696    |3       |1.0       |[0.16695312457729852,0.8309045802321399,0.002075952974094293,2.3149442526910977E-5,4.3192773940475423E-5]  |\n",
      "|697    |2       |1.0       |[0.14597704075527565,0.8526248317864212,0.0012389683487229808,1.1630859318955421E-4,4.285051639057819E-5]  |\n",
      "|698    |2       |1.0       |[0.17794250695771535,0.8194449576701689,0.0025330763189199672,3.079203243774433E-5,4.866702075786046E-5]   |\n",
      "|699    |3       |1.0       |[0.33328217144571964,0.6600524122197449,0.0064585359936244185,1.1659567488080241E-4,9.028466603027656E-5]  |\n",
      "|700    |2       |1.0       |[0.15821582056583733,0.8394568082524916,0.002262805721033896,2.144873951152458E-5,4.31167211258524E-5]     |\n",
      "|701    |3       |1.0       |[0.1695109891993757,0.8289211891928127,0.0014003369642913818,1.1184846002106309E-4,5.563618349913159E-5]   |\n",
      "|702    |3       |1.0       |[0.3240157528619879,0.6718241825859829,0.0035053974585138063,5.506437096258587E-4,1.0402338388951253E-4]   |\n",
      "|703    |2       |1.0       |[0.3722411483977365,0.6238335538325268,0.0025219621710339275,0.00128571726740163,1.1761833130114235E-4]    |\n",
      "|704    |2       |1.0       |[0.15316495564295948,0.8454151974908259,0.001281440769385394,9.139025979539785E-5,4.701583703384944E-5]    |\n",
      "|705    |2       |1.0       |[0.1397556933059381,0.8588556451572071,0.0012478432177693126,9.919292387763425E-5,4.162539520773138E-5]    |\n",
      "|706    |3       |1.0       |[0.3067413624850403,0.6889606993404397,0.003605690649311108,5.927333430176368E-4,9.951418219138144E-5]     |\n",
      "|707    |3       |1.0       |[0.14694881565223233,0.8515073173127911,0.0013781536264886906,1.2377534800706334E-4,4.193806048096067E-5]  |\n",
      "|708    |2       |1.0       |[0.13077644958952908,0.8679166949662853,0.0011819770653991146,8.515388530599028E-5,3.972449348042459E-5]   |\n",
      "|709    |3       |1.0       |[0.3842924651590478,0.6077723051273327,0.006727218472662706,0.0010789045836101644,1.2910665734658258E-4]   |\n",
      "|710    |2       |1.0       |[0.16332744352782955,0.834919057504998,0.0015168351440371397,1.8165981913689726E-4,5.5004003998396006E-5]  |\n",
      "|711    |2       |1.0       |[0.16462899681258575,0.8336987509371069,0.0014656236971030236,1.5548930333123863E-4,5.113924987306816E-5]  |\n",
      "|712    |3       |1.0       |[0.2937347287147901,0.7000865558090895,0.005300488403032747,7.80508009781803E-4,9.77190633059161E-5]       |\n",
      "|713    |2       |1.0       |[0.15299028479211224,0.8452685015546191,0.0014976540960114164,1.927815399261018E-4,5.077801733117375E-5]   |\n",
      "|714    |2       |1.0       |[0.15589418205527378,0.8425124037681873,0.0014335665266345722,1.1281266187489634E-4,4.70349880294997E-5]   |\n",
      "|715    |2       |1.0       |[0.14026869277969917,0.8582529520279185,0.0013700820890560923,6.78569652684918E-5,4.041613805777453E-5]    |\n",
      "|716    |2       |1.0       |[0.14026869277969917,0.8582529520279185,0.0013700820890560923,6.78569652684918E-5,4.041613805777453E-5]    |\n",
      "|717    |2       |1.0       |[0.1560146566986074,0.8416878186408621,0.002219057454635159,3.458213102253928E-5,4.388507487273525E-5]     |\n",
      "|718    |2       |1.0       |[0.1980683425817121,0.7992379215720852,0.0024915282962102506,1.389210536200456E-4,6.328649637252039E-5]    |\n",
      "|719    |2       |1.0       |[0.36352050203516456,0.6330355255753504,0.002426123020211219,9.051442882870644E-4,1.1270508098677836E-4]   |\n",
      "|720    |2       |1.0       |[0.15420315696184647,0.8442948013219682,0.0013449383452859313,1.1296119779812642E-4,4.4142173101032035E-5] |\n",
      "|721    |2       |1.0       |[0.13495090470384988,0.8637603000031935,0.0011936848963810906,5.484083127273835E-5,4.026956530302903E-5]   |\n",
      "|722    |2       |1.0       |[0.16835570504673977,0.8292408018496485,0.0023253205938581793,3.219236259469839E-5,4.598014715868032E-5]   |\n",
      "|723    |2       |1.0       |[0.1665738621470853,0.8310368133056075,0.0023247371753970713,2.0171491833135644E-5,4.441588007706713E-5]   |\n",
      "|724    |2       |1.0       |[0.17267487788913666,0.8256857055044837,0.0014784632819638097,1.072619273550012E-4,5.3691397060871715E-5]  |\n",
      "|725    |2       |1.0       |[0.16674904444437944,0.8316409972741854,0.0014210086303927858,1.3732442029656746E-4,5.1625230745675674E-5] |\n",
      "|726    |2       |1.0       |[0.14416773097524724,0.8543561359740957,0.0012956025590329298,1.4083793699627006E-4,3.969255462801344E-5]  |\n",
      "|727    |2       |1.0       |[0.14416778713968542,0.8543560792340503,0.0012956030992963554,1.4083796199961294E-4,3.9692564968290784E-5] |\n",
      "|728    |2       |1.0       |[0.16600536371959174,0.8316020911575677,0.002317369628218079,2.9306327981114903E-5,4.5869166641376544E-5]  |\n",
      "|729    |3       |1.0       |[0.1652977466220106,0.8324797798338274,0.0021479430424024,2.78948386443412E-5,4.663566311535146E-5]        |\n",
      "|730    |2       |1.0       |[0.1533284113970952,0.8447209664416071,0.0018803317952010771,2.3636604209848202E-5,4.665376188673284E-5]   |\n",
      "|731    |2       |1.0       |[0.15173839259224206,0.846321293974108,0.0018732050581394418,2.1347137652897247E-5,4.576123785764561E-5]   |\n",
      "|732    |2       |1.0       |[0.3600485240058998,0.6358803998119371,0.0037508575938088274,2.0961843778192398E-4,1.106001505723416E-4]   |\n",
      "|733    |3       |1.0       |[0.3030274890640669,0.691519434442466,0.005280695330954506,8.001116237386273E-5,9.237000013861019E-5]      |\n",
      "|734    |3       |1.0       |[0.1912854520376289,0.8063527004218762,0.0021461068570284804,1.470603893791262E-4,6.868029408735172E-5]    |\n",
      "|735    |3       |1.0       |[0.19128510038855817,0.8063530565068429,0.002146102617500405,1.4706027494739552E-4,6.868021215112723E-5]   |\n",
      "|736    |2       |1.0       |[0.1912849597290481,0.8063531989407097,0.0021461009216908033,1.4706022917467492E-4,6.868017937664616E-5]   |\n",
      "|737    |2       |1.0       |[0.19704057710641407,0.8004933463858562,0.002266557572351082,1.297607053298825E-4,6.97582300488648E-5]     |\n",
      "|738    |3       |1.0       |[0.1781716740005185,0.8197140465992727,0.001909301125439724,1.403396536656043E-4,6.463862110335264E-5]     |\n",
      "|739    |2       |1.0       |[0.17625928784749942,0.8216492987314037,0.0019012709956417412,1.2674873428717963E-4,6.33936911679546E-5]   |\n",
      "|740    |2       |1.0       |[0.17625948595238095,0.8216490982246712,0.0019012732889593783,1.2674879611608735E-4,6.339373787238965E-5]  |\n",
      "|741    |2       |1.0       |[0.17936412252754994,0.8183658200397874,0.0020729706963970576,1.334221623771073E-4,6.366457388865096E-5]   |\n",
      "|742    |2       |1.0       |[0.1778717513061457,0.8197948847967094,0.0021460768019675052,1.250115926533483E-4,6.22755025240313E-5]     |\n",
      "|743    |2       |1.0       |[0.15059813512684725,0.8478541224270442,0.001317569299520357,1.785043193467436E-4,5.166882724165696E-5]    |\n",
      "|744    |2       |1.0       |[0.1438397682122307,0.8547062060540096,0.001287025929442416,1.2257241548521464E-4,4.4427388832176865E-5]   |\n",
      "|745    |3       |1.0       |[0.1477940501291992,0.8507880046805455,0.001263718459138248,1.0969513848698764E-4,4.453159262990168E-5]    |\n",
      "|746    |2       |1.0       |[0.3020178188489594,0.6937973858952661,0.003568608924319889,5.252857381134442E-4,9.090059334129657E-5]     |\n",
      "|747    |3       |1.0       |[0.2637330002619842,0.7326337183548883,0.0033390146486942675,2.1520842833748917E-4,7.905830609548258E-5]   |\n",
      "|748    |2       |1.0       |[0.17436858531590263,0.8237331242553635,0.0018454216123277517,6.4308766746640565E-6,4.6437939731488734E-5] |\n",
      "|749    |2       |1.0       |[0.23181725879774867,0.765829436507315,0.0022199494859640873,6.83208132284529E-5,6.503439574346655E-5]     |\n",
      "|750    |2       |1.0       |[0.20820891557179508,0.7896837574966507,0.0020053730468539747,4.2446558989369305E-5,5.950732571080055E-5]  |\n",
      "|751    |2       |1.0       |[0.4783005984438045,0.5171816156813878,0.003899996397166945,4.76132189796735E-4,1.4165728784422166E-4]     |\n",
      "|752    |3       |1.0       |[0.1708318583852956,0.82788169362368,0.001196661362118368,4.671100833518876E-5,4.307562057090936E-5]       |\n",
      "|753    |2       |1.0       |[0.1931651462875969,0.8048153385084154,0.0019558315349616708,1.4610066321499213E-5,4.907360270459122E-5]   |\n",
      "|754    |2       |1.0       |[0.1857547835849486,0.8122276254491807,0.0019627161596171097,1.2141572743608831E-5,4.273323350994531E-5]   |\n",
      "|755    |2       |1.0       |[0.13988091633432514,0.8590384294060985,0.0010119369617333591,3.271353595078403E-5,3.6003761892186826E-5]  |\n",
      "|756    |2       |1.0       |[0.13834275276229524,0.8605849846307881,0.0010074310138101447,2.9533574094345572E-5,3.529801901215362E-5]  |\n",
      "|757    |2       |1.0       |[0.15994970966996983,0.8388351632820196,0.0011450924689201446,2.9842395422725992E-5,4.01921836677561E-5]   |\n",
      "|758    |2       |1.0       |[0.17804892788212437,0.8200723586542267,0.001825276519603776,1.2322443999487484E-5,4.111450004577138E-5]   |\n",
      "|759    |2       |1.0       |[0.17804859501777454,0.8200726952415053,0.00182527285697841,1.2322434027044368E-5,4.111444971452833E-5]    |\n",
      "|760    |2       |1.0       |[0.18120520809045493,0.8169199385558794,0.0018187725050493146,1.2105444173420551E-5,4.397540444284968E-5]  |\n",
      "|761    |3       |1.0       |[0.17259372777291704,0.8255609551915276,0.0017339545039675866,6.545051555246032E-5,4.5912016035439154E-5]  |\n",
      "|762    |3       |1.0       |[0.19430338834602645,0.8042618896751982,0.001323383047736366,6.295685395925406E-5,4.838207707956629E-5]    |\n",
      "|763    |3       |1.0       |[0.16831114654181661,0.829859873058683,0.0017070066419023519,7.62317979388125E-5,4.574195965938139E-5]     |\n",
      "|764    |2       |1.0       |[0.1465720139369141,0.8522487104770455,0.0010716340922601085,6.999560436863386E-5,3.764588941162383E-5]    |\n",
      "|765    |2       |1.0       |[0.17890614699925222,0.819164696706713,0.0018714201041834441,1.5649574136082337E-5,4.208661571525496E-5]   |\n",
      "|766    |2       |1.0       |[0.2239055466216339,0.7737383816826923,0.0022077267438594893,8.717124477122236E-5,6.117370704307741E-5]    |\n",
      "|767    |2       |1.0       |[0.2239038088470025,0.7737401384947198,0.002207708277447432,8.717097486743852E-5,6.117340596312431E-5]     |\n",
      "|768    |3       |1.0       |[0.17593721378792546,0.8225939744070904,0.0013133635432295817,1.0821012691246679E-4,4.723813484235796E-5]  |\n",
      "|769    |2       |1.0       |[0.3627098406022507,0.6338911941007883,0.002434658547535794,8.449318403899404E-4,1.1937490903521169E-4]    |\n",
      "|770    |3       |1.0       |[0.2806255248545287,0.7155450367867812,0.0034440392552297166,2.938413072521017E-4,9.155779620834034E-5]    |\n",
      "|771    |2       |1.0       |[0.13435696768342267,0.8644943541467186,0.001078215569994016,3.4618340662020226E-5,3.584425920278044E-5]   |\n",
      "|772    |2       |1.0       |[0.1594165401884949,0.838635632875073,0.0018848097465141052,2.063487639620636E-5,4.2382313521862214E-5]    |\n",
      "|773    |2       |1.0       |[0.15300133974974608,0.8445988272192763,0.0023377886713561087,1.819499996807781E-5,4.38493596535091E-5]    |\n",
      "|774    |2       |1.0       |[0.15709674652161398,0.8411429992577055,0.001713448843274839,5.035348475493651E-6,4.177002893000546E-5]    |\n",
      "|775    |3       |1.0       |[0.14340954012462878,0.8543625923404795,0.0021699462460245916,1.673149607071997E-5,4.118979279645378E-5]   |\n",
      "|776    |3       |1.0       |[0.3566755343135416,0.6362668248984293,0.006157286330914262,7.73787208034498E-4,1.265672490803133E-4]      |\n",
      "|777    |3       |1.0       |[0.4114126670878885,0.5841113814719913,0.0034888981390546487,8.516868562829231E-4,1.3536644478273383E-4]   |\n",
      "|778    |3       |1.0       |[0.33180973322444907,0.6625572648211311,0.005020122079043654,4.894382482049511E-4,1.2344162717130974E-4]   |\n",
      "|779    |3       |1.0       |[0.17930867441256906,0.8184739636654232,0.0020276858150405203,1.2290268675621414E-4,6.677342021106323E-5]  |\n",
      "|780    |3       |1.0       |[0.17283604617898773,0.8249086824988257,0.0021586210849779855,4.128556683627828E-5,5.536467037227919E-5]   |\n",
      "|781    |3       |1.0       |[0.38748240522273936,0.6085135487681023,0.0035610909987251332,3.169963358127189E-4,1.2595867462033135E-4]  |\n",
      "|782    |3       |1.0       |[0.17096755015771503,0.8267914971417982,0.002149374164920214,3.7284429102081664E-5,5.429410646422821E-5]   |\n",
      "|783    |3       |1.0       |[0.17576872716061898,0.8220698034137307,0.002039969068333396,6.698835416462948E-5,5.451200315222727E-5]    |\n",
      "|784    |2       |1.0       |[0.40123405855087346,0.5943163569560175,0.00358361937072575,7.241092736607379E-4,1.4185584872263102E-4]    |\n",
      "|785    |2       |1.0       |[0.14703939934001348,0.8516885807325834,0.0011524595452582114,7.46210224385043E-5,4.4939359706678036E-5]   |\n",
      "|786    |3       |1.0       |[0.295295457561905,0.7005937954233792,0.0034131535881056383,5.936528217954278E-4,1.0394060481463536E-4]    |\n",
      "|787    |3       |1.0       |[0.13749882262490826,0.8596340545729758,0.0027962516946619526,2.9577844732186583E-5,4.129326272180178E-5]  |\n",
      "|788    |2       |1.0       |[0.37119078694238977,0.6248700614752705,0.003601353659999883,2.293909983113735E-4,1.0840692402845864E-4]   |\n",
      "|789    |2       |1.0       |[0.15853889818795858,0.8391669890466269,0.0022279363996013345,2.2412742480622132E-5,4.376362333256917E-5]  |\n",
      "|790    |2       |1.0       |[0.36419766602372877,0.6317813148936924,0.0037808062666845202,1.3967092984070912E-4,1.0054188605343138E-4] |\n",
      "|791    |2       |1.0       |[0.2043621884707465,0.7928635100457617,0.002552436356107566,1.5708068235812322E-4,6.478444502616706E-5]    |\n",
      "|792    |2       |1.0       |[0.1951333624492188,0.8023534988170767,0.002337261618609795,1.1308772501945335E-4,6.278939007499051E-5]    |\n",
      "|793    |2       |1.0       |[0.12961917888763314,0.8690605696235201,0.0012226394941912476,5.901772716690203E-5,3.859426748855816E-5]   |\n",
      "|794    |2       |1.0       |[0.12961851116028933,0.8690612443604689,0.0012226327599876457,5.901758582590681E-5,3.859413342825556E-5]   |\n",
      "|795    |3       |1.0       |[0.15213416895622842,0.8458846716109208,0.0019078372861074493,2.6323700181698894E-5,4.6998446561613215E-5] |\n",
      "|796    |2       |1.0       |[0.13091968101322077,0.8673743266156527,0.0016569222955247472,1.0944505394756136E-5,3.8125570207087397E-5] |\n",
      "|797    |2       |1.0       |[0.16593108985730395,0.8321925831546292,0.0018243782167162495,6.280706438017766E-6,4.566806491269052E-5]   |\n",
      "|798    |2       |1.0       |[0.42152324525338936,0.5737747729918499,0.00346380317299289,0.0010883391330451914,1.4983944872261356E-4]   |\n",
      "|799    |2       |1.0       |[0.20012932708651895,0.7974162047778963,0.002238371821882366,1.4607539279723762E-4,7.002092090511705E-5]   |\n",
      "|800    |2       |1.0       |[0.19815062159450283,0.7998440500371546,0.0018963318084275823,4.572629376563485E-5,6.327026614938691E-5]   |\n",
      "|801    |3       |1.0       |[0.1969333746808571,0.8004525445712531,0.002408854269053857,1.403610702476272E-4,6.486540858851899E-5]     |\n",
      "|802    |3       |1.0       |[0.16637782011195282,0.8322649072579728,0.0012593047863839908,5.322854140886669E-5,4.473930228152742E-5]   |\n",
      "|803    |3       |1.0       |[0.16637832521796586,0.8322643978916597,0.0012593088857536614,5.322861271063604E-5,4.473939191012174E-5]   |\n",
      "|804    |2       |1.0       |[0.19678175990316255,0.8010241596419366,0.0021320747019664167,1.2711103481470408E-5,4.9294649453075004E-5] |\n",
      "|805    |2       |1.0       |[0.20155313331617672,0.7959150250680312,0.0024678474148356823,1.701816639741661E-5,4.697603455897528E-5]   |\n",
      "|806    |3       |1.0       |[0.19109192685007598,0.8065070751258245,0.002339121752999122,1.5792791923614583E-5,4.60834791768882E-5]    |\n",
      "|807    |2       |1.0       |[0.1741848287633483,0.8238037547628843,0.001958387440454857,1.134460083067843E-5,4.168443248192938E-5]     |\n",
      "|808    |2       |1.0       |[0.17412530106466764,0.8238640116305674,0.0019576693110932913,1.1342912330974457E-5,4.167508134064661E-5]  |\n",
      "|809    |3       |1.0       |[0.17073716539367834,0.8272882514570237,0.0019240513517268757,9.951290163056575E-6,4.058050740786182E-5]   |\n",
      "|810    |2       |1.0       |[0.3796813087640314,0.617003432766286,0.003164270304638173,6.153045663134323E-5,8.94577084131899E-5]       |\n",
      "|811    |3       |1.0       |[0.4027096903722863,0.5914608464552354,0.005418465681449714,2.9929898745951285E-4,1.1169850356910815E-4]   |\n",
      "|812    |3       |1.0       |[0.21296017472387815,0.784824149454274,0.002089997601004556,6.62823722018514E-5,5.9395848641520355E-5]     |\n",
      "|813    |3       |1.0       |[0.22937560859701459,0.768093782543558,0.002407425797025787,5.839811338139892E-5,6.47849490202536E-5]      |\n",
      "|814    |3       |1.0       |[0.29790416443370416,0.6986770990383866,0.0030406929609763556,2.964836574618764E-4,8.155990947100013E-5]   |\n",
      "|815    |2       |1.0       |[0.1574158978895598,0.8411473821053698,0.001349784230725859,4.6619931061606965E-5,4.031584328282322E-5]    |\n",
      "|816    |3       |1.0       |[0.29812235537733944,0.6981754020662242,0.0030905702496607647,5.277895409094023E-4,8.388276586618907E-5]   |\n",
      "|817    |2       |1.0       |[0.15147618094893284,0.847202150263493,0.001168790863825354,1.1058116160016728E-4,4.229676214858909E-5]    |\n",
      "|818    |2       |1.0       |[0.3395635600943784,0.657817973932436,0.001886781933991982,6.407178965807776E-4,9.096614261280807E-5]      |\n",
      "|819    |2       |1.0       |[0.14339135655427607,0.8554090818591196,0.001089910218865483,7.212425110340596E-5,3.752711663539838E-5]    |\n",
      "|820    |2       |1.0       |[0.39038170354201374,0.606332797901153,0.003053742120774478,1.3424636438710014E-4,9.751007167170215E-5]    |\n",
      "|821    |3       |1.0       |[0.23018718795114018,0.7671375410409155,0.0025089454650155598,1.014259292574662E-4,6.489961367133122E-5]   |\n",
      "|822    |2       |1.0       |[0.1694462446049781,0.8292099844918794,0.001186427507413175,1.1114381725505072E-4,4.61995784743616E-5]     |\n",
      "|823    |3       |1.0       |[0.1520079205573045,0.8468323558076954,0.001034981150642214,8.253216290370534E-5,4.2210321454045834E-5]    |\n",
      "|824    |3       |1.0       |[0.1794904696873674,0.8182532383197565,0.0021930966399916194,2.067904270632901E-5,4.2516310177922265E-5]   |\n",
      "|825    |2       |1.0       |[0.4504097302789055,0.5445107229659742,0.0037848528861591543,0.0011583032798642146,1.3639058909693187E-4]  |\n",
      "|826    |2       |1.0       |[0.21118366292790242,0.7861815368642071,0.002427553560533603,1.447725759485658E-4,6.247407140826722E-5]    |\n",
      "|827    |2       |1.0       |[0.3994621667972348,0.5934327141019902,0.006345375253713138,6.417016330892489E-4,1.1804221397277193E-4]    |\n",
      "|828    |3       |1.0       |[0.21117510878431542,0.7861902008738738,0.0024274477091968776,1.4477018062910104E-4,6.247245198472785E-5]  |\n",
      "|829    |2       |1.0       |[0.14759620392136646,0.8509453901375542,0.0013052966243030147,1.0684210821892592E-4,4.6267208557226254E-5] |\n",
      "|830    |3       |1.0       |[0.1426917949321304,0.8559564641646217,0.0012024238244545634,1.0597366615077772E-4,4.334341264248439E-5]   |\n",
      "|831    |3       |1.0       |[0.16423243766537768,0.8334547888284444,0.0022275312796917247,3.807156562143151E-5,4.717066086510569E-5]   |\n",
      "|832    |2       |1.0       |[0.15843997967713697,0.8391596969244594,0.002329590119908648,2.724157029420545E-5,4.349170820094683E-5]    |\n",
      "|833    |2       |1.0       |[0.15473724846987397,0.8437792281470815,0.001327638370941795,1.0856968741914504E-4,4.731532468365713E-5]   |\n",
      "|834    |2       |1.0       |[0.14251353712676895,0.85598385278032,0.0013329705882871273,1.221030729989529E-4,4.753643162487721E-5]     |\n",
      "|835    |2       |1.0       |[0.14251637403603898,0.8559809857289701,0.0013329989868663542,1.221041831396714E-4,4.7537064984946293E-5]  |\n",
      "|836    |2       |1.0       |[0.33556887806884433,0.6610854459517126,0.0023654423440181717,8.694270601739799E-4,1.1080657525093442E-4]  |\n",
      "|837    |3       |1.0       |[0.17325412350359293,0.8244964928312595,0.0021835551910757284,1.7600408030557456E-5,4.822806604145926E-5]  |\n",
      "|838    |2       |1.0       |[0.17813248723420763,0.819590135764495,0.002213618013512552,1.5790964621678432E-5,4.796802316303391E-5]    |\n",
      "|839    |2       |1.0       |[0.3960225030678854,0.5998390430789847,0.003937332909134422,9.964570625787212E-5,1.0147523773770817E-4]    |\n",
      "|840    |2       |1.0       |[0.18031715898512898,0.8159469563835036,0.0031734909869696195,4.924892127560018E-4,6.990443164192176E-5]   |\n",
      "|841    |3       |1.0       |[0.1196353716316695,0.8787390582836944,0.0014231428413232065,1.6335749566088442E-4,3.9069747651803215E-5]  |\n",
      "|842    |3       |1.0       |[0.14607037567511946,0.8512513918679915,0.0025107500954994998,1.1905212678182759E-4,4.843023460766956E-5]  |\n",
      "|843    |3       |1.0       |[0.2827636265847038,0.7087675471824011,0.007884137561811805,4.908458932460143E-4,9.384277783736941E-5]     |\n",
      "|844    |2       |1.0       |[0.14430307812239965,0.852560765301471,0.0029750751072338487,1.13141360804845E-4,4.794010809051134E-5]     |\n",
      "|845    |3       |1.0       |[0.2885558656960891,0.7034580460595894,0.00747228869291149,4.2422724697489805E-4,8.957230443509493E-5]     |\n",
      "|846    |3       |1.0       |[0.13865665772202537,0.8586057903528412,0.002615396205212959,7.943384175108367E-5,4.272187816939084E-5]    |\n",
      "|847    |2       |1.0       |[0.13761208757316248,0.8602051373841203,0.0016160894383117255,5.165445566067545E-4,5.01410477986433E-5]    |\n",
      "|848    |2       |1.0       |[0.13319234457456808,0.8647493373846803,0.0015833697994995823,4.283379580815342E-4,4.661028317036824E-5]   |\n",
      "|849    |3       |1.0       |[0.2746218894417651,0.7174436211142742,0.007089916510759792,7.471600332095962E-4,9.741289999143403E-5]     |\n",
      "|850    |2       |1.0       |[0.3392102589403049,0.6545726607935077,0.005063574105917004,0.0010374301143043826,1.1607604596604635E-4]   |\n",
      "|851    |2       |1.0       |[0.3029276852905779,0.6902415311929658,0.0027134854092243786,0.004011199672361167,1.0609843487076022E-4]   |\n",
      "|852    |2       |1.0       |[0.14271307123004215,0.8547682746539509,0.002347698732775465,1.1831288100129339E-4,5.264250223017491E-5]   |\n",
      "|853    |3       |1.0       |[0.17361857608832376,0.8229987441048063,0.00239387949896914,9.113834784572909E-4,7.741682944346201E-5]     |\n",
      "|854    |2       |1.0       |[0.12855788347463007,0.8694080673242613,0.0012924636171269962,6.869369671956323E-4,5.46486167862098E-5]    |\n",
      "|855    |2       |1.0       |[0.11959734149114741,0.8781702220597178,0.0014705252691356587,7.129975058717153E-4,4.891367412742486E-5]   |\n",
      "|856    |2       |1.0       |[0.09966245819557487,0.8980972068025496,0.0018305791283261229,3.7208482764144676E-4,3.767104590785399E-5]  |\n",
      "|857    |2       |1.0       |[0.1329876883649004,0.8643736440770707,0.0024791921182640026,1.1505950347552213E-4,4.441593628932384E-5]   |\n",
      "|858    |2       |1.0       |[0.3163467515248769,0.6780981815706948,0.002304123589686709,0.0031462365041553505,1.0470681058630864E-4]   |\n",
      "|859    |2       |1.0       |[0.14593066219525944,0.8518369543812114,0.002131380505222479,5.752336401287362E-5,4.347955429388334E-5]    |\n",
      "|860    |2       |1.0       |[0.1672769459306697,0.8301451262315667,0.002228012593474508,2.870352934816713E-4,6.287995080750234E-5]     |\n",
      "|861    |2       |1.0       |[0.13155425537118354,0.8664053142677529,0.001433400137963517,5.560579439200904E-4,5.0972279180075616E-5]   |\n",
      "|862    |2       |1.0       |[0.13011425554200554,0.8679066669143707,0.0014271000445965788,5.020035093057964E-4,4.997398972146048E-5]   |\n",
      "|863    |3       |1.0       |[0.13771136547481336,0.8601766557255216,0.0015314047780223088,5.352138303511523E-4,4.536019129175385E-5]   |\n",
      "|864    |2       |1.0       |[0.16454668502285127,0.8319084709491281,0.0027888430536163592,6.915767312279299E-4,6.442424317623519E-5]   |\n",
      "|865    |2       |1.0       |[0.16287649301507273,0.8336566849443152,0.0027788924613342193,6.247240686716273E-4,6.320551060612894E-5]   |\n",
      "|866    |2       |1.0       |[0.3915362384188196,0.597417080404657,0.005193683599972256,0.005700464825045217,1.525327515057857E-4]      |\n",
      "|867    |3       |1.0       |[0.3244854550895474,0.6653282844571586,0.008122981041342051,0.0019434137608409226,1.1986565111082409E-4]   |\n",
      "|868    |3       |1.0       |[0.13008457638746934,0.867554101017355,0.0015975046638725364,7.175448288259139E-4,4.627310247726961E-5]    |\n",
      "|869    |3       |1.0       |[0.12984237610390525,0.8677316761828393,0.0015418808317896853,8.386475204587639E-4,4.541936100693989E-5]   |\n",
      "|870    |2       |1.0       |[0.13584330982868018,0.8616090468981382,0.002414169500591611,9.196513006719176E-5,4.150864252274253E-5]    |\n",
      "|871    |2       |1.0       |[0.16320157945872263,0.8335245802226381,0.0026162224380272873,5.866608643068671E-4,7.095701630499727E-5]   |\n",
      "|872    |2       |1.0       |[0.10802903210006674,0.8901701307087385,0.001372440595471774,3.893982995480506E-4,3.899829617510703E-5]    |\n",
      "|873    |2       |1.0       |[0.13547311142898147,0.8618899451898457,0.002438692065713303,1.5319297884507088E-4,4.5058336614431575E-5]  |\n",
      "|874    |3       |1.0       |[0.2863333001059372,0.7054050856597094,0.007451441641761018,7.168163726753591E-4,9.335621991703528E-5]     |\n",
      "|875    |3       |1.0       |[0.3471916397893159,0.6425163270724285,0.007656668252875777,0.0024945414467944307,1.4082343858535712E-4]   |\n",
      "|876    |3       |1.0       |[0.1186012098789349,0.8794967159861337,0.0014146009741751763,4.4346947554049516E-4,4.400368521564972E-5]   |\n",
      "|877    |2       |1.0       |[0.1362111981170044,0.8610360383006017,0.002580426283608956,1.238544516353073E-4,4.848284714957518E-5]     |\n",
      "|878    |2       |1.0       |[0.15808370754243464,0.8386897136235673,0.0025106824527184957,6.513729103818533E-4,6.452347089785103E-5]   |\n",
      "|879    |2       |1.0       |[0.10846292350989944,0.8885083468814423,0.00198300202421486,9.99718751807931E-4,4.600883263545506E-5]      |\n",
      "|880    |3       |1.0       |[0.22557523719577163,0.7678811133793598,0.0037582118688798743,0.0026932804425818355,9.215711340684634E-5]  |\n",
      "|881    |3       |1.0       |[0.23497104714791764,0.7588807692640597,0.004573549657219549,0.0014892197449282941,8.541418587473357E-5]   |\n",
      "|882    |3       |1.0       |[0.1378883223053814,0.859680789715569,0.001573857545469194,8.028612684552745E-4,5.4169165125371455E-5]     |\n",
      "|883    |2       |1.0       |[0.1274282068050986,0.8704296195721375,0.0014064071307842785,6.839849898216638E-4,5.178150215788365E-5]    |\n",
      "|884    |2       |1.0       |[0.1520306015894803,0.8464560353321372,0.0013722169868678974,8.79669029000843E-5,5.317918861453615E-5]     |\n",
      "|885    |2       |1.0       |[0.11684914636367905,0.881834866822504,0.0011443275827203662,1.2873637385390238E-4,4.292285724264145E-5]   |\n",
      "|886    |3       |1.0       |[0.26298920002818815,0.7302906571173633,0.004577923221595577,0.002049480018709237,9.273961414388846E-5]    |\n",
      "|887    |2       |1.0       |[0.12392098227002789,0.8740880688383299,0.0015712598008290313,3.763140223642878E-4,4.337506844899329E-5]   |\n",
      "|888    |2       |1.0       |[0.11872058667579824,0.8794556357468446,0.0014265188920852075,3.5460027359830433E-4,4.265841167371363E-5]  |\n",
      "|889    |2       |1.0       |[0.11872049144664779,0.8794557323557374,0.0014265176696016363,3.546001395451489E-4,4.2658388467838765E-5]  |\n",
      "|890    |3       |1.0       |[0.22476160787660915,0.7678060841769141,0.0055049604498927885,0.0018411647094903785,8.618278709360384E-5]  |\n",
      "|891    |2       |1.0       |[0.11872006291632982,0.879456167094885,0.0014265121684373316,3.54599536306314E-4,4.2658284041527736E-5]    |\n",
      "|892    |2       |1.0       |[0.25125041516795926,0.7427289894375808,0.004171875057755581,0.0017585407357122768,9.017960099224148E-5]   |\n",
      "|893    |2       |1.0       |[0.1197267608463516,0.8784241559080188,0.0014427532184925168,3.634014293659902E-4,4.2928597771062514E-5]   |\n",
      "|894    |2       |1.0       |[0.11972652103663003,0.8784243992062257,0.0014427501308873646,3.6340108676722086E-4,4.292853948960101E-5]  |\n",
      "|895    |2       |1.0       |[0.1128154983374687,0.8856229899605189,0.0013274692308818887,1.93228864652485E-4,4.08136064780158E-5]      |\n",
      "|896    |2       |1.0       |[0.1368233409107292,0.8606391900038448,0.002358158581839618,1.308326006380972E-4,4.84779029483299E-5]      |\n",
      "|897    |2       |1.0       |[0.13682323352853354,0.8606392994388324,0.002358156601776162,1.3083255344213253E-4,4.84778774155875E-5]    |\n",
      "|898    |2       |1.0       |[0.1341776944574674,0.8631404028000981,0.002524135235644834,1.126174925017467E-4,4.51500142880654E-5]      |\n",
      "|899    |2       |1.0       |[0.13413133860233015,0.8631877198735152,0.0025232024468147657,1.125995465369285E-4,4.513953080293822E-5]   |\n",
      "|900    |2       |1.0       |[0.13417764165268403,0.8631404566998334,0.002524134173073707,1.1261747206173E-4,4.515000234697445E-5]      |\n",
      "|901    |2       |1.0       |[0.1341775888479445,0.8631405105995243,0.002524133110503462,1.1261745162172362E-4,4.5149990405894427E-5]   |\n",
      "|902    |2       |1.0       |[0.32353931382803935,0.6711650738588254,0.0041868972971857345,9.95408905704303E-4,1.1330611024516695E-4]   |\n",
      "|903    |2       |1.0       |[0.32071013430131445,0.6741000307087684,0.004177671223320319,9.008166006629449E-4,1.1134716593396527E-4]   |\n",
      "|904    |3       |1.0       |[0.24169819158493214,0.7485122984172385,0.009176181261905097,5.217976392373026E-4,9.153109668692445E-5]    |\n",
      "|905    |3       |1.0       |[0.32096136275604165,0.6682588111777669,0.007393595718275182,0.0032547362067255636,1.3149414119091772E-4]  |\n",
      "|906    |2       |1.0       |[0.17354427764987032,0.8230343507329264,0.002883004798002677,4.733761909081768E-4,6.49906282925242E-5]     |\n",
      "|907    |2       |1.0       |[0.2841676033748354,0.7129776859987479,0.002019416385637356,7.320148737280678E-4,1.0327936705128726E-4]    |\n",
      "|908    |2       |1.0       |[0.13215686480060757,0.8657625881803976,0.0019989843813355444,3.857365610233566E-5,4.298898155702874E-5]   |\n",
      "|909    |2       |1.0       |[0.13333505549865257,0.8644712875819379,0.002084131864392133,6.708871139935538E-5,4.243634361793794E-5]    |\n",
      "|910    |2       |1.0       |[0.13899878690485132,0.8586584224845988,0.002231649927607485,6.782153959569144E-5,4.33191433467851E-5]     |\n",
      "|911    |2       |1.0       |[0.13327832984234228,0.8645110671178458,0.0020978248102932755,7.060552551271356E-5,4.217270400590019E-5]   |\n",
      "|912    |2       |1.0       |[0.13267195849570568,0.8650181195039701,0.00223869698019937,2.773438675481493E-5,4.3490633370070684E-5]    |\n",
      "|913    |3       |1.0       |[0.1299847867191168,0.868280584057433,0.0012368473318489948,4.516994163316989E-4,4.608247526938316E-5]     |\n",
      "|914    |2       |1.0       |[0.17875098833122005,0.8177612424072181,0.002702094755076436,7.204407838144766E-4,6.523372267100701E-5]    |\n",
      "|915    |3       |1.0       |[0.1832513505723079,0.8131376972663352,0.002919858275270458,6.258757501534794E-4,6.521813593296103E-5]     |\n",
      "|916    |3       |1.0       |[0.12858447437748965,0.8692943103988061,0.0014795469466412326,5.94443019832562E-4,4.722525723068226E-5]    |\n",
      "|917    |3       |1.0       |[0.11241071640447375,0.8851657158863604,0.0017894638713515797,5.910112721819891E-4,4.3092565632402726E-5]  |\n",
      "|918    |2       |1.0       |[0.30759205147136554,0.684431975608223,0.0026846646377681888,0.005185780719489206,1.0552756315429786E-4]   |\n",
      "|919    |3       |1.0       |[0.25444663014420665,0.7401462621556497,0.003602595207403559,0.0017189847165489016,8.552777619117249E-5]   |\n",
      "|920    |3       |1.0       |[0.12629267008962616,0.8719421937366952,0.0013348560047236459,3.8725048148142613E-4,4.302968747363162E-5]  |\n",
      "|921    |3       |1.0       |[0.2510761214548537,0.7427986552075367,0.00384962602581475,0.0021877704971687935,8.782681462579368E-5]     |\n",
      "|922    |2       |1.0       |[0.12711666018266543,0.8713327911908288,0.001365055514193315,1.4518921389392348E-4,4.030389841833259E-5]   |\n",
      "|923    |2       |1.0       |[0.18600412686084536,0.8105644414091386,0.002926296131957961,4.394351197280735E-4,6.570047832988056E-5]    |\n",
      "|924    |2       |1.0       |[0.14058684430782925,0.8574036203726342,0.001481290729595912,4.843079693866675E-4,4.393662055404787E-5]    |\n",
      "|925    |2       |1.0       |[0.13019641520209418,0.8677761626537591,0.001445435586528847,5.34506169818372E-4,4.7480387799348765E-5]    |\n",
      "|926    |2       |1.0       |[0.1301968274324369,0.8677757446428133,0.0014454404801745317,5.345069554682833E-4,4.7480489107003663E-5]   |\n",
      "|927    |2       |1.0       |[0.11628564221715457,0.8811300595513155,0.002008702979513671,5.329795105013874E-4,4.261574151477589E-5]    |\n",
      "|928    |2       |1.0       |[0.1149916992339432,0.8824859402848716,0.0019995091267966075,4.810780196824092E-4,4.177333470605083E-5]    |\n",
      "|929    |3       |1.0       |[0.2445613184096411,0.7500834913621055,0.003830593568246798,0.0014395624080750173,8.503425193145931E-5]    |\n",
      "|930    |3       |1.0       |[0.13089599745093794,0.8666529797312079,0.00232435974179066,8.232186331714599E-5,4.434121274637555E-5]     |\n",
      "|931    |3       |1.0       |[0.13517318267861436,0.8626453757571657,0.0015917677256333475,5.351832587503357E-4,5.449057983633096E-5]   |\n",
      "|932    |2       |1.0       |[0.12780560188904985,0.8699702082755284,0.0015373580072944476,6.396036797028032E-4,4.722814842443401E-5]   |\n",
      "|933    |3       |1.0       |[0.2227965337363412,0.7693752916689709,0.005686663801630416,0.0020496741661333476,9.183662692403777E-5]    |\n",
      "|934    |2       |1.0       |[0.12248643464027173,0.8758767693025692,0.0012622400451223698,3.3175490352686007E-4,4.280110850978268E-5]  |\n",
      "|935    |2       |1.0       |[0.12303411024301973,0.8750919699194486,0.0015355101821663952,2.94893118519352E-4,4.351653684594687E-5]    |\n",
      "|936    |3       |1.0       |[0.14253988492269154,0.8549460496901286,0.0023294587271116512,1.367686998819525E-4,4.7837960186548045E-5]  |\n",
      "|937    |2       |1.0       |[0.17357769533635536,0.8226052920532532,0.002884271520953616,8.644497646273164E-4,6.829132481074089E-5]    |\n",
      "|938    |3       |1.0       |[0.1315887672277015,0.8665403399691001,0.0013663119584296213,4.50218821541342E-4,5.43620232274907E-5]      |\n",
      "|939    |3       |1.0       |[0.13262615846775888,0.8651672747171804,0.0016010268598538604,5.519819745805324E-4,5.35579806261727E-5]    |\n",
      "|940    |3       |1.0       |[0.1209636770753864,0.8774551239537994,0.0011970849632706453,3.331030505951313E-4,5.101095694849957E-5]    |\n",
      "|941    |2       |1.0       |[0.1141019556304998,0.8844001130657653,0.0011090633263842224,3.4036066881555535E-4,4.8507308535117426E-5]  |\n",
      "|942    |2       |1.0       |[0.11133343879046441,0.8872435326884464,0.0010854248694181026,2.9043848189647424E-4,4.716516977447687E-5]  |\n",
      "|943    |2       |1.0       |[0.11133330365006446,0.8872436694424378,0.0010854234626022862,2.904383141093809E-4,4.716513078624829E-5]   |\n",
      "|944    |3       |1.0       |[0.1191403433207531,0.8793018519238965,0.0012586251281747139,2.495167667421367E-4,4.9662860433531214E-5]   |\n",
      "|945    |3       |1.0       |[0.11625127782331944,0.8818796109599103,0.0013682741452644908,4.563208639402083E-4,4.4516207565603674E-5]  |\n",
      "|946    |2       |1.0       |[0.11424922082094173,0.8840664935360892,0.0012905467163395225,3.51435846191227E-4,4.230308043821486E-5]    |\n",
      "|947    |3       |1.0       |[0.24332110393206494,0.7510371007975026,0.0037982578454842154,0.0017535526943802423,8.998473056786362E-5]  |\n",
      "|948    |2       |1.0       |[0.11429252607606315,0.8840225917907348,0.0012910691652846477,3.514989868356249E-4,4.231398108173647E-5]   |\n",
      "|949    |2       |1.0       |[0.11425069480791657,0.8840649992462523,0.001290564498758198,3.514379955821775E-4,4.230345149086217E-5]    |\n",
      "|950    |2       |1.0       |[0.11853713199065169,0.879801431370628,0.0012921745314014973,3.261812739835159E-4,4.308083333507064E-5]    |\n",
      "|951    |2       |1.0       |[0.11849324165581222,0.8798459006190694,0.0012916633908351455,3.2612432173296255E-4,4.3070012550321804E-5] |\n",
      "|952    |2       |1.0       |[0.12324563689394716,0.8750698005006515,0.001362885690846063,2.779969661074895E-4,4.367994844770388E-5]    |\n",
      "|953    |2       |1.0       |[0.12188061173383678,0.8764689118207041,0.0013567179546579052,2.5093965627093743E-4,4.281883453050622E-5]  |\n",
      "|954    |2       |1.0       |[0.1323147967335704,0.8639713383593449,0.003527893349954689,1.3994074721066524E-4,4.603080991935434E-5]    |\n",
      "|955    |2       |1.0       |[0.33633759919187656,0.6584781408332255,0.0043165264853456805,7.572647801283115E-4,1.1046870942363674E-4]  |\n",
      "|956    |2       |1.0       |[0.1256494103320191,0.8717924085829858,0.002452879140430425,6.133530940989778E-5,4.396663515481048E-5]     |\n",
      "|957    |2       |1.0       |[0.12936611391957928,0.8679704064648771,0.0025597308820756487,5.944240118098332E-5,4.430633228711296E-5]   |\n",
      "|958    |3       |1.0       |[0.1834588290277881,0.8132970699527414,0.0026494209451493746,5.254705056778204E-4,6.92095686432578E-5]     |\n",
      "|959    |2       |1.0       |[0.17844807136828564,0.8184717071313902,0.00258745108893546,4.256313543323683E-4,6.713905705630599E-5]     |\n",
      "|960    |2       |1.0       |[0.17844740532496128,0.8184723843917143,0.002587440721983344,4.2563066822123136E-4,6.713889311971572E-5]   |\n",
      "|961    |2       |1.0       |[0.1621565641274042,0.8347500804784755,0.002696373044934423,3.3264965707016186E-4,6.433269211556315E-5]    |\n",
      "|962    |2       |1.0       |[0.16614381829947789,0.8306415532706705,0.0028368626035780334,3.133124726850564E-4,6.445335358866474E-5]   |\n",
      "|963    |3       |1.0       |[0.2769221213598046,0.7130159556798681,0.00935949557032823,6.098219900911331E-4,9.260539990794203E-5]      |\n",
      "|964    |3       |1.0       |[0.12529768182553397,0.8727322395817821,0.0014636504848944444,4.5827553036171875E-4,4.815257742766376E-5]  |\n",
      "|965    |3       |1.0       |[0.11893069608236208,0.8795448580042257,0.0011986454782691215,2.7841008955479184E-4,4.7390345588284435E-5] |\n",
      "|966    |3       |1.0       |[0.11893079148757625,0.8795447614406818,0.0011986465055481236,2.78410194822489E-4,4.739037137128292E-5]    |\n",
      "|967    |2       |1.0       |[0.11892439948476552,0.879551231049804,0.001198577679575758,2.784031419220255E-4,4.738864393243591E-5]     |\n",
      "|968    |2       |1.0       |[0.10870826353669961,0.8900142873483432,0.0010460605929029168,1.874169852966789E-4,4.3971536757639823E-5]  |\n",
      "|969    |3       |1.0       |[0.11558239186249171,0.8829708693482481,0.0012188836706743607,1.8253490493730327E-4,4.532021364855049E-5]  |\n",
      "|970    |2       |1.0       |[0.11434103788841367,0.8842616396267928,0.00117588255482365,1.7618451469002876E-4,4.5255415279861734E-5]   |\n",
      "|971    |2       |1.0       |[0.11434214441742808,0.8842605198382204,0.0011758947082148015,1.7618532308571542E-4,4.525571305107707E-5]  |\n",
      "|972    |2       |1.0       |[0.11026918831616261,0.8883121757347716,0.0011706225973105433,2.0410614912847795E-4,4.390720262691527E-5]  |\n",
      "|973    |3       |1.0       |[0.2255220939228991,0.7701634708842691,0.003188924460896885,0.001034417054416919,9.109367751816256E-5]     |\n",
      "|974    |2       |1.0       |[0.10450101199037194,0.8941828811897548,0.0010693152606825105,2.045361838517959E-4,4.225537533908669E-5]   |\n",
      "|975    |2       |1.0       |[0.123637363637833,0.8743834493085725,0.001573029308010681,3.6082535673185434E-4,4.5332388852001446E-5]    |\n",
      "|976    |2       |1.0       |[0.16562609408221443,0.8314901126189803,0.0025019405515950013,3.1890499382853846E-4,6.294775338157187E-5]  |\n",
      "|977    |3       |1.0       |[0.14059616434221264,0.8570827521393768,0.0017164614876634726,5.528460045288228E-4,5.177602621811633E-5]   |\n",
      "|978    |3       |1.0       |[0.24388117727839237,0.7487069504713497,0.005358190045989148,0.001946274237176485,1.0740796709217236E-4]   |\n",
      "|979    |3       |1.0       |[0.15722197635087876,0.8394678143221307,0.003062977018536195,1.8246247593906785E-4,6.476983251528413E-5]   |\n",
      "|980    |2       |1.0       |[0.13521870352929383,0.8627515445204077,0.0015379602454949181,4.416232847478732E-4,5.016842005566527E-5]   |\n",
      "|981    |2       |1.0       |[0.11588088723220981,0.8825510572745465,0.0014887848575166696,3.390608895847095E-5,4.5364546768665596E-5]  |\n",
      "|982    |3       |1.0       |[0.2408931114655852,0.7532759064975879,0.0049412425287499355,7.980669031858926E-4,9.167260489112966E-5]    |\n",
      "|983    |2       |1.0       |[0.12794297386139117,0.8698819198527457,0.0020436946176028725,8.237534812760001E-5,4.903632013272357E-5]   |\n",
      "|984    |3       |1.0       |[0.3205299663326295,0.6731058479656616,0.0029184604655253807,0.003333676616876284,1.1204861930727224E-4]   |\n",
      "|985    |3       |1.0       |[0.13315340149193614,0.8648793657244733,0.0015449148318005365,3.7556823552411394E-4,4.6749716265949664E-5] |\n",
      "|986    |3       |1.0       |[0.1331535590468626,0.8648792059717264,0.0015449167869227736,3.7556844100711907E-4,4.674975348129516E-5]   |\n",
      "|987    |2       |1.0       |[0.13169660600569857,0.8663803659074062,0.001538133138037338,3.3906061222595566E-4,4.583433663198889E-5]   |\n",
      "|988    |3       |1.0       |[0.12477235756854821,0.8737274369197024,0.0013403521916164867,1.161501123494337E-4,4.370320778351509E-5]   |\n",
      "|989    |2       |1.0       |[0.12776183985701833,0.8705004391187484,0.0014819853275673455,2.1000601236330802E-4,4.572968430247602E-5]  |\n",
      "|990    |2       |1.0       |[0.12141612100670139,0.8769860157281114,0.0014653041208041793,9.179643952518574E-5,4.076270485789103E-5]   |\n",
      "|991    |3       |1.0       |[0.2528997218964573,0.7415418938728223,0.004290032492832553,0.0011805611017930136,8.779063609483308E-5]    |\n",
      "|992    |2       |1.0       |[0.1098845799032536,0.8887695161722208,0.0011931196827755842,1.122698167040354E-4,4.0514425045974056E-5]   |\n",
      "|993    |2       |1.0       |[0.12164614258645742,0.8766659777037075,0.00146606164830465,1.8225417230104718E-4,3.956388922957066E-5]    |\n",
      "|994    |2       |1.0       |[0.11533959039445073,0.8834186607530896,0.0011442432655057524,5.628103943580092E-5,4.1224547518326776E-5]  |\n",
      "|995    |2       |1.0       |[0.15370526751762986,0.8436020002647803,0.0025528768884925134,9.067714106063573E-5,4.917818803661254E-5]   |\n",
      "|996    |2       |1.0       |[0.15375544176292189,0.843363719808442,0.0027595892138127793,7.497742332128444E-5,4.627179150194946E-5]    |\n",
      "|997    |3       |1.0       |[0.27735552880784414,0.7149917547399055,0.007209328445288447,3.482160632713242E-4,9.517194369069525E-5]    |\n",
      "|998    |3       |1.0       |[0.2951503399656871,0.697231572721317,0.0072902781968308735,2.3691011486751218E-4,9.08990012974461E-5]     |\n",
      "|999    |2       |1.0       |[0.14759680565858563,0.8501665394436007,0.0017825354810060492,4.018692146203678E-4,5.2250202187129035E-5]  |\n",
      "+-------+--------+----------+-----------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %% [python]\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, unix_timestamp, when,\n",
    "    hour, dayofweek, month, year,\n",
    "    monotonically_increasing_id\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import StringIndexerModel, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from pyspark.ml import Pipeline\n",
    "import os\n",
    "\n",
    "# 1) Spark başlat\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LR Batch Predict 100 Rows (full)\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\",\"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2) df_no_na’yı baştan oluştur\n",
    "df = spark.read.csv(\"US_Accidents_March23.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 2.1) İstenmeyen sütunları düş\n",
    "drop_cols = [\n",
    "    \"ID\",\"Source\",\"Zipcode\",\"Timezone\",\"Airport_Code\",\"Amenity\",\"Bump\",\n",
    "    \"Give_Way\",\"No_Exit\",\"Railway\",\"Description\",\"County\",\"Roundabout\",\n",
    "    \"Station\",\"Stop\",\"Nautical_Twilight\",\"Astronomical_Twilight\",\"Country\"\n",
    "]\n",
    "df2 = df.drop(*drop_cols)\n",
    "\n",
    "# 2.2) Süre & tarih/saat özellikleri\n",
    "df3 = (\n",
    "    df2\n",
    "    .withColumn(\"Start_TS\", to_timestamp(col(\"Start_Time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .withColumn(\"End_TS\",   to_timestamp(col(\"End_Time\"),   \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .withColumn(\"Duration\",\n",
    "        ((unix_timestamp(col(\"End_TS\")) - unix_timestamp(col(\"Start_TS\"))) / 60)\n",
    "        .cast(DoubleType())\n",
    "    )\n",
    "    .withColumn(\"HourOfDay\", hour(col(\"Start_TS\")))\n",
    "    .withColumn(\"DayOfWeek\", dayofweek(col(\"Start_TS\")))\n",
    "    .withColumn(\"Month\",    month(col(\"Start_TS\")))\n",
    "    .withColumn(\"Year\",     year(col(\"Start_TS\")))\n",
    "    .drop(\"Start_TS\",\"End_TS\",\"Start_Time\",\"End_Time\")\n",
    ")\n",
    "\n",
    "# 2.3) City/Street cardinality düşürme\n",
    "def clean_column(df, column_name, top_n=32):\n",
    "    top_vals = [\n",
    "        r[column_name] for r in\n",
    "        df.groupBy(column_name).count()\n",
    "          .orderBy(col(\"count\").desc())\n",
    "          .limit(top_n)\n",
    "          .collect()\n",
    "    ]\n",
    "    return df.withColumn(\n",
    "        f\"{column_name}_Cleaned\",\n",
    "        when(col(column_name).isin(top_vals), col(column_name)).otherwise(\"Other\")\n",
    "    )\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df3 = df3.withColumn(\"City_Cleaned\", col(\"City\")) \\\n",
    "         .withColumn(\"Street_Cleaned\", col(\"Street\"))\n",
    "\n",
    "# 2.4) İlgili sütunları seç ve dropna\n",
    "selected_cols = [\n",
    "    \"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\", \"Precipitation(in)\", \"Wind_Chill(F)\", \"Traffic_Signal\",\n",
    "    \"Weather_Condition\",\"Wind_Direction\",\"Civil_Twilight\",\"Sunrise_Sunset\",\n",
    "    \"State\",\"City_Cleaned\",\"Street_Cleaned\",\"Junction\",\"Duration\",\"Severity\",\n",
    "    \"HourOfDay\",\"DayOfWeek\",\"Month\",\"Year\"\n",
    "]\n",
    "df_no_na = df3.select(*selected_cols).dropna().cache()\n",
    "\n",
    "# 3) 100 satırlık örnek + row_id\n",
    "sample100 = df_no_na.limit(1000) \\\n",
    "                   .withColumn(\"_row_id\", monotonically_increasing_id())\n",
    "\n",
    "# 4) Küçük‐model’den indexer aşamaları (00…07) yükle\n",
    "base = \"models/us_accidents_lr_optimized_small_spark/stages\"\n",
    "indexer_paths = [os.path.join(base, d) for d in sorted(os.listdir(base)) if d.startswith(\"0\") and \"StringIndexer\" in d]\n",
    "prep_indexers = [StringIndexerModel.load(p) for p in indexer_paths]\n",
    "\n",
    "# 5) VectorAssembler’ı yeniden tanımla\n",
    "feature_cols = [\n",
    "    \"Temperature(F)\",\"Humidity(%)\",\"Pressure(in)\",\"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\",\"Precipitation(in)\",\"Wind_Chill(F)\",\"Traffic_Signal\",\n",
    "    \"Junction\",\"Duration\",\n",
    "    \"HourOfDay\",\"DayOfWeek\",\"Month\",\"Year\"\n",
    "] + [c + \"_Idx\" for c in [\n",
    "    \"Weather_Condition\",\"Wind_Direction\",\"Civil_Twilight\",\n",
    "    \"Sunrise_Sunset\",\"State\",\"City_Cleaned\",\"Street_Cleaned\"\n",
    "]]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "\n",
    "# 6) Son aşama olarak final LR modelini yükle\n",
    "lr_stage = \"models/us_accidents_lr_final_full_spark/stages/09_LogisticRegression_ea33fdabd04f\"\n",
    "lr_final = LogisticRegressionModel.load(lr_stage)\n",
    "\n",
    "# 7) Pipeline’ı oluştur ve tahmin yap\n",
    "pipe_full = Pipeline(stages = prep_indexers + [assembler, lr_final])\n",
    "predictions = pipe_full.fit(df_no_na).transform(sample100) \\\n",
    "                       .select(\"_row_id\",\"Severity\",\"prediction\",\"probability\")\n",
    "\n",
    "# 8) Sonuçları göster\n",
    "predictions.show(1000, truncate=False)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "457069e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, unix_timestamp, when,\n",
    "    hour, dayofweek, month, year,\n",
    "    monotonically_increasing_id\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import StringIndexerModel, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from pyspark.ml import Pipeline\n",
    "import os\n",
    "\n",
    "# 1) Spark başlat\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LR Batch Predict 100 Rows (full)\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\",\"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2) df_no_na’yı baştan oluştur\n",
    "df = spark.read.csv(\"US_Accidents_March23.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "913384e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID='A-1', Source='Source2', Severity=3, Start_Time=datetime.datetime(2016, 2, 8, 5, 46), End_Time=datetime.datetime(2016, 2, 8, 11, 0), Start_Lat=39.865147, Start_Lng=-84.058723, End_Lat=None, End_Lng=None, Distance(mi)=0.01, Description='Right lane blocked due to accident on I-70 Eastbound at Exit 41 OH-235 State Route 4.', Street='I-70 E', City='Dayton', County='Montgomery', State='OH', Zipcode='45424', Country='US', Timezone='US/Eastern', Airport_Code='KFFO', Weather_Timestamp=datetime.datetime(2016, 2, 8, 5, 58), Temperature(F)=36.9, Wind_Chill(F)=None, Humidity(%)=91.0, Pressure(in)=29.68, Visibility(mi)=10.0, Wind_Direction='Calm', Wind_Speed(mph)=None, Precipitation(in)=0.02, Weather_Condition='Light Rain', Amenity=False, Bump=False, Crossing=False, Give_Way=False, Junction=False, No_Exit=False, Railway=False, Roundabout=False, Station=False, Stop=False, Traffic_Calming=False, Traffic_Signal=False, Turning_Loop=False, Sunrise_Sunset='Night', Civil_Twilight='Night', Nautical_Twilight='Night', Astronomical_Twilight='Night')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4313f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dfa2 = pd.read_csv(\"US_Accidents_March23.csv\")\n",
    "df_read=dfa2.head(1000)\n",
    "df_read\n",
    "df_read.to_csv(\"a.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9e87d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Source', 'Severity', 'Start_Time', 'End_Time', 'Start_Lat',\n",
       "       'Start_Lng', 'End_Lat', 'End_Lng', 'Distance(mi)', 'Description',\n",
       "       'Street', 'City', 'County', 'State', 'Zipcode', 'Country', 'Timezone',\n",
       "       'Airport_Code', 'Weather_Timestamp', 'Temperature(F)', 'Wind_Chill(F)',\n",
       "       'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Direction',\n",
       "       'Wind_Speed(mph)', 'Precipitation(in)', 'Weather_Condition', 'Amenity',\n",
       "       'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway',\n",
       "       'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal',\n",
       "       'Turning_Loop', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight',\n",
       "       'Astronomical_Twilight'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfa2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e36fb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [\n",
    "    {\n",
    "        \"ID\": \"A-1\",\n",
    "        \"Source\": \"Source2\",\n",
    "        \"Severity\": 3,\n",
    "        \"Start_Time\": \"2016-02-08 05:46:00\",\n",
    "        \"End_Time\": \"2016-02-08 11:00:00\",\n",
    "        \"Start_Lat\": 39.865147,\n",
    "        \"Start_Lng\": -84.058723,\n",
    "        \"End_Lat\": 39.865147,\n",
    "        \"End_Lng\": -84.058723,\n",
    "        \"Distance(mi)\": 0.01,\n",
    "        \"Description\": \"Right lane blocked due to accident on I-70 Eastbound at Exit 41 OH-235 State Route 4.\",\n",
    "        \"Street\": \"I-70 E\",\n",
    "        \"City\": \"Dayton\",\n",
    "        \"County\": \"Montgomery\",\n",
    "        \"State\": \"OH\",\n",
    "        \"Zipcode\": \"45424\",\n",
    "        \"Country\": \"US\",\n",
    "        \"Timezone\": \"US/Eastern\",\n",
    "        \"Airport_Code\": \"KFFO\",\n",
    "        \"Weather_Timestamp\": \"2016-02-08 05:58:00\",\n",
    "        \"Temperature(F)\": 36.9,\n",
    "        \"Wind_Chill(F)\": 36.9,\n",
    "        \"Humidity(%)\": 91.0,\n",
    "        \"Pressure(in)\": 29.68,\n",
    "        \"Visibility(mi)\": 10.0,\n",
    "        \"Wind_Direction\": \"Calm\",\n",
    "        \"Wind_Speed(mph)\": 0.0,\n",
    "        \"Precipitation(in)\": 0.02,\n",
    "        \"Weather_Condition\": \"Light Rain\",\n",
    "        \"Amenity\": False,\n",
    "        \"Bump\": False,\n",
    "        \"Crossing\": False,\n",
    "        \"Give_Way\": False,\n",
    "        \"Junction\": False,\n",
    "        \"No_Exit\": False,\n",
    "        \"Railway\": False,\n",
    "        \"Roundabout\": False,\n",
    "        \"Station\": False,\n",
    "        \"Stop\": False,\n",
    "        \"Traffic_Calming\": False,\n",
    "        \"Traffic_Signal\": False,\n",
    "        \"Turning_Loop\": False,\n",
    "        \"Sunrise_Sunset\": \"Night\",\n",
    "        \"Civil_Twilight\": \"Night\",\n",
    "        \"Nautical_Twilight\": \"Night\",\n",
    "        \"Astronomical_Twilight\": \"Night\"\n",
    "    }\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"A-1.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f825616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+---------------------+\n",
      "|_row_id|Severity|prediction|probability          |\n",
      "+-------+--------+----------+---------------------+\n",
      "|0      |3       |0.0       |[1.0,0.0,0.0,0.0,0.0]|\n",
      "+-------+--------+----------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, unix_timestamp, when,\n",
    "    hour, dayofweek, month, year,\n",
    "    monotonically_increasing_id\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import StringIndexerModel, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from pyspark.ml import Pipeline\n",
    "import os\n",
    "\n",
    "# 1) Spark başlat\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LR Batch Predict 100 Rows (full)\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\",\"8g\") \\\n",
    "    .getOrCreate()\n",
    "rows = [{\n",
    "    \"Source\": \"Source2\",\n",
    "    \"Severity\": 3,\n",
    "    \"Start_Time\": \"2016-02-08 05:46:00\",\n",
    "    \"End_Time\":   \"2016-02-08 11:00:00\",\n",
    "    \"Start_Lat\": 39.865147,\n",
    "    \"Start_Lng\": -84.058723,\n",
    "    \"Distance(mi)\": 0.01,\n",
    "    \"Description\": (\"Right lane blocked due to accident on I-70 Eastbound \"\n",
    "                    \"at Exit 41 OH-235 State Route 4.\"),\n",
    "    \"Street\": \"I-70 E\",\n",
    "    \"City\": \"Dayton\",\n",
    "    \"County\": \"Montgomery\",\n",
    "    \"State\": \"OH\",\n",
    "    \"Zipcode\": \"45424\",\n",
    "    \"Country\": \"US\",\n",
    "    \"Timezone\": \"US/Eastern\",\n",
    "    \"Airport_Code\": \"KFFO\",\n",
    "    \"Weather_Timestamp\": \"2016-02-08 05:58:00\",\n",
    "    \"Temperature(F)\": 36.9,\n",
    "    \"Humidity(%)\": 91.0,\n",
    "    \"Pressure(in)\": 29.68,\n",
    "    \"Visibility(mi)\": 10.0,\n",
    "    \"Wind_Direction\": \"Calm\",\n",
    "    \"Precipitation(in)\": 0.02,\n",
    "    \"Weather_Condition\": \"Light Rain\",\n",
    "    # all your boolean columns:\n",
    "    \"Amenity\": False,\n",
    "    \"Bump\": False,\n",
    "    \"Crossing\": False,\n",
    "    \"Give_Way\": False,\n",
    "    \"Junction\": False,\n",
    "    \"No_Exit\": False,\n",
    "    \"Railway\": False,\n",
    "    \"Roadway\": False,\n",
    "    \"Station\": False,\n",
    "    \"Stop\": False,\n",
    "    \"Traffic_Signal\": False,\n",
    "    \"Turning_Loop\": False,\n",
    "    # twilight columns:\n",
    "    \"Sunrise_Sunset\": \"Night\",\n",
    "    \"Civil_Twilight\": \"Night\",\n",
    "    \"Nautical_Twilight\": \"Night\",\n",
    "    \"Astronomical_Twilight\": \"Night\",\n",
    "}]\n",
    "\n",
    "\n",
    "# 2) df_no_na’yı baştan oluştur\n",
    "df = spark.read.csv(\"A-1.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 2.1) İstenmeyen sütunları düş\n",
    "drop_cols = [\n",
    "    \"ID\",\"Source\",\"Zipcode\",\"Timezone\",\"Airport_Code\",\"Amenity\",\"Bump\",\n",
    "    \"Give_Way\",\"No_Exit\",\"Railway\",\"Description\",\"County\",\"Roundabout\",\n",
    "    \"Station\",\"Stop\",\"Nautical_Twilight\",\"Astronomical_Twilight\",\"Country\"\n",
    "]\n",
    "df2 = df.drop(*drop_cols)\n",
    "\n",
    "# 2.2) Süre & tarih/saat özellikleri\n",
    "df3 = (\n",
    "    df2\n",
    "    .withColumn(\"Start_TS\", to_timestamp(col(\"Start_Time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .withColumn(\"End_TS\",   to_timestamp(col(\"End_Time\"),   \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .withColumn(\"Duration\",\n",
    "        ((unix_timestamp(col(\"End_TS\")) - unix_timestamp(col(\"Start_TS\"))) / 60)\n",
    "        .cast(DoubleType())\n",
    "    )\n",
    "    .withColumn(\"HourOfDay\", hour(col(\"Start_TS\")))\n",
    "    .withColumn(\"DayOfWeek\", dayofweek(col(\"Start_TS\")))\n",
    "    .withColumn(\"Month\",    month(col(\"Start_TS\")))\n",
    "    .withColumn(\"Year\",     year(col(\"Start_TS\")))\n",
    "    .drop(\"Start_TS\",\"End_TS\",\"Start_Time\",\"End_Time\")\n",
    ")\n",
    "\n",
    "# 2.3) City/Street cardinality düşürme\n",
    "def clean_column(df, column_name, top_n=32):\n",
    "    top_vals = [\n",
    "        r[column_name] for r in\n",
    "        df.groupBy(column_name).count()\n",
    "          .orderBy(col(\"count\").desc())\n",
    "          .limit(top_n)\n",
    "          .collect()\n",
    "    ]\n",
    "    return df.withColumn(\n",
    "        f\"{column_name}_Cleaned\",\n",
    "        when(col(column_name).isin(top_vals), col(column_name)).otherwise(\"Other\")\n",
    "    )\n",
    "\n",
    "for c in [\"City\",\"Street\"]:\n",
    "    df3 = clean_column(df3, c, top_n=32)\n",
    "\n",
    "# 2.4) İlgili sütunları seç ve dropna\n",
    "selected_cols = [\n",
    "    \"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\", \"Precipitation(in)\", \"Wind_Chill(F)\", \"Traffic_Signal\",\n",
    "    \"Weather_Condition\",\"Wind_Direction\",\"Civil_Twilight\",\"Sunrise_Sunset\",\n",
    "    \"State\",\"City_Cleaned\",\"Street_Cleaned\",\"Junction\",\"Duration\",\"Severity\",\n",
    "    \"HourOfDay\",\"DayOfWeek\",\"Month\",\"Year\"\n",
    "]\n",
    "df_no_na = df3.select(*selected_cols).dropna().cache()\n",
    "\n",
    "# 3) 100 satırlık örnek + row_id\n",
    "sample100 = df_no_na.limit(1000) \\\n",
    "                   .withColumn(\"_row_id\", monotonically_increasing_id())\n",
    "\n",
    "# 4) Küçük‐model’den indexer aşamaları (00…07) yükle\n",
    "base = \"models/us_accidents_dt_final_last_full_spark/stages\"\n",
    "indexer_paths = [os.path.join(base, d) for d in sorted(os.listdir(base)) if d.startswith(\"0\") and \"StringIndexer\" in d]\n",
    "prep_indexers = [StringIndexerModel.load(p) for p in indexer_paths]\n",
    "\n",
    "# 5) VectorAssembler’ı yeniden tanımla\n",
    "feature_cols = [\n",
    "    \"Temperature(F)\",\"Humidity(%)\",\"Pressure(in)\",\"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\",\"Precipitation(in)\",\"Wind_Chill(F)\",\"Traffic_Signal\",\n",
    "    \"Junction\",\"Duration\",\n",
    "    \"HourOfDay\",\"DayOfWeek\",\"Month\",\"Year\"\n",
    "] + [c + \"_Idx\" for c in [\n",
    "    \"Weather_Condition\",\"Wind_Direction\",\"Civil_Twilight\",\n",
    "    \"Sunrise_Sunset\",\"State\",\"City_Cleaned\",\"Street_Cleaned\"\n",
    "]]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "from pyspark.ml.classification import DecisionTreeClassificationModel\n",
    "# 6) Son aşama olarak final LR modelini yükle\n",
    "lr_stage = \"models/us_accidents_dt_final_last_full_spark/stages/09_DecisionTreeClassifier_03f6571d1b4a\"\n",
    "dt_final = DecisionTreeClassificationModel.load(lr_stage)\n",
    "\n",
    "# 7) Pipeline’ı oluştur ve tahmin yap\n",
    "pipe_full = Pipeline(stages = prep_indexers + [assembler, dt_final])\n",
    "predictions = pipe_full.fit(df_no_na).transform(sample100) \\\n",
    "                       .select(\"_row_id\",\"Severity\",\"prediction\",\"probability\")\n",
    "\n",
    "# 8) Sonuçları göster\n",
    "predictions.show(1000, truncate=False)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0a63ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+----------------------------------------------------------------------------------------------------------+\n",
      "|_row_id|Severity|prediction|probability                                                                                               |\n",
      "+-------+--------+----------+----------------------------------------------------------------------------------------------------------+\n",
      "|0      |3       |1.0       |[0.12174241227344809,0.872311702384795,0.005232305851078391,6.672239728104335E-4,4.6355517867870195E-5]   |\n",
      "|1      |3       |1.0       |[0.12438972230370629,0.8698682314319333,0.0051463468667118754,5.4772628593413E-4,4.797311171448235E-5]    |\n",
      "|2      |3       |1.0       |[0.060468464507702054,0.9374907376514571,0.001890167188593261,1.272876968319723E-4,2.334295541596523E-5]  |\n",
      "|3      |2       |1.0       |[0.13762110078704323,0.858517424614645,0.0029161651448025654,8.909143381787794E-4,5.439511533046569E-5]   |\n",
      "|4      |2       |1.0       |[0.12828126343556057,0.8662073558965239,0.0051060920524676,3.5639981478343086E-4,4.888880066451048E-5]    |\n",
      "|5      |2       |1.0       |[0.13188858880474427,0.8648482219646014,0.0028806576466569216,3.332731108144601E-4,4.9258473182885705E-5] |\n",
      "|6      |3       |1.0       |[0.04551555653974827,0.9530127556253072,0.0014163755549778988,3.813533908002647E-5,1.7176940886685196E-5] |\n",
      "|7      |2       |1.0       |[0.1745521548552368,0.8020375770665434,0.02309260153665079,2.5033460196274176E-4,6.733193960621604E-5]    |\n",
      "|8      |2       |1.0       |[0.2946405796733862,0.6944315339785962,0.008498655918589216,0.0023185761390118664,1.1065429041640267E-4]  |\n",
      "|9      |2       |1.0       |[0.2794160762975317,0.7097471656605846,0.008617820695336169,0.0021239437458630875,9.499360068444637E-5]   |\n",
      "|10     |2       |1.0       |[0.273245163353781,0.716139532606535,0.009107058000844496,0.0014089889523156518,9.925708652369221E-5]     |\n",
      "|11     |2       |1.0       |[0.10955278365266817,0.8850585553133482,0.005149476498552041,2.0199293673245532E-4,3.7191598699049494E-5] |\n",
      "|12     |2       |1.0       |[0.13088759702104016,0.8605130204337855,0.008492349023779738,6.452929894586408E-5,4.250422244864871E-5]   |\n",
      "|13     |2       |1.0       |[0.3038349106826834,0.6800599224647101,0.015584193766157078,4.2040369894564276E-4,1.0056938750362128E-4]  |\n",
      "|14     |2       |1.0       |[0.17238733635026096,0.8185732740364752,0.008840097820008457,1.3748824424860047E-4,6.180354900700379E-5]  |\n",
      "|15     |3       |1.0       |[0.17823412835979732,0.8130790024269132,0.008432170044789921,1.8724105371022433E-4,6.745811478914637E-5]  |\n",
      "|16     |2       |1.0       |[0.2582250008240389,0.7016580949730696,0.039945170394952524,8.032513504153237E-5,9.140867289733315E-5]    |\n",
      "|17     |2       |1.0       |[0.39403133253999134,0.5902196415397936,0.014442547722618592,0.0011627891643666104,1.436890332298017E-4]  |\n",
      "|18     |2       |1.0       |[0.2501052875792786,0.7112926542079195,0.038446117072615624,6.593227020664207E-5,9.000886997971737E-5]    |\n",
      "|19     |3       |1.0       |[0.06252510099678321,0.9342080070632662,0.0032182318079670745,2.4417799659856805E-5,2.424233232372917E-5] |\n",
      "|20     |3       |1.0       |[0.38708879369681193,0.5986096180924705,0.013233279502545875,9.203026780068911E-4,1.4800603016486874E-4]  |\n",
      "|21     |3       |1.0       |[0.17367815168388154,0.8174609232381849,0.008650095508909324,1.4787268260682313E-4,6.295688641743451E-5]  |\n",
      "|22     |2       |1.0       |[0.3879550298780174,0.5974059666073371,0.01334193383611143,0.0011487224922041173,1.4834718632988333E-4]   |\n",
      "|23     |3       |1.0       |[0.1773107458391069,0.8136461887439329,0.008828582604402285,1.50835991099031E-4,6.36468214589576E-5]      |\n",
      "|24     |2       |1.0       |[0.07061092734134077,0.9269048562004436,0.0024401485912930853,1.829636094813384E-5,2.5771505974358173E-5] |\n",
      "|25     |2       |1.0       |[0.1715568836718813,0.8198051541127457,0.008461996078356043,1.1342709953665082E-4,6.253903748030748E-5]   |\n",
      "|26     |3       |1.0       |[0.07061092734134077,0.9269048562004436,0.0024401485912930853,1.829636094813384E-5,2.5771505974358173E-5] |\n",
      "|27     |3       |1.0       |[0.07524390923037948,0.9221606107861349,0.0025521581299209535,1.6867462259984366E-5,2.6454391304752497E-5]|\n",
      "|28     |2       |1.0       |[0.3385401411725748,0.6515448706143542,0.008588716859740282,0.0011997654620044633,1.265058913261613E-4]   |\n",
      "|29     |2       |1.0       |[0.14810363128692075,0.8464283467241325,0.005226975165874246,1.881008730543988E-4,5.2945950018267107E-5]  |\n",
      "|30     |2       |1.0       |[0.05979051353970178,0.9386799092021566,0.0014784161578781162,2.976058844146505E-5,2.1400511822098574E-5] |\n",
      "|31     |2       |1.0       |[0.05979051353970178,0.9386799092021566,0.0014784161578781162,2.976058844146505E-5,2.1400511822098574E-5] |\n",
      "|32     |3       |1.0       |[0.06046136668598997,0.9380572613390318,0.0014235903151569062,3.537330271167503E-5,2.2408357109562136E-5] |\n",
      "|33     |2       |1.0       |[0.1457007569965502,0.8491919925373729,0.004890718548769048,1.6212760977166464E-4,5.4404307536186075E-5]  |\n",
      "|34     |2       |1.0       |[0.22137193506643654,0.754521681653134,0.023934736101551023,9.515812819762107E-5,7.648905068097327E-5]    |\n",
      "|35     |2       |1.0       |[0.22137193506643654,0.754521681653134,0.023934736101551023,9.515812819762107E-5,7.648905068097327E-5]    |\n",
      "|36     |2       |1.0       |[0.21404095127973313,0.7621166130400218,0.023673070469164774,9.457473828630972E-5,7.479047279402211E-5]   |\n",
      "|37     |3       |1.0       |[0.10298522661025042,0.8939659443291214,0.002949097539092,6.402456443985241E-5,3.5706957096432896E-5]     |\n",
      "|38     |3       |1.0       |[0.22137193506643654,0.754521681653134,0.023934736101551023,9.515812819762107E-5,7.648905068097327E-5]    |\n",
      "|39     |2       |1.0       |[0.20861519720759228,0.7792236144749434,0.011971012358196951,1.1493126625536188E-4,7.52446930121116E-5]   |\n",
      "|40     |3       |1.0       |[0.19119840195792076,0.7999899060654025,0.008679433189842128,6.91895671142824E-5,6.30692197202629E-5]     |\n",
      "|41     |2       |1.0       |[0.41680613289790897,0.5686578283888786,0.013828238873325674,5.669006725248624E-4,1.408991673619984E-4]   |\n",
      "|42     |2       |1.0       |[0.17710156523571305,0.8140812815436933,0.008690131545592074,6.826745657004497E-5,5.875421843174308E-5]   |\n",
      "|43     |3       |1.0       |[0.06735896765026778,0.9311048051590026,0.001496182463801036,1.7698827439722675E-5,2.2345899488713172E-5] |\n",
      "|44     |2       |1.0       |[0.15435903419299832,0.840586854829707,0.004914144559729693,8.76178232575328E-5,5.234859430745086E-5]     |\n",
      "|45     |2       |1.0       |[0.16214665705188666,0.8325889623027533,0.005110524521659094,9.983989447287387E-5,5.401622922809954E-5]   |\n",
      "|46     |3       |1.0       |[0.15278596912707784,0.8424367111200194,0.004652748664204157,7.26583151276036E-5,5.191277357116034E-5]    |\n",
      "|47     |3       |1.0       |[0.1224872866015522,0.8709871021262441,0.006434063772106962,5.0797964892385685E-5,4.074953520446097E-5]   |\n",
      "|48     |3       |1.0       |[0.13234014103383754,0.8627695869262397,0.004816321635364588,3.3567343708912106E-5,4.038306084927385E-5]  |\n",
      "|49     |3       |1.0       |[0.09854758034095072,0.889335373568082,0.012086000041253736,1.7418868695594893E-6,2.9304162843933968E-5]  |\n",
      "|50     |3       |1.0       |[0.1951729651612291,0.7960009774968884,0.008708861491683458,5.813885880202702E-5,5.905699139720657E-5]    |\n",
      "|51     |3       |1.0       |[0.19525222073885076,0.7958301827870314,0.008784858753488405,6.92020532448465E-5,6.353566738453421E-5]    |\n",
      "|52     |3       |1.0       |[0.16894962531466887,0.8259557531531815,0.004961290304563145,7.583551301592958E-5,5.749571457061523E-5]   |\n",
      "|53     |3       |1.0       |[0.06868862938350115,0.9288445438875133,0.0024438431682220177,4.274448436823605E-6,1.8709112326916418E-5] |\n",
      "|54     |2       |1.0       |[0.05107185842060883,0.9477401521439219,0.0011634064164304134,8.220528470157157E-6,1.636249056857881E-5]  |\n",
      "|55     |3       |1.0       |[0.04613334047215184,0.9522786382963122,0.00156353431989928,8.199983774274702E-6,1.6286927862146592E-5]   |\n",
      "|56     |3       |1.0       |[0.1273028908044936,0.8636610951510914,0.008983764248456611,1.2041601158444748E-5,4.0208194799961865E-5]  |\n",
      "|57     |3       |1.0       |[0.1272788427364925,0.8636869646660482,0.008981948955667847,1.2040551887473675E-5,4.020308990413041E-5]   |\n",
      "|58     |3       |1.0       |[0.07367588410353117,0.9234342487181274,0.002867288428345881,2.6502188321831118E-6,1.9928531163313406E-5] |\n",
      "|59     |3       |1.0       |[0.04858374021428666,0.949618822349341,0.00177840343562189,2.9925777944983333E-6,1.604142295579232E-5]    |\n",
      "|60     |3       |1.0       |[0.1494824482403191,0.8372281655201376,0.013171031593014717,6.139372138814352E-5,5.696092514061024E-5]    |\n",
      "|61     |2       |1.0       |[0.14210577171240524,0.8519957356050147,0.005783042058926368,6.483725278750427E-5,5.061337086604331E-5]   |\n",
      "|62     |3       |1.0       |[0.14079543435396252,0.8532693677353541,0.005815018389956432,6.852362143992684E-5,5.165589928702352E-5]   |\n",
      "|63     |2       |1.0       |[0.14360821920370936,0.8509661896866243,0.005176748048734011,1.9620871973584939E-4,5.263434119653093E-5]  |\n",
      "|64     |2       |1.0       |[0.14029295819256185,0.8537805394599562,0.005796717853458841,7.77018610569837E-5,5.208263296601007E-5]    |\n",
      "|65     |2       |1.0       |[0.1444286581444032,0.8503557018000072,0.005020183448541291,1.4554406690998762E-4,4.9912540138280036E-5]  |\n",
      "|66     |3       |1.0       |[0.08401477311507678,0.9130641256500243,0.002856756548396513,3.3245911711653734E-5,3.109877479065746E-5]  |\n",
      "|67     |2       |1.0       |[0.32989507280161845,0.65942626805518,0.009869330305304718,6.871300945121434E-4,1.2219874338456E-4]       |\n",
      "|68     |2       |1.0       |[0.3353275254596086,0.6543361512032517,0.008806996647842551,0.0013995638809745678,1.297628083225976E-4]   |\n",
      "|69     |2       |1.0       |[0.32989507280161845,0.65942626805518,0.009869330305304718,6.871300945121434E-4,1.2219874338456E-4]       |\n",
      "|70     |2       |1.0       |[0.32691713794399324,0.6623088748291898,0.0099087717858486,7.452382241663099E-4,1.1997721680219217E-4]    |\n",
      "|71     |2       |1.0       |[0.13196458231022978,0.8618578431475904,0.006020283934935932,1.0947375553206168E-4,4.781685171185417E-5]  |\n",
      "|72     |2       |1.0       |[0.3164393726402177,0.6736303018925596,0.008054551513096553,0.0017571902713664892,1.1858368275958872E-4]  |\n",
      "|73     |3       |1.0       |[0.14091267254734263,0.8477389364640545,0.01097370738200232,3.1545825849460274E-4,5.922534810575469E-5]   |\n",
      "|74     |3       |1.0       |[0.14771791560036565,0.8412895749914063,0.010589134870424757,3.405302174912798E-4,6.284432031201362E-5]   |\n",
      "|75     |3       |1.0       |[0.13510918895736088,0.8530261502418933,0.011608071243222001,2.01255095747661E-4,5.533446177610422E-5]    |\n",
      "|76     |3       |1.0       |[0.15324161897194172,0.8375790309522226,0.008925522964618961,1.9507264631721016E-4,5.875446489955318E-5]  |\n",
      "|77     |2       |1.0       |[0.15765867214683668,0.833575972566147,0.008547126501744788,1.57777205327752E-4,6.0451579943750596E-5]    |\n",
      "|78     |3       |1.0       |[0.10230843796810858,0.8861946093255568,0.01143119029593232,2.5969063643935033E-5,3.979334675812244E-5]   |\n",
      "|79     |2       |1.0       |[0.1856712100520948,0.7897049434646379,0.02440302576737165,1.4992610036341722E-4,7.089461553213302E-5]    |\n",
      "|80     |3       |1.0       |[0.13760089733543607,0.8570705484759635,0.004882895144266532,3.9228046945999774E-4,5.337857487396306E-5]  |\n",
      "|81     |2       |1.0       |[0.3195990711960842,0.6693947492705083,0.008939854332577149,0.00195005116181069,1.162740390195393E-4]     |\n",
      "|82     |3       |1.0       |[0.10536321282815202,0.8879118204834149,0.006602487165283012,8.642401213895183E-5,3.605551101112431E-5]   |\n",
      "|83     |2       |1.0       |[0.165630031576918,0.8250431679189579,0.009136104912414875,1.3320332453413988E-4,5.749226717504822E-5]    |\n",
      "|84     |3       |1.0       |[0.13785135502156262,0.8565364798722249,0.005370382389759313,1.9399643995157259E-4,4.778627650161266E-5]  |\n",
      "|85     |3       |1.0       |[0.05309655213814502,0.9455123151320362,0.0013377754452802374,3.451461102404179E-5,1.8842673514811824E-5] |\n",
      "|86     |2       |1.0       |[0.15295535773825902,0.8423797563366402,0.004083708650587914,5.15651426692863E-4,6.552584781980871E-5]    |\n",
      "|87     |2       |1.0       |[0.32781302990754674,0.6568335335130971,0.012274243287499613,0.002939081753211314,1.4011153864518816E-4]  |\n",
      "|88     |3       |1.0       |[0.08274983588324507,0.9136351314151462,0.003480988273998572,9.800256950516048E-5,3.6041858104966535E-5]  |\n",
      "|89     |3       |1.0       |[0.14802922015858594,0.8438676079161658,0.007950288449305832,9.553617299326133E-5,5.734730294907203E-5]   |\n",
      "|90     |3       |1.0       |[0.12369262412338258,0.8695566046702385,0.006397158751181248,3.0101700340224745E-4,5.259545179524272E-5]  |\n",
      "|91     |2       |1.0       |[0.32278562433120633,0.661496390228234,0.012470327232132835,0.0031103365867346075,1.3732162169220936E-4]  |\n",
      "|92     |2       |1.0       |[0.3154635679025065,0.6672975860443912,0.01335256086728078,0.0037534221207506688,1.328630650709544E-4]    |\n",
      "|93     |2       |1.0       |[0.3314845709617291,0.6508924793996282,0.013303211103882362,0.004183406059612863,1.3633247514754294E-4]   |\n",
      "|94     |3       |1.0       |[0.04633104248103773,0.9524797759497053,0.0011278413422339804,4.2114607429224775E-5,1.9225619593625494E-5]|\n",
      "|95     |3       |1.0       |[0.04328693143180994,0.9553910424086288,0.001219824825036479,8.417312558348827E-5,1.802820894151297E-5]   |\n",
      "|96     |2       |1.0       |[0.10885675227277845,0.8861979530004535,0.004407299230399337,4.931036773427333E-4,4.489181902591947E-5]   |\n",
      "|97     |2       |1.0       |[0.10854666887935545,0.887333886823218,0.0036575324747027834,4.173549681799538E-4,4.455685454386281E-5]   |\n",
      "|98     |2       |1.0       |[0.0934342808346224,0.8723301256903049,0.033388390488879795,8.06121820325205E-4,4.108116586765469E-5]     |\n",
      "|99     |2       |1.0       |[0.04282430534311253,0.9558379852129417,0.0012269438908761806,9.314760287464701E-5,1.7617950194985293E-5] |\n",
      "|100    |2       |1.0       |[0.043924942129797626,0.9548012849206401,0.0011750215764588282,8.053373134968005E-5,1.821764175380289E-5] |\n",
      "|101    |2       |1.0       |[0.09836183979579077,0.8961757334454415,0.004972984027459564,4.457329116622597E-4,4.370981964590374E-5]   |\n",
      "|102    |3       |1.0       |[0.1116825092811184,0.8836302952301825,0.004158986100649361,4.806244390486488E-4,4.758494900092796E-5]    |\n",
      "|103    |3       |1.0       |[0.11207238492481307,0.883605736201042,0.0038413290884442438,4.339317599220094E-4,4.6618025778696716E-5]  |\n",
      "|104    |3       |1.0       |[0.04324972210392344,0.9556930650736336,9.973066333211732E-4,4.206243557730095E-5,1.7843753544320294E-5]  |\n",
      "|105    |3       |1.0       |[0.11207238492481307,0.883605736201042,0.0038413290884442438,4.339317599220094E-4,4.6618025778696716E-5]  |\n",
      "|106    |2       |1.0       |[0.11431033256653733,0.8808113452526406,0.00438116144687657,4.4930894334723235E-4,4.785179059837759E-5]   |\n",
      "|107    |1       |1.0       |[0.16429437415720438,0.8155162472852203,0.019648304472891,4.73227298003248E-4,6.784678668102107E-5]       |\n",
      "|108    |2       |1.0       |[0.11431033256653733,0.8808113452526406,0.00438116144687657,4.4930894334723235E-4,4.785179059837759E-5]   |\n",
      "|109    |2       |1.0       |[0.11039276011825215,0.8849888057577048,0.004138081901821411,4.3372216771055637E-4,4.663005451126652E-5]  |\n",
      "|110    |2       |1.0       |[0.11681347675855577,0.8784248993307976,0.004336566076995464,3.7691329820652104E-4,4.814453544458156E-5]  |\n",
      "|111    |2       |1.0       |[0.11039276011825215,0.8849888057577048,0.004138081901821411,4.3372216771055637E-4,4.663005451126652E-5]  |\n",
      "|112    |2       |1.0       |[0.2766121632915328,0.7109861995489138,0.007353680562203675,0.00492906678621398,1.1888981113563105E-4]    |\n",
      "|113    |3       |1.0       |[0.1235200390413527,0.8717650861691254,0.0044381153425036264,2.2955562250560074E-4,4.720382451271962E-5]  |\n",
      "|114    |2       |1.0       |[0.10599255952969824,0.8886534298238368,0.004923402738218227,3.880645455316723E-4,4.2543362715109286E-5]  |\n",
      "|115    |2       |1.0       |[0.1613265857627683,0.830507735396943,0.007913701163766863,1.885450817143375E-4,6.343259480761195E-5]     |\n",
      "|116    |2       |1.0       |[0.179850351664478,0.7985835681455742,0.02140503838558079,9.108458802079942E-5,6.995721634602296E-5]      |\n",
      "|117    |2       |1.0       |[0.1260333093572313,0.8690163818509327,0.004579241503753751,3.2217472808006124E-4,4.8892560001969986E-5]  |\n",
      "|118    |3       |1.0       |[0.12143715079559625,0.8735577372905383,0.004772332134458658,1.8668066067765914E-4,4.609911872915989E-5]  |\n",
      "|119    |2       |1.0       |[0.2946982153935418,0.6952969178800781,0.008048996074399475,0.0018445095844463015,1.1136106753424577E-4]  |\n",
      "|120    |2       |1.0       |[0.12949424998947492,0.8657143993660742,0.004522384481275785,2.2054125722468005E-4,4.842490595033697E-5]  |\n",
      "|121    |2       |1.0       |[0.12205133609287515,0.8737027231524535,0.004055159202487871,1.45638840331972E-4,4.514271185146112E-5]    |\n",
      "|122    |2       |1.0       |[0.11893495707588894,0.8785117716177243,0.0023683556384107456,1.427673512786982E-4,4.214831669726124E-5]  |\n",
      "|123    |2       |1.0       |[0.10733199962561889,0.8883701147385789,0.004173157776892258,8.761811107271677E-5,3.710974783738227E-5]   |\n",
      "|124    |3       |1.0       |[0.12869206042059358,0.8667478385877107,0.0044485816560230875,7.132843397476485E-5,4.019090169801435E-5]  |\n",
      "|125    |3       |1.0       |[0.06865671419202919,0.9289774980907927,0.002242438296734008,9.739543949543794E-5,2.5953980948792427E-5]  |\n",
      "|126    |3       |1.0       |[0.17313402223341245,0.8066055560806488,0.019955391538635542,2.3661505051288673E-4,6.841509679010772E-5]  |\n",
      "|127    |2       |1.0       |[0.10834008730549552,0.8891377199806938,0.0024016020198181108,8.329628470794854E-5,3.7294409284624745E-5] |\n",
      "|128    |2       |1.0       |[0.1971761992323611,0.7799231672239126,0.02280544220809434,3.853567254667627E-5,5.665566308535588E-5]     |\n",
      "|129    |2       |1.0       |[0.2233659544129474,0.7527700582894563,0.023637034780888903,1.5134680599743344E-4,7.560571070999665E-5]   |\n",
      "|130    |2       |1.0       |[0.14086363685768713,0.8538331223546392,0.005032094755153286,2.2362698523407408E-4,4.7519047286409756E-5] |\n",
      "|131    |3       |1.0       |[0.08602487886983734,0.9115190309833061,0.0023615424380416376,6.384922254978426E-5,3.069848626515995E-5]  |\n",
      "|132    |2       |1.0       |[0.05530463773092385,0.9433512145008889,0.0012981646345898267,2.6846073809682816E-5,1.9137059787535803E-5]|\n",
      "|133    |2       |1.0       |[0.13562927228998262,0.859310039546496,0.004801985431090918,2.0907614457454666E-4,4.9626587856033134E-5]  |\n",
      "|134    |2       |1.0       |[0.1307342757885607,0.8643995982569825,0.004669335757486722,1.505239926870422E-4,4.626620428323664E-5]    |\n",
      "|135    |2       |1.0       |[0.052983239042972835,0.9456542740069935,0.0013196282895244944,2.4236781344907614E-5,1.862187916440214E-5]|\n",
      "|136    |2       |1.0       |[0.13420682516527188,0.8609121100478456,0.004699867211283001,1.3499878039868033E-4,4.6198795200797395E-5] |\n",
      "|137    |2       |1.0       |[0.135941230488049,0.8592741734797166,0.004617202822951358,1.2128811864872361E-4,4.610509063438051E-5]    |\n",
      "|138    |3       |1.0       |[0.1275182907546123,0.867861791062791,0.004440599002286955,1.3318888074253306E-4,4.6130299567262666E-5]   |\n",
      "|139    |2       |1.0       |[0.051355526438864846,0.947305379197012,0.0013004717702240491,2.0590000951319577E-5,1.8032592947852947E-5]|\n",
      "|140    |3       |1.0       |[0.11803264763517433,0.8759150975932642,0.005925827185589322,8.284898234582693E-5,4.357860362652477E-5]   |\n",
      "|141    |3       |1.0       |[0.0849431066447568,0.912337191989787,0.0026518370686255807,3.843641523795745E-5,2.942788159260697E-5]    |\n",
      "|142    |3       |1.0       |[0.15040513469379757,0.8416165441899672,0.00787916563364035,4.963108238113569E-5,4.952440021379233E-5]    |\n",
      "|143    |3       |1.0       |[0.15031515887396865,0.8417116032816984,0.007874115301960862,4.9617802230279734E-5,4.950474014188815E-5]  |\n",
      "|144    |2       |1.0       |[0.1503583509800508,0.8263481932825774,0.02259753236554348,6.412313034633519E-4,5.469206836482012E-5]     |\n",
      "|145    |3       |1.0       |[0.050854404898903915,0.9477424188730581,0.0013027199476270664,8.026862171444582E-5,2.0187658696417972E-5]|\n",
      "+-------+--------+----------+----------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, unix_timestamp, when,\n",
    "    hour, dayofweek, month, year,\n",
    "    monotonically_increasing_id\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import StringIndexerModel, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from pyspark.ml import Pipeline\n",
    "import os\n",
    "\n",
    "# 1) Spark başlat\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LR Batch Predict 100 Rows (full)\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\",\"8g\") \\\n",
    "    .getOrCreate()\n",
    "rows = [{\n",
    "    \"Source\": \"Source2\",\n",
    "    \"Severity\": 3,\n",
    "    \"Start_Time\": \"2016-02-08 05:46:00\",\n",
    "    \"End_Time\":   \"2016-02-08 11:00:00\",\n",
    "    \"Start_Lat\": 39.865147,\n",
    "    \"Start_Lng\": -84.058723,\n",
    "    \"Distance(mi)\": 0.01,\n",
    "    \"Description\": (\"Right lane blocked due to accident on I-70 Eastbound \"\n",
    "                    \"at Exit 41 OH-235 State Route 4.\"),\n",
    "    \"Street\": \"I-70 E\",\n",
    "    \"City\": \"Dayton\",\n",
    "    \"County\": \"Montgomery\",\n",
    "    \"State\": \"OH\",\n",
    "    \"Zipcode\": \"45424\",\n",
    "    \"Country\": \"US\",\n",
    "    \"Timezone\": \"US/Eastern\",\n",
    "    \"Airport_Code\": \"KFFO\",\n",
    "    \"Weather_Timestamp\": \"2016-02-08 05:58:00\",\n",
    "    \"Temperature(F)\": 36.9,\n",
    "    \"Humidity(%)\": 91.0,\n",
    "    \"Pressure(in)\": 29.68,\n",
    "    \"Visibility(mi)\": 10.0,\n",
    "    \"Wind_Direction\": \"Calm\",\n",
    "    \"Precipitation(in)\": 0.02,\n",
    "    \"Weather_Condition\": \"Light Rain\",\n",
    "    # all your boolean columns:\n",
    "    \"Amenity\": False,\n",
    "    \"Bump\": False,\n",
    "    \"Crossing\": False,\n",
    "    \"Give_Way\": False,\n",
    "    \"Junction\": False,\n",
    "    \"No_Exit\": False,\n",
    "    \"Railway\": False,\n",
    "    \"Roadway\": False,\n",
    "    \"Station\": False,\n",
    "    \"Stop\": False,\n",
    "    \"Traffic_Signal\": False,\n",
    "    \"Turning_Loop\": False,\n",
    "    # twilight columns:\n",
    "    \"Sunrise_Sunset\": \"Night\",\n",
    "    \"Civil_Twilight\": \"Night\",\n",
    "    \"Nautical_Twilight\": \"Night\",\n",
    "    \"Astronomical_Twilight\": \"Night\",\n",
    "}]\n",
    "\n",
    "\n",
    "# 2) df_no_na’yı baştan oluştur\n",
    "df = spark.read.csv(\"A-1.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 2.1) İstenmeyen sütunları düş\n",
    "drop_cols = [\n",
    "    \"ID\",\"Source\",\"Zipcode\",\"Timezone\",\"Airport_Code\",\"Amenity\",\"Bump\",\n",
    "    \"Give_Way\",\"No_Exit\",\"Railway\",\"Description\",\"County\",\"Roundabout\",\n",
    "    \"Station\",\"Stop\",\"Nautical_Twilight\",\"Astronomical_Twilight\",\"Country\"\n",
    "]\n",
    "df2 = df.drop(*drop_cols)\n",
    "\n",
    "# 2.2) Süre & tarih/saat özellikleri\n",
    "df3 = (\n",
    "    df2\n",
    "    .withColumn(\"Start_TS\", to_timestamp(col(\"Start_Time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .withColumn(\"End_TS\",   to_timestamp(col(\"End_Time\"),   \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .withColumn(\"Duration\",\n",
    "        ((unix_timestamp(col(\"End_TS\")) - unix_timestamp(col(\"Start_TS\"))) / 60)\n",
    "        .cast(DoubleType())\n",
    "    )\n",
    "    .withColumn(\"HourOfDay\", hour(col(\"Start_TS\")))\n",
    "    .withColumn(\"DayOfWeek\", dayofweek(col(\"Start_TS\")))\n",
    "    .withColumn(\"Month\",    month(col(\"Start_TS\")))\n",
    "    .withColumn(\"Year\",     year(col(\"Start_TS\")))\n",
    "    .drop(\"Start_TS\",\"End_TS\",\"Start_Time\",\"End_Time\")\n",
    ")\n",
    "\n",
    "# 2.3) City/Street cardinality düşürme\n",
    "def clean_column(df, column_name, top_n=32):\n",
    "    top_vals = [\n",
    "        r[column_name] for r in\n",
    "        df.groupBy(column_name).count()\n",
    "          .orderBy(col(\"count\").desc())\n",
    "          .limit(top_n)\n",
    "          .collect()\n",
    "    ]\n",
    "    return df.withColumn(\n",
    "        f\"{column_name}_Cleaned\",\n",
    "        when(col(column_name).isin(top_vals), col(column_name)).otherwise(\"Other\")\n",
    "    )\n",
    "\n",
    "for c in [\"City\",\"Street\"]:\n",
    "    df3 = clean_column(df3, c, top_n=32)\n",
    "\n",
    "# 2.4) İlgili sütunları seç ve dropna\n",
    "selected_cols = [\n",
    "    \"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\", \"Precipitation(in)\", \"Wind_Chill(F)\", \"Traffic_Signal\",\n",
    "    \"Weather_Condition\",\"Wind_Direction\",\"Civil_Twilight\",\"Sunrise_Sunset\",\n",
    "    \"State\",\"City_Cleaned\",\"Street_Cleaned\",\"Junction\",\"Duration\",\"Severity\",\n",
    "    \"HourOfDay\",\"DayOfWeek\",\"Month\",\"Year\"\n",
    "]\n",
    "df_no_na = df3.select(*selected_cols).dropna().cache()\n",
    "\n",
    "# 3) 100 satırlık örnek + row_id\n",
    "sample100 = df_no_na.limit(1000) \\\n",
    "                   .withColumn(\"_row_id\", monotonically_increasing_id())\n",
    "\n",
    "# 4) Küçük‐model’den indexer aşamaları (00…07) yükle\n",
    "base = \"models/us_accidents_lr_optimized_small_spark/stages\"\n",
    "indexer_paths = [os.path.join(base, d) for d in sorted(os.listdir(base)) if d.startswith(\"0\") and \"StringIndexer\" in d]\n",
    "prep_indexers = [StringIndexerModel.load(p) for p in indexer_paths]\n",
    "\n",
    "# 5) VectorAssembler’ı yeniden tanımla\n",
    "feature_cols = [\n",
    "    \"Temperature(F)\",\"Humidity(%)\",\"Pressure(in)\",\"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\",\"Precipitation(in)\",\"Wind_Chill(F)\",\"Traffic_Signal\",\n",
    "    \"Junction\",\"Duration\",\n",
    "    \"HourOfDay\",\"DayOfWeek\",\"Month\",\"Year\"\n",
    "] + [c + \"_Idx\" for c in [\n",
    "    \"Weather_Condition\",\"Wind_Direction\",\"Civil_Twilight\",\n",
    "    \"Sunrise_Sunset\",\"State\",\"City_Cleaned\",\"Street_Cleaned\"\n",
    "]]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "\n",
    "# 6) Son aşama olarak final LR modelini yükle\n",
    "lr_stage = \"models/us_accidents_lr_final_full_spark/stages/09_LogisticRegression_ea33fdabd04f\"\n",
    "lr_final = LogisticRegressionModel.load(lr_stage)\n",
    "\n",
    "# 7) Pipeline’ı oluştur ve tahmin yap\n",
    "pipe_full = Pipeline(stages = prep_indexers + [assembler, lr_final])\n",
    "predictions = pipe_full.fit(df_no_na).transform(sample100) \\\n",
    "                       .select(\"_row_id\",\"Severity\",\"prediction\",\"probability\")\n",
    "\n",
    "# 8) Sonuçları göster\n",
    "predictions.show(1000, truncate=False)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12c6b718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+----------------------------------------------------------------------------------------------------------+\n",
      "|_row_id|Severity|prediction|probability                                                                                               |\n",
      "+-------+--------+----------+----------------------------------------------------------------------------------------------------------+\n",
      "|0      |3       |1.0       |[0.17874585581159066,0.7960696227204713,0.024674170962156,4.4248753374946464E-4,6.786297203255087E-5]     |\n",
      "|1      |3       |1.0       |[0.18241842617852375,0.7929082762697467,0.02424033565452272,3.628132249768405E-4,7.014867222988984E-5]    |\n",
      "|2      |3       |1.0       |[0.09312441311369424,0.8974016595389032,0.009349538978597766,8.854343288822121E-5,3.584493591644712E-5]   |\n",
      "|3      |2       |1.0       |[0.3973740108084888,0.5609034025443987,0.038601326465150694,0.002964842136159676,1.5641804580225833E-4]   |\n",
      "|4      |2       |1.0       |[0.1877395976858138,0.7879520617680273,0.024001404852157817,2.3559466040880633E-4,7.13410335923528E-5]    |\n",
      "|5      |2       |1.0       |[0.3865255188563951,0.5735025790165138,0.03870243404372532,0.0011256993594122358,1.4376872395361224E-4]   |\n",
      "|6      |3       |1.0       |[0.17002022078503423,0.805497060341118,0.024254639413302626,1.6417972500088935E-4,6.389973554426129E-5]   |\n",
      "|7      |2       |1.0       |[0.1745521548552368,0.8020375770665434,0.02309260153665079,2.5033460196274176E-4,6.733193960621604E-5]    |\n",
      "|8      |2       |1.0       |[0.39039324404860576,0.5719057216196135,0.03616723832695109,0.0013876067843876056,1.461892204420381E-4]   |\n",
      "|9      |2       |1.0       |[0.3729018150254974,0.5887515301111778,0.036939918218538495,0.001280328570092011,1.2640807469431595E-4]   |\n",
      "|10     |2       |1.0       |[0.36512674449400273,0.5948042796106993,0.0390863064298188,8.504211818073094E-4,1.3224828367187886E-4]    |\n",
      "|11     |2       |1.0       |[0.16197880037666193,0.8133772856201362,0.024454185930693758,1.3489823386842758E-4,5.4829838639735625E-5] |\n",
      "|12     |2       |1.0       |[0.1888441537409816,0.7716987032483369,0.039353943072661934,4.205303224149575E-5,6.114690577801295E-5]    |\n",
      "|13     |2       |1.0       |[0.39109663086027524,0.5441001653969005,0.0644297000559021,2.4442647783113046E-4,1.2907720909100716E-4]   |\n",
      "|14     |2       |1.0       |[0.24290199470490542,0.7169164698639675,0.040007200354166696,8.750373833390446E-5,8.683133862646135E-5]   |\n",
      "|15     |3       |1.0       |[0.25073423222582364,0.7109528261257921,0.03809934322450916,1.1897594433277648E-4,9.462247954227078E-5]   |\n",
      "|16     |2       |1.0       |[0.2582250008240389,0.7016580949730696,0.039945170394952524,8.032513504153237E-5,9.140867289733315E-5]    |\n",
      "|17     |2       |0.0       |[0.48769518192017997,0.4540635499892735,0.0574138786918341,6.50060837628524E-4,1.7732856108383397E-4]     |\n",
      "|18     |2       |1.0       |[0.2501052875792786,0.7112926542079195,0.038446117072615624,6.593227020664207E-5,9.000886997971737E-5]    |\n",
      "|19     |3       |1.0       |[0.2165649942352217,0.7321532077697456,0.05110070132561699,9.747462558853433E-5,8.36220438273066E-5]      |\n",
      "|20     |3       |0.0       |[0.4825165383737343,0.46379981382312974,0.05298152548669481,5.181644455099074E-4,1.8395787093132324E-4]   |\n",
      "|21     |3       |1.0       |[0.24472253917041387,0.7159473030262442,0.03914759192428076,9.411352767025211E-5,8.845235139099906E-5]    |\n",
      "|22     |2       |0.0       |[0.4832526110978939,0.4625382497755047,0.053378574720531785,6.463135580108278E-4,1.8425084805873297E-4]   |\n",
      "|23     |3       |1.0       |[0.24919611665363356,0.7107667164183727,0.03985222437542704,9.575170460729864E-5,8.919084795935916E-5]    |\n",
      "|24     |2       |1.0       |[0.24217179799226435,0.7193021135796371,0.03836574232334092,7.232152306270113E-5,8.802458169497218E-5]    |\n",
      "|25     |2       |1.0       |[0.24217179799226435,0.7193021135796371,0.03836574232334092,7.232152306270113E-5,8.802458169497218E-5]    |\n",
      "|26     |3       |1.0       |[0.24217179799226435,0.7193021135796371,0.03836574232334092,7.232152306270113E-5,8.802458169497218E-5]    |\n",
      "|27     |3       |1.0       |[0.2545070048965109,0.7057639725421416,0.03957415496694817,6.57550828949416E-5,8.911251150429175E-5]      |\n",
      "|28     |2       |1.0       |[0.4386540896847861,0.5247369134361565,0.03574338392357176,7.021722898945582E-4,1.6344066559102952E-4]    |\n",
      "|29     |2       |1.0       |[0.21428941645847605,0.7612205239500562,0.02429074418859219,1.2293092397950076E-4,7.638447889628715E-5]   |\n",
      "|30     |2       |1.0       |[0.21428941645847605,0.7612205239500562,0.02429074418859219,1.2293092397950076E-4,7.638447889628715E-5]   |\n",
      "|31     |2       |1.0       |[0.21428941645847605,0.7612205239500562,0.02429074418859219,1.2293092397950076E-4,7.638447889628715E-5]   |\n",
      "|32     |3       |1.0       |[0.21647178975682518,0.7599363620515767,0.02336598287855639,1.4596547733612512E-4,7.989983570567078E-5]   |\n",
      "|33     |2       |1.0       |[0.21135565037389098,0.7656728001030199,0.022786629642936942,1.0622930836172542E-4,7.869057179027047E-5]  |\n",
      "|34     |2       |1.0       |[0.22137193506643654,0.754521681653134,0.023934736101551023,9.515812819762107E-5,7.648905068097327E-5]    |\n",
      "|35     |2       |1.0       |[0.22137193506643654,0.754521681653134,0.023934736101551023,9.515812819762107E-5,7.648905068097327E-5]    |\n",
      "|36     |2       |1.0       |[0.21404095127973313,0.7621166130400218,0.023673070469164774,9.457473828630972E-5,7.479047279402211E-5]   |\n",
      "|37     |3       |1.0       |[0.15412840343120912,0.8315990794721726,0.01417595271346326,4.328024421932881E-5,5.3284138935761096E-5]   |\n",
      "|38     |3       |1.0       |[0.22137193506643654,0.754521681653134,0.023934736101551023,9.515812819762107E-5,7.648905068097327E-5]    |\n",
      "|39     |2       |1.0       |[0.4092643376302269,0.556492504237286,0.03352039689689067,5.753233830947562E-4,1.4743785250159543E-4]     |\n",
      "|40     |3       |1.0       |[0.2668826158915028,0.6940740534461971,0.038911928977040294,4.3622641539325824E-5,8.777904372043893E-5]   |\n",
      "|41     |2       |0.0       |[0.5113723325408982,0.43365007245497716,0.05449107402895335,3.1415604300690447E-4,1.7236493216435825E-4]  |\n",
      "|42     |2       |1.0       |[0.24905109927858057,0.711572420233687,0.039250733984079134,4.336257914293219E-5,8.23839245102792E-5]     |\n",
      "|43     |3       |1.0       |[0.23639660163384035,0.7393820488198489,0.024071660414321062,7.158820856293556E-5,7.810092342681859E-5]   |\n",
      "|44     |2       |1.0       |[0.2228328706293984,0.7542495699455721,0.022785076911717075,5.713141510628282E-5,7.535109820617498E-5]    |\n",
      "|45     |2       |1.0       |[0.2329136190618335,0.7433661958368225,0.023578041574490474,6.477781828610318E-5,7.736570856735373E-5]    |\n",
      "|46     |3       |1.0       |[0.22096711336495542,0.7572978563200151,0.02161270523939209,4.746405199649344E-5,7.486102364077934E-5]    |\n",
      "|47     |3       |1.0       |[0.17892074710758463,0.7908000485738259,0.03018633715711014,3.351593363739304E-5,5.9351227842004434E-5]   |\n",
      "|48     |3       |1.0       |[0.19344278150740096,0.7838645737969392,0.022611625557759172,2.2162218379575134E-5,5.885691952100644E-5]  |\n",
      "|49     |3       |1.0       |[0.22418671406904184,0.736493174458101,0.039243416571290664,1.0111130727183993E-5,6.658377083925561E-5]   |\n",
      "|50     |3       |1.0       |[0.2718306988312845,0.689092807651165,0.03895790491631863,3.6574690823473796E-5,8.201391040833774E-5]     |\n",
      "|51     |3       |1.0       |[0.27185526649230307,0.6887275400703166,0.03928546700279631,4.3520719660425284E-5,8.820571492360868E-5]   |\n",
      "|52     |3       |1.0       |[0.24192346524755765,0.7351277528574045,0.022817642426931375,4.904883651961493E-5,8.209063158698212E-5]   |\n",
      "|53     |3       |1.0       |[0.23678872580954383,0.7245089124838661,0.03862114839445728,1.6982727681951938E-5,6.423058445065539E-5]   |\n",
      "|54     |2       |1.0       |[0.18854399524416668,0.791671211521079,0.019689658339055494,3.497695313682333E-5,6.015794256193475E-5]    |\n",
      "|55     |3       |1.0       |[0.17162851894264278,0.8016099926412683,0.026665986326528463,3.515918006095123E-5,6.034290949950137E-5]   |\n",
      "|56     |3       |1.0       |[0.1836921951147466,0.774606379418113,0.041635727097676496,7.848234951490279E-6,5.785013451247614E-5]     |\n",
      "|57     |3       |1.0       |[0.18366115326789928,0.7746450118504943,0.04162814323222681,7.847707399548841E-6,5.784394198012181E-5]    |\n",
      "|58     |3       |1.0       |[0.11216051485768275,0.8737876265307597,0.014019786129011745,1.822349589071447E-6,3.0250132956837194E-5]  |\n",
      "|59     |3       |1.0       |[0.17886329400008455,0.7910503386328603,0.030014854939286323,1.2697754350883327E-5,5.881467341780283E-5]  |\n",
      "|60     |3       |1.0       |[0.21018235412953884,0.7305047507579405,0.059192994728577135,4.0057975663959454E-5,7.984240827954036E-5]  |\n",
      "|61     |2       |1.0       |[0.20613798687399384,0.7669320702163056,0.02681310662299141,4.364453870935458E-5,7.319174799972523E-5]    |\n",
      "|62     |3       |1.0       |[0.2043602525164518,0.7685412424493221,0.02697760692393007,4.6153767225613095E-5,7.474434307052219E-5]    |\n",
      "|63     |2       |1.0       |[0.20833754652350855,0.7673364495754074,0.024121296409947816,1.28570659592976E-4,7.61368315433429E-5]     |\n",
      "|64     |2       |1.0       |[0.20370161184563318,0.7692686055321878,0.02690204075326371,5.2353895653357874E-5,7.538797326205026E-5]   |\n",
      "|65     |2       |1.0       |[0.20955438056019343,0.7668832816986365,0.023394745409466962,9.538348651699549E-5,7.220884518600872E-5]   |\n",
      "|66     |3       |1.0       |[0.13764794975755948,0.8468219389280196,0.015451800496126255,2.7522924967094388E-5,5.078789332741683E-5]  |\n",
      "|67     |2       |1.0       |[0.4278326194550226,0.5306858064540644,0.04091006974657033,4.135195284937419E-4,1.5798481584897137E-4]    |\n",
      "|68     |2       |1.0       |[0.43487633410251036,0.5274517768260217,0.036684260554685544,8.19831526987199E-4,1.6779698979511683E-4]   |\n",
      "|69     |2       |1.0       |[0.4278326194550226,0.5306858064540644,0.04091006974657033,4.135195284937419E-4,1.5798481584897137E-4]    |\n",
      "|70     |2       |1.0       |[0.4245422937128476,0.5337243451290526,0.04112894515427534,4.4909413734178246E-4,1.5532186648253973E-4]   |\n",
      "|71     |2       |1.0       |[0.19233259417593085,0.7794787943194844,0.028045097073731304,7.403967579422737E-5,6.947475505931849E-5]   |\n",
      "|72     |2       |1.0       |[0.4153155829853704,0.5495340421880738,0.03395349175411654,0.0010416979779474564,1.5518509449192387E-4]   |\n",
      "|73     |3       |1.0       |[0.2003639884543406,0.7492332138120046,0.0501162261307626,2.0260345035280131E-4,8.396815253956912E-5]     |\n",
      "|74     |3       |1.0       |[0.20957065586882875,0.7418704685127316,0.048251758989468214,2.1821684721618707E-4,8.889978175527051E-5]  |\n",
      "|75     |3       |1.0       |[0.19225829356340754,0.7544801460121483,0.05305369417367075,1.2935475600188746E-4,7.851149477136591E-5]   |\n",
      "|76     |3       |1.0       |[0.21808661007783858,0.7409063909493617,0.04079822891685326,1.2539608143251734E-4,8.337397451399046E-5]   |\n",
      "|77     |2       |1.0       |[0.22414997487852892,0.736633206462767,0.03902980039439202,1.0132121957218104E-4,8.569704473997826E-5]    |\n",
      "|78     |3       |1.0       |[0.23181669835218435,0.7309734775728102,0.03696962366495029,1.5014311669299396E-4,9.005729336205041E-5]   |\n",
      "|79     |2       |1.0       |[0.1856712100520948,0.7897049434646379,0.02440302576737165,1.4992610036341722E-4,7.089461553213302E-5]    |\n",
      "|80     |3       |1.0       |[0.2005148531584792,0.77629560732013,0.02285378035849427,2.582006244059778E-4,7.755853849041886E-5]       |\n",
      "|81     |2       |1.0       |[0.41756891545123137,0.5436134945243044,0.03751530346283079,0.0011508109302318745,1.5147563140153775E-4]  |\n",
      "|82     |3       |1.0       |[0.15527982320328051,0.8133568620570181,0.03125280173920655,5.753018442808023E-5,5.2982816066819963E-5]   |\n",
      "|83     |2       |1.0       |[0.23397119903585634,0.724411377863683,0.04145145338025055,8.499114882074841E-5,8.097857138924197E-5]     |\n",
      "|84     |3       |1.0       |[0.20047402108231657,0.7742446282964206,0.025084626592405478,1.2743130822255407E-4,6.929272063477473E-5]  |\n",
      "|85     |3       |1.0       |[0.1943307354754266,0.7830092476631116,0.022445747608430582,1.45589263846131E-4,6.867998918489621E-5]     |\n",
      "|86     |2       |1.0       |[0.42099579866358044,0.5253788990202658,0.051809705878731585,0.0016365878578165607,1.7900857960569276E-4] |\n",
      "|87     |2       |1.0       |[0.42235063786712146,0.5252431835451957,0.0505160530195042,0.001709523551395035,1.8060201678374305E-4]    |\n",
      "|88     |3       |1.0       |[0.1251865287973786,0.8578698658028729,0.016822121185866335,6.693363297718052E-5,5.455058090495825E-5]    |\n",
      "|89     |3       |1.0       |[0.2122933390335904,0.7511408805719284,0.0364216438412567,6.185477565961395E-5,8.228177756487668E-5]      |\n",
      "|90     |3       |1.0       |[0.18036112910043783,0.7892405378110613,0.03012376608739088,1.983555377805538E-4,7.62114633293167E-5]     |\n",
      "|91     |2       |1.0       |[0.41664231270142044,0.5299499223116905,0.05141795255717407,0.0018124793064530863,1.773331232619315E-4]   |\n",
      "|92     |2       |1.0       |[0.40664068902494843,0.5354187291722705,0.055583921588777965,0.00218646832564231,1.7019188836082654E-4]   |\n",
      "|93     |2       |1.0       |[0.42453999734467585,0.5181432228969781,0.054722653125997923,0.0024200293575338426,1.7409727481447278E-4] |\n",
      "|94     |3       |1.0       |[0.17368943688827335,0.8067788731316149,0.019277796819918803,1.8187225217231238E-4,7.202090802067229E-5]  |\n",
      "|95     |3       |1.0       |[0.16303190837159579,0.8153585205062222,0.02117661732196234,3.6556053093119226E-4,6.739326928859936E-5]   |\n",
      "|96     |2       |1.0       |[0.16127376916919656,0.8172435261994802,0.021086470696784713,3.301419892495805E-4,6.60919452889647E-5]    |\n",
      "|97     |2       |1.0       |[0.16130640199521543,0.8207947213224986,0.017552795411991146,2.8028176980018406E-4,6.579950049464489E-5]  |\n",
      "|98     |2       |1.0       |[0.09629130753622517,0.8699811117458817,0.032873463547731456,8.119360004803512E-4,4.218116968127127E-5]   |\n",
      "|99     |2       |1.0       |[0.16148328417780264,0.816719958373356,0.02132579636685935,4.0502232525737404E-4,6.593875672470916E-5]    |\n",
      "|100    |2       |1.0       |[0.1652519638060637,0.81395437686554,0.020376265074896593,3.493681307016852E-4,6.802612279813126E-5]      |\n",
      "|101    |2       |1.0       |[0.14645698585686928,0.8293959361262138,0.023782411062507003,2.9977365995276586E-4,6.489329445724537E-5]  |\n",
      "|102    |3       |1.0       |[0.16556910330580318,0.814235447556542,0.019803272837251258,3.2183650490073384E-4,7.033979550286607E-5]   |\n",
      "|103    |3       |1.0       |[0.16631172182469067,0.8150195887652735,0.01830885264114814,2.908579929766178E-4,6.897877591136939E-5]    |\n",
      "|104    |3       |1.0       |[0.1639522919523232,0.8185590689716129,0.017237367157817422,1.8367960655223171E-4,6.759231169437974E-5]   |\n",
      "|105    |3       |1.0       |[0.16631172182469067,0.8150195887652735,0.01830885264114814,2.908579929766178E-4,6.897877591136939E-5]    |\n",
      "|106    |2       |1.0       |[0.16885321203686185,0.8098771046816213,0.02089950940602688,2.999321777353323E-4,7.024169775451558E-5]    |\n",
      "|107    |1       |1.0       |[0.16429437415720438,0.8155162472852203,0.019648304472891,4.73227298003248E-4,6.784678668102107E-5]       |\n",
      "|108    |2       |1.0       |[0.16885321203686185,0.8098771046816213,0.02089950940602688,2.999321777353323E-4,7.024169775451558E-5]    |\n",
      "|109    |2       |1.0       |[0.16378686753286054,0.8161341239731922,0.01971936543957162,2.9066010358016396E-4,6.8982950795364E-5]     |\n",
      "|110    |2       |1.0       |[0.17277717696835682,0.806410597971298,0.020489300523985517,2.5168154031443845E-4,7.124299604502908E-5]   |\n",
      "|111    |2       |1.0       |[0.16378686753286054,0.8161341239731922,0.01971936543957162,2.9066010358016396E-4,6.8982950795364E-5]     |\n",
      "|112    |2       |1.0       |[0.371158337390931,0.5938291355582951,0.031865130696011885,0.0029888685108986463,1.5852784386328786E-4]   |\n",
      "|113    |3       |1.0       |[0.18193522271152476,0.7969608797631026,0.020881692307915794,1.5264542067566675E-4,6.955979678117027E-5]  |\n",
      "|114    |2       |1.0       |[0.15737694954203657,0.8189480070780539,0.023351718512529672,2.601274532116898E-4,6.31974141681859E-5]    |\n",
      "|115    |2       |1.0       |[0.22944857055722465,0.7341898396991516,0.03615051000044066,1.2112392654619593E-4,8.995581663692356E-5]   |\n",
      "|116    |2       |1.0       |[0.179850351664478,0.7985835681455742,0.02140503838558079,9.108458802079942E-5,6.995721634602296E-5]      |\n",
      "|117    |2       |1.0       |[0.1850481132377266,0.7930718945046825,0.021594752853802704,2.1366142044448308E-4,7.157798334380442E-5]   |\n",
      "|118    |3       |1.0       |[0.17861904038217358,0.7986437033765711,0.022545621528254692,1.2402545566109216E-4,6.760925733928619E-5]  |\n",
      "|119    |2       |1.0       |[0.39101989751244903,0.5734254055748439,0.03430192046553497,0.001105446139175749,1.4733030799645106E-4]   |\n",
      "|120    |2       |1.0       |[0.18980091737490837,0.7886925496149038,0.02128975546436783,1.4600676812465677E-4,7.077077769523615E-5]   |\n",
      "|121    |2       |1.0       |[0.17995086178819764,0.8006825372066747,0.019203247095741192,9.698932220202857E-5,6.636458718430274E-5]   |\n",
      "|122    |2       |1.0       |[0.3617479410004271,0.6046007210957858,0.033023199393349904,5.004683795336978E-4,1.276701309034764E-4]    |\n",
      "|123    |2       |1.0       |[0.15948521415128894,0.8204845738263644,0.01991642469071256,5.880583362950707E-5,5.498149800474059E-5]    |\n",
      "|124    |3       |1.0       |[0.18875586112497797,0.7901812743722082,0.020956831754741258,4.72548838695847E-5,5.87778642031583E-5]     |\n",
      "|125    |3       |1.0       |[0.11387614668250932,0.8736626826925361,0.012338799852191428,7.94523872224561E-5,4.2918385540638326E-5]   |\n",
      "|126    |3       |1.0       |[0.17313402223341245,0.8066055560806488,0.019955391538635542,2.3661505051288673E-4,6.841509679010772E-5]  |\n",
      "|127    |2       |1.0       |[0.33785852937123734,0.6273924220480264,0.03433384384874641,2.9937982500893824E-4,1.1582490698091279E-4]  |\n",
      "|128    |2       |1.0       |[0.1971761992323611,0.7799231672239126,0.02280544220809434,3.853567254667627E-5,5.665566308535588E-5]     |\n",
      "|129    |2       |1.0       |[0.2233659544129474,0.7527700582894563,0.023637034780888903,1.5134680599743344E-4,7.560571070999665E-5]   |\n",
      "|130    |2       |1.0       |[0.20477771182893653,0.7715108919624516,0.02349567724295105,1.4683964210449282E-4,6.887932355631125E-5]   |\n",
      "|131    |3       |1.0       |[0.14084906219439766,0.8462223146429508,0.012827095080728306,5.141668632524097E-5,5.011139559792778E-5]   |\n",
      "|132    |2       |1.0       |[0.20128583822755186,0.7768722514612185,0.021659933735905544,1.1261173295541894E-4,6.936484236868369E-5]  |\n",
      "|133    |2       |1.0       |[0.19790881651024486,0.7793757172240204,0.022505461167170376,1.3780072816376704E-4,7.220437040077016E-5]  |\n",
      "|134    |2       |1.0       |[0.19137692126225403,0.7865021771440691,0.02195384382809608,9.952705354318749E-5,6.753071203744353E-5]    |\n",
      "|135    |2       |1.0       |[0.19404132682020653,0.7836328774441325,0.02215557519284467,1.0230146876182201E-4,6.791907405451008E-5]   |\n",
      "|136    |2       |1.0       |[0.19605964249262733,0.7817316490795192,0.022052333906790553,8.907970457126749E-5,6.72948164916065E-5]    |\n",
      "|137    |2       |1.0       |[0.19846451083829897,0.7797379908360819,0.02165040288004605,7.998070727559806E-5,6.711473829760825E-5]    |\n",
      "|138    |3       |1.0       |[0.18716415309683798,0.7917462834980485,0.020933753985225966,8.829853117651799E-5,6.751088871104892E-5]   |\n",
      "|139    |2       |1.0       |[0.1890123665321713,0.7888920247029905,0.02194217350594551,8.733947844725974E-5,6.609578044533436E-5]     |\n",
      "|140    |3       |1.0       |[0.17317429333955162,0.7987825368258514,0.02792451413591542,5.49039730405938E-5,6.375172564102799E-5]     |\n",
      "|141    |3       |1.0       |[0.12858822400987271,0.8584474435852463,0.012893631772564423,2.6281566857427532E-5,4.441906545934461E-5]  |\n",
      "|142    |3       |1.0       |[0.21520219594648057,0.7484858152062179,0.03620925874300657,3.207547538044865E-5,7.065462891462598E-5]    |\n",
      "|143    |3       |1.0       |[0.215087963543322,0.74862084575072,0.03618849030604575,3.206905559477505E-5,7.063134431753306E-5]        |\n",
      "|144    |2       |1.0       |[0.1503583509800508,0.8263481932825774,0.02259753236554348,6.412313034633519E-4,5.469206836482012E-5]     |\n",
      "|145    |3       |1.0       |[0.18738939179229624,0.7901895320537361,0.022006104435618786,3.408893427997721E-4,7.40823755492148E-5]    |\n",
      "|146    |2       |1.0       |[0.37982950415263356,0.6118119342934707,0.00818171539250647,7.208374992381138E-5,1.0476241146541224E-4]   |\n",
      "|147    |2       |1.0       |[0.43741515265450076,0.5465393870264532,0.015861510937599244,6.453459759718173E-5,1.1941478384966727E-4]  |\n",
      "|148    |2       |1.0       |[0.255928011105608,0.7379704692113253,0.00601796370574763,1.5232376030312471E-5,6.832360128887347E-5]     |\n",
      "|149    |2       |1.0       |[0.41837726485847465,0.5684995596145039,0.012966324375021253,4.622339232816031E-5,1.1062775967187213E-4]  |\n",
      "|150    |2       |1.0       |[0.4654265840621581,0.5087363573951106,0.02569676051220323,1.9483375485129246E-5,1.208146550430623E-4]    |\n",
      "|151    |2       |1.0       |[0.464347519206493,0.5128659006477624,0.022630387252784996,2.5024023888971367E-5,1.311688690706852E-4]    |\n",
      "|152    |2       |1.0       |[0.22463672126551465,0.7638530353833646,0.01143666131967187,5.712573368323472E-6,6.786945808065244E-5]    |\n",
      "|153    |2       |1.0       |[0.4566570668319173,0.5205681141807282,0.022633170117523013,1.3953810609046953E-5,1.2769505922256095E-4]  |\n",
      "|154    |2       |1.0       |[0.4095100678523364,0.577224922487149,0.013130357720352608,2.5862834172999302E-5,1.0878910598892786E-4]   |\n",
      "|155    |2       |0.0       |[0.7222424707815838,0.24971927654810483,0.027736132709940912,1.118368330766982E-4,1.9028312729386498E-4]  |\n",
      "|156    |3       |1.0       |[0.3736178423191164,0.6085780816992624,0.017670425499898,2.76162758256971E-5,1.0603420589754399E-4]       |\n",
      "|157    |2       |1.0       |[0.4539514307882817,0.5235793262803529,0.022317026096645286,2.4328813608297526E-5,1.2788802111175491E-4]  |\n",
      "|158    |2       |0.0       |[0.7244746567776619,0.2487745508218357,0.026419372919832058,1.3342545909255215E-4,1.9799402157780728E-4]  |\n",
      "|159    |3       |1.0       |[0.4078064244941918,0.5731130401864772,0.01895549846013442,1.6165185733292836E-5,1.0887167346325513E-4]   |\n",
      "|160    |2       |1.0       |[0.4574572443496829,0.5202632272565255,0.022132427901802466,1.8624481632449417E-5,1.284760103568023E-4]   |\n",
      "|161    |3       |1.0       |[0.3760871730528114,0.6138164502118301,0.009979877964089019,1.681009727611118E-5,9.968867399341484E-5]    |\n",
      "|162    |2       |1.0       |[0.41358839565964817,0.5743561696282417,0.011919551048670332,2.2718343047190885E-5,1.1316532039249294E-4] |\n",
      "|163    |2       |0.0       |[0.7394383595114874,0.24596272167124736,0.013005949273389912,0.0013383954904514284,2.545740534237638E-4]  |\n",
      "|164    |3       |1.0       |[0.44940153417334766,0.5298889488696858,0.02047858678952346,8.922071366681902E-5,1.4170945377606876E-4]   |\n",
      "|165    |3       |1.0       |[0.45659373429397987,0.5221371772755726,0.02103185713461243,9.325890605538502E-5,1.4397238977959355E-4]   |\n",
      "|166    |2       |0.0       |[0.5404960731560041,0.4361393933908018,0.023070946957376406,1.2280974415649783E-4,1.7077675166110546E-4]  |\n",
      "|167    |3       |0.0       |[0.5178081774414751,0.4598576906420037,0.022002111496644154,1.6077468676654592E-4,1.7124573311058658E-4]  |\n",
      "|168    |2       |0.0       |[0.5264687635680909,0.44790889840685444,0.025300520741954918,1.5354338534366734E-4,1.6827389775618374E-4] |\n",
      "|169    |3       |0.0       |[0.5189310376535863,0.4581710170567047,0.022583489413248357,1.46850969266945E-4,1.676049071937156E-4]     |\n",
      "|170    |2       |0.0       |[0.540661212334061,0.4354926691893909,0.02353938640845501,1.3547254380899662E-4,1.712595242840382E-4]     |\n",
      "|171    |2       |0.0       |[0.501989930475989,0.4746615521813979,0.02306792056600199,1.225749496820282E-4,1.5802182692899243E-4]     |\n",
      "|172    |3       |1.0       |[0.4571159164867456,0.5239419372963107,0.018726403748550183,6.913177292961132E-5,1.4661069546397602E-4]   |\n",
      "|173    |3       |0.0       |[0.5148194996994505,0.4619276929597795,0.022990018969722775,9.771311659304649E-5,1.6507525445406054E-4]   |\n",
      "|174    |2       |1.0       |[0.47401833440329944,0.5103997752534811,0.015274301424271483,1.583329780026804E-4,1.4925594094532198E-4]  |\n",
      "|175    |3       |1.0       |[0.3958503116131195,0.5966249719278314,0.0072565237895819016,1.479978896874782E-4,1.201947797796568E-4]   |\n",
      "|176    |2       |0.0       |[0.7151648888482811,0.2660225311025643,0.017658343474566095,9.275502088980706E-4,2.266863656902882E-4]    |\n",
      "|177    |3       |1.0       |[0.37824036024867613,0.6049365501202548,0.016585538088963805,1.1043404134576907E-4,1.271175007593516E-4]  |\n",
      "|178    |3       |1.0       |[0.4470755162763999,0.5357151247019349,0.01702004787307523,6.126271220066397E-5,1.2804843638917742E-4]    |\n",
      "|179    |3       |1.0       |[0.36825055971189913,0.624072426387316,0.007481240732429372,8.539278794938988E-5,1.1038038040600362E-4]   |\n",
      "|180    |2       |0.0       |[0.6787114185791117,0.3061519432444525,0.014703852139844759,2.3905649651041898E-4,1.9372954008051467E-4]  |\n",
      "|181    |2       |1.0       |[0.41751569099043906,0.5502780876865213,0.03205677800102925,2.645490821567094E-5,1.229884137947106E-4]    |\n",
      "|182    |2       |1.0       |[0.4498489589725637,0.5377584202818235,0.01223245884429344,2.7490904435204304E-5,1.3267099688395648E-4]   |\n",
      "|183    |2       |1.0       |[0.4538754218598354,0.5281179326307458,0.017858280278017502,2.6639096600259957E-5,1.2172613480098432E-4]  |\n",
      "|184    |2       |1.0       |[0.4633756371260309,0.5212991344088986,0.014963271822768543,2.1232710730520213E-4,1.4962953499665738E-4]  |\n",
      "|185    |2       |1.0       |[0.4533034713319215,0.5307505869528713,0.015635282858366722,1.6933538606149728E-4,1.4132347077910263E-4]  |\n",
      "|186    |2       |1.0       |[0.4450587378110385,0.5393711369989797,0.015339847678601139,1.0022051796606487E-4,1.3005699341452932E-4]  |\n",
      "|187    |3       |1.0       |[0.44923376385376906,0.5314375591871714,0.019196640609333408,6.142762881407492E-6,1.2589358684465408E-4]  |\n",
      "|188    |3       |1.0       |[0.44886417733073813,0.5227174830002941,0.028285151289369845,1.005764842023564E-5,1.2313073117767745E-4]  |\n",
      "|189    |3       |1.0       |[0.4295735051094295,0.5472711422840963,0.02303380863003347,1.4679681147764162E-6,1.2007600832580773E-4]   |\n",
      "|190    |3       |1.0       |[0.43666940244557684,0.5395389157434354,0.02366809131374496,1.535187410925967E-6,1.2205530983196757E-4]   |\n",
      "|191    |3       |1.0       |[0.45215251055955225,0.5292017968784094,0.018515979131607508,7.417533611586631E-6,1.2229589681950594E-4]  |\n",
      "|192    |3       |1.0       |[0.44268373286555635,0.5356380559355599,0.021561035918470577,5.397111427615106E-6,1.1177816898551452E-4]  |\n",
      "|193    |2       |1.0       |[0.44474999061452947,0.5262786018214424,0.028845905363902268,7.604806590403601E-6,1.1789739353557678E-4]  |\n",
      "|194    |3       |1.0       |[0.46475062789758137,0.5054576452701887,0.029672157668042008,5.195323042089063E-6,1.1437384114585407E-4]  |\n",
      "|195    |2       |1.0       |[0.4276376585195052,0.5320759239561549,0.040168700704873106,6.626450346406184E-6,1.1109036912035626E-4]   |\n",
      "|196    |3       |0.0       |[0.5284191224180652,0.4419145322512755,0.02947215873389952,3.793989507274992E-5,1.5624670168696176E-4]    |\n",
      "|197    |3       |1.0       |[0.45756667629443815,0.5273200962671502,0.014777639978107653,1.9002990220814555E-4,1.4555755809596586E-4] |\n",
      "|198    |2       |1.0       |[0.4650205796100181,0.5195235445312965,0.015141761867148325,1.729723102598445E-4,1.4114168127740014E-4]   |\n",
      "|199    |3       |1.0       |[0.36650240164243575,0.6253747239012759,0.007896701846740982,1.2101964185720445E-4,1.0515296768994223E-4] |\n",
      "|200    |3       |1.0       |[0.33233125535050595,0.6489117550696728,0.018568845804289684,6.214730084612514E-5,1.2599647468549807E-4]  |\n",
      "|201    |2       |1.0       |[0.40412083284888917,0.5765594732946718,0.01892796191644371,2.4976353529503986E-4,1.4196840470016332E-4]  |\n",
      "|202    |2       |1.0       |[0.3881832650836543,0.5917349739870902,0.01964120766900458,3.0925840082398574E-4,1.312948594270304E-4]    |\n",
      "|203    |2       |1.0       |[0.44383433682359735,0.5249787039028605,0.03096938917239344,7.129196597277996E-5,1.4627813517582953E-4]   |\n",
      "|204    |2       |1.0       |[0.4438348662506949,0.5249781331984088,0.03096943036020377,7.129197542799524E-5,1.4627821526440977E-4]    |\n",
      "|205    |2       |1.0       |[0.43256106655412896,0.5377025041343481,0.029523855465362632,6.914712098478162E-5,1.4342672517577893E-4]  |\n",
      "|206    |3       |1.0       |[0.40001362076180524,0.5674834674368924,0.032213627420502884,1.6023534434683815E-4,1.2904903645267565E-4] |\n",
      "|207    |2       |1.0       |[0.432560220774809,0.5377034142679256,0.0295237912613552,6.914710308042943E-5,1.434265928298967E-4]       |\n",
      "|208    |2       |1.0       |[0.4300570444538589,0.5418025724046293,0.027916398493339066,7.223889208963587E-5,1.5174575608313247E-4]   |\n",
      "|209    |2       |1.0       |[0.3817111761098114,0.5765624057365435,0.041359152135692155,2.298232420516685E-4,1.37442775901329E-4]     |\n",
      "|210    |2       |1.0       |[0.44962174136388317,0.5220110839212416,0.0281813818604442,3.504543844100834E-5,1.507474159899822E-4]     |\n",
      "|211    |2       |1.0       |[0.4553719712151563,0.5144905269509988,0.029909401498100594,7.346887360613952E-5,1.546314621381142E-4]    |\n",
      "|212    |2       |1.0       |[0.45537133237081345,0.5144912127830426,0.029909354617183772,7.34688640079932E-5,1.5463136495214993E-4]   |\n",
      "|213    |2       |1.0       |[0.45537324890396425,0.5144891552867399,0.02990949525998442,7.346889280201978E-5,1.5463165650935897E-4]   |\n",
      "|214    |2       |1.0       |[0.45220744865409046,0.517676169766963,0.02989754070057165,6.66076379479906E-5,1.5223324042699376E-4]     |\n",
      "|215    |2       |1.0       |[0.4374428685665173,0.5310471812424268,0.03131006683880237,6.0585903679428234E-5,1.3929744857400837E-4]   |\n",
      "|216    |2       |1.0       |[0.3847373531502016,0.5822832942237307,0.032714536270969014,1.385305146757332E-4,1.26285840422909E-4]     |\n",
      "|217    |2       |1.0       |[0.4332562889945694,0.5383477044581645,0.02820339310299354,4.7680883924915675E-5,1.4493256034768008E-4]   |\n",
      "|218    |3       |1.0       |[0.3359698582528477,0.6369891784233411,0.026901292299005102,2.1899926107946083E-5,1.177710986981643E-4]   |\n",
      "|219    |2       |1.0       |[0.45690581270052444,0.5122862030812881,0.029847519504979995,7.773867405381959E-4,1.830779726692079E-4]   |\n",
      "|220    |2       |0.0       |[0.4877002416284698,0.48134663215004375,0.029746278336328105,0.0010126965597591622,1.9415132539925834E-4] |\n",
      "|221    |3       |1.0       |[0.4637738034906849,0.5061780798607999,0.02837820782766582,0.0014772837661936452,1.926250546556863E-4]    |\n",
      "|222    |3       |1.0       |[0.45678919427674636,0.512495332761229,0.02922104439406703,0.0013113740945133604,1.830544734441706E-4]    |\n",
      "|223    |3       |1.0       |[0.3121972219206526,0.6785975913728675,0.00817386813237131,9.12547238168291E-4,1.1877133594034385E-4]     |\n",
      "|224    |3       |1.0       |[0.3854830152971634,0.5949311932150806,0.018531654354629192,9.116318586126936E-4,1.4250527451413052E-4]   |\n",
      "|225    |2       |0.0       |[0.6597179857321415,0.3118949841167104,0.021977870826198852,0.006152614264652798,2.5654506029635564E-4]   |\n",
      "|226    |2       |1.0       |[0.42198569737406594,0.5590024879347243,0.0185666882170513,2.9195269153976577E-4,1.531737826187512E-4]    |\n",
      "|227    |2       |1.0       |[0.4034750043649924,0.5771555158186433,0.01894082534093773,2.836647333692383E-4,1.4498974205719135E-4]    |\n",
      "|228    |3       |1.0       |[0.37272475618927525,0.6124734898118982,0.014505621051014847,1.659692840800786E-4,1.3016366373171814E-4]  |\n",
      "|229    |3       |1.0       |[0.32386514134480743,0.6621340641614767,0.013703949046196585,1.8385960287405805E-4,1.1298584464509124E-4] |\n",
      "|230    |2       |1.0       |[0.3809551523072103,0.5977763973291933,0.020997727517217755,1.3686800951902284E-4,1.338548368595649E-4]   |\n",
      "|231    |2       |1.0       |[0.36291821205712166,0.6190736007145544,0.01764476943860888,2.3585107644340413E-4,1.275667132714924E-4]   |\n",
      "|232    |3       |1.0       |[0.31821958882593115,0.6618309459477228,0.019749527390064386,8.235572990806282E-5,1.1758210637352349E-4]  |\n",
      "|233    |2       |1.0       |[0.3762787220071805,0.605634441777597,0.017404835537945865,5.446758631797726E-4,1.3732481409691033E-4]    |\n",
      "|234    |2       |1.0       |[0.4306278017895913,0.5431080149091407,0.02604335497805648,7.462637895456101E-5,1.4620194425692045E-4]    |\n",
      "|235    |2       |1.0       |[0.36915723729522937,0.6030547935364283,0.027587611010072777,7.467298977356E-5,1.2568516849582165E-4]     |\n",
      "|236    |2       |1.0       |[0.39114549904916246,0.5788638005894656,0.029732808838066616,1.2529948086807745E-4,1.3259204243725473E-4] |\n",
      "|237    |2       |1.0       |[0.36732272276489975,0.6067520171953406,0.025723963296279972,6.937988807133677E-5,1.3191685540852612E-4]  |\n",
      "|238    |2       |1.0       |[0.3238980893804726,0.6442428381078931,0.031677544207891734,5.51415325995983E-5,1.2638677114280787E-4]    |\n",
      "|239    |2       |1.0       |[0.3664186413713931,0.6160483083090571,0.016973369484784975,4.184736162919865E-4,1.4120721847284175E-4]   |\n",
      "|240    |2       |1.0       |[0.3828261111338258,0.597929961970168,0.018652170334780907,4.483838393668048E-4,1.4337272185854395E-4]    |\n",
      "|241    |2       |1.0       |[0.35666640791367565,0.6277069959464402,0.015158478977954968,3.2883565390378456E-4,1.3928150802540576E-4] |\n",
      "|242    |2       |1.0       |[0.35972125110182174,0.6201024746981093,0.019928152971892935,1.1716802451634677E-4,1.309532036595447E-4]  |\n",
      "|243    |3       |1.0       |[0.3345162175016706,0.6516648711178635,0.013615754415316511,8.567168736530241E-5,1.1748527778402788E-4]   |\n",
      "|244    |2       |1.0       |[0.42638393097340255,0.5431195200673332,0.030209012381374002,1.3956878162616412E-4,1.4796779626406634E-4] |\n",
      "|245    |3       |1.0       |[0.4136133806620637,0.5578001202107363,0.02826183680389573,1.853552218313284E-4,1.3930710147280783E-4]    |\n",
      "|246    |3       |1.0       |[0.39756699985597155,0.5720447658175251,0.03009715555897675,1.625575900029234E-4,1.2852117752372517E-4]   |\n",
      "|247    |2       |1.0       |[0.48095738858453074,0.48817608584105243,0.029612926034461598,0.0010548702933075704,1.987292466477885E-4] |\n",
      "|248    |2       |1.0       |[0.40472620960851413,0.5718134767538423,0.023012704344010437,3.081332874416278E-4,1.3947600619137175E-4]  |\n",
      "|249    |2       |1.0       |[0.4266345554701851,0.5439763320186654,0.02917291403824679,8.62813468245479E-5,1.2991712607829538E-4]     |\n",
      "|250    |3       |1.0       |[0.4118034783794862,0.5591809355908828,0.02876117212051474,1.2791990335736398E-4,1.264940057588546E-4]    |\n",
      "|251    |3       |1.0       |[0.45382735285860965,0.519297778647889,0.02594811720037827,7.477693239795246E-4,1.7898196914345867E-4]    |\n",
      "|252    |3       |1.0       |[0.4506816548334559,0.5225259030793006,0.02593828295867363,6.779495581662843E-4,1.7620957040369713E-4]    |\n",
      "|253    |2       |0.0       |[0.48877028542293016,0.4824624331631921,0.027929998522253804,6.570468227679949E-4,1.8023606885588937E-4]  |\n",
      "|254    |3       |1.0       |[0.44776764288161686,0.5242568449440439,0.027076815130946574,7.277937880545188E-4,1.709032553380537E-4]   |\n",
      "|255    |2       |1.0       |[0.38313276868662843,0.5954970864015052,0.02004700024587434,0.0011667187803284314,1.5642588566370695E-4]  |\n",
      "|256    |3       |1.0       |[0.4476496328516808,0.5245355569811985,0.026947390889396315,6.944336884113771E-4,1.7298558931310725E-4]   |\n",
      "|257    |2       |1.0       |[0.3013338206268938,0.6839395772486228,0.014388547008260255,2.2286967809314004E-4,1.1518543813003083E-4]  |\n",
      "|258    |2       |1.0       |[0.30133194309751193,0.683941553183818,0.01438844946628298,2.2286923461318193E-4,1.1518501777392487E-4]   |\n",
      "|259    |2       |1.0       |[0.44584939280731756,0.5158244015600774,0.03753284403261439,6.124388834117845E-4,1.809227165790305E-4]    |\n",
      "|260    |2       |1.0       |[0.419742947611207,0.5653947105006619,0.013865050963197125,8.318734251805268E-4,1.6541749975346854E-4]    |\n",
      "|261    |2       |1.0       |[0.29892022735605317,0.6874858528248088,0.013277985887127745,1.9819007474218382E-4,1.1774385726824322E-4] |\n",
      "|262    |2       |0.0       |[0.7074432903308423,0.25894962691670886,0.029595139838733586,0.0037273683350698217,2.845745786454388E-4]  |\n",
      "|263    |2       |1.0       |[0.48396875154847413,0.48831627479335055,0.026873601361835947,6.553757433898581E-4,1.859965529495441E-4]  |\n",
      "|264    |3       |0.0       |[0.7240996619039519,0.2416797176430822,0.03028853254655897,0.003651552464205168,2.8053544220177097E-4]    |\n",
      "|265    |3       |1.0       |[0.4220437724144668,0.5633348996377374,0.013774830830695743,6.800733130520984E-4,1.664238040479813E-4]    |\n",
      "|266    |2       |1.0       |[0.36150219437171915,0.6223865019158636,0.015683564814379877,2.856827623731646E-4,1.4205613566413002E-4]  |\n",
      "|267    |2       |0.0       |[0.6281403979224179,0.3410966824676165,0.02988626648486198,6.289808135547977E-4,2.4767231154880617E-4]    |\n",
      "|268    |2       |1.0       |[0.39422580599434787,0.5871211890300942,0.018089407191057037,4.1349223624089924E-4,1.5010554825984387E-4] |\n",
      "|269    |2       |1.0       |[0.46137292508866923,0.510000390686781,0.02733652661038551,0.00109994040064137,1.902172135227332E-4]      |\n",
      "|270    |3       |1.0       |[0.3625597638655504,0.6189863794697682,0.01773738825391172,5.78864795250002E-4,1.3760361551967178E-4]     |\n",
      "|271    |3       |1.0       |[0.299898611803517,0.6879599980358448,0.010742445747194749,0.0012728322708460687,1.2611214259718756E-4]   |\n",
      "|272    |3       |1.0       |[0.2952803304430453,0.6935583688857598,0.01004724972678673,9.944862378366431E-4,1.195647065714846E-4]     |\n",
      "|273    |2       |1.0       |[0.3162464334930305,0.6705098019284339,0.01177766811338713,0.0013342725190152727,1.3182394613323807E-4]   |\n",
      "|274    |3       |1.0       |[0.4175189621973998,0.5629009312918163,0.018654046918719868,7.805914084200874E-4,1.454681836439753E-4]    |\n",
      "|275    |3       |1.0       |[0.293442018639346,0.6945257251938242,0.010862269439110776,0.0010520371338020446,1.1794959391699961E-4]   |\n",
      "|276    |3       |1.0       |[0.35431068023485984,0.6297280783869283,0.015241594499536089,5.942512880359552E-4,1.2539559063982054E-4]  |\n",
      "|277    |3       |1.0       |[0.35431169665983214,0.6297270134158687,0.015241642403067681,5.942517327102815E-4,1.2539578852139286E-4]  |\n",
      "|278    |2       |1.0       |[0.3840078035824154,0.595613843108661,0.019800873197519893,4.472424796883566E-4,1.3023763171544353E-4]    |\n",
      "|279    |2       |1.0       |[0.37354497899876093,0.608258494246237,0.017665031845471815,4.006270993679575E-4,1.308678101622758E-4]    |\n",
      "|280    |3       |1.0       |[0.4458755584782535,0.5372098634867366,0.015670363645821476,0.0010756853338064294,1.6852905538206373E-4]  |\n",
      "|281    |2       |1.0       |[0.35753530556383634,0.6246774036664521,0.01763745524658551,4.2675829596971225E-5,1.0715969352919243E-4]  |\n",
      "|282    |2       |0.0       |[0.4939329064990572,0.4757999069034139,0.029591252591927217,4.97622712527272E-4,1.7831129307423446E-4]    |\n",
      "|283    |2       |0.0       |[0.4940279808532227,0.47569839868899355,0.029597670751155974,4.976245768650706E-4,1.7832512976272147E-4]  |\n",
      "|284    |3       |1.0       |[0.42781936639182655,0.5436868275803318,0.028235947149068158,1.184850407495477E-4,1.3937383802389988E-4]  |\n",
      "|285    |2       |1.0       |[0.4751059363397226,0.4944024319066221,0.028840997894216118,0.0014603877628420077,1.9024609659726793E-4]  |\n",
      "|286    |2       |1.0       |[0.4172023803203052,0.5640864606090353,0.017707660036275923,8.492132775475486E-4,1.5428575683599586E-4]   |\n",
      "|287    |2       |1.0       |[0.3979801275751925,0.5843616444168032,0.01656430415022969,9.359830707789478E-4,1.57940786995615E-4]      |\n",
      "|288    |3       |1.0       |[0.3369280512801209,0.6482765365354988,0.014366007280794728,3.0490497328694414E-4,1.2449993029866558E-4]  |\n",
      "|289    |2       |1.0       |[0.35072270116231097,0.6397395682229248,0.008407234477363424,9.921895903478983E-4,1.3830654705297376E-4]  |\n",
      "|290    |2       |1.0       |[0.40330502812377644,0.5791599644350608,0.016660257667274143,7.241731570484776E-4,1.5057661684024845E-4]  |\n",
      "|291    |3       |1.0       |[0.37956695926210887,0.6036968099094517,0.01591519994327044,6.729839927976789E-4,1.4804689237139465E-4]   |\n",
      "|292    |2       |1.0       |[0.3777457677536987,0.6043303366019032,0.01732058411183381,4.642692555258413E-4,1.3904227703852624E-4]    |\n",
      "|293    |2       |1.0       |[0.35523274212632394,0.6260486790512553,0.01829850183515712,2.9564925317189554E-4,1.2442773409193666E-4]  |\n",
      "|294    |3       |1.0       |[0.3047994443782436,0.6771659868790944,0.017853789751184478,6.625874334941437E-5,1.1452024812810036E-4]   |\n",
      "|295    |2       |1.0       |[0.36945414669883175,0.6054251882578288,0.02492158712001653,6.612406315716761E-5,1.329538601657174E-4]    |\n",
      "|296    |2       |0.0       |[0.6369356061471964,0.32854023246420416,0.0338737718814231,4.2745738023780685E-4,2.2293212693854766E-4]   |\n",
      "|297    |3       |1.0       |[0.33827498874614303,0.6467004107638175,0.014520247339385926,3.769518395297861E-4,1.2740131112365618E-4]  |\n",
      "|298    |2       |1.0       |[0.413144487873207,0.5578249762295412,0.028821325656523143,6.964374458127977E-5,1.3956649614724277E-4]    |\n",
      "|299    |2       |1.0       |[0.43827286784608965,0.5325384661042321,0.02895211912480068,8.398108619350651E-5,1.5256583868411897E-4]   |\n",
      "|300    |2       |1.0       |[0.4723083209210229,0.49750139107669267,0.029505998772394636,4.929431176608568E-4,1.9134611222911105E-4]  |\n",
      "|301    |3       |1.0       |[0.435202475125622,0.5362222280344436,0.027829951106073548,5.663429488433929E-4,1.7900278501727093E-4]    |\n",
      "|302    |3       |1.0       |[0.4351545774372626,0.5358919523849845,0.028141047157458215,6.344304842369643E-4,1.7799253605765467E-4]   |\n",
      "|303    |3       |1.0       |[0.4028845233987697,0.578327721236953,0.01742395417481856,0.0012052545048709875,1.5854668458755095E-4]    |\n",
      "|304    |3       |1.0       |[0.3781582726187839,0.6032836145149081,0.017245574757819387,0.0011673405649844201,1.4519754350417535E-4]  |\n",
      "|305    |2       |1.0       |[0.19272355930618087,0.7974044401522664,0.009562404868470651,2.2717351609508147E-4,8.242215698697078E-5]  |\n",
      "|306    |2       |1.0       |[0.40674329952200206,0.5733582328127689,0.01875471899642449,9.894607764002368E-4,1.542878924044421E-4]    |\n",
      "|307    |3       |1.0       |[0.41165551249158183,0.5574905048967778,0.030484660745691335,2.3344807280616721E-4,1.358737931428014E-4]  |\n",
      "|308    |3       |1.0       |[0.37379830559423327,0.5849905871914787,0.040831204898016754,2.4809588774980405E-4,1.318064285216854E-4]  |\n",
      "|309    |3       |1.0       |[0.40856598583738385,0.5606332930102652,0.030455500382886972,2.1152882179320027E-4,1.3369194767093808E-4] |\n",
      "|310    |3       |1.0       |[0.3708366816930443,0.5880339796293769,0.04077499835998407,2.247054503305533E-4,1.2963486726420392E-4]    |\n",
      "|311    |2       |0.0       |[0.6675198269915313,0.2960137513189047,0.035633946174259176,6.001516321431917E-4,2.3232388316155473E-4]   |\n",
      "|312    |2       |1.0       |[0.40067495603843867,0.5682942220494609,0.030691340208417222,2.0709643470907834E-4,1.323852689742702E-4]  |\n",
      "|313    |3       |1.0       |[0.3976878308889969,0.5721531352786225,0.029857642813278847,1.6969016789028757E-4,1.3170085121156494E-4]  |\n",
      "|314    |3       |1.0       |[0.4071815027130484,0.5626835846962817,0.029825756056198156,1.7510264839651005E-4,1.340538860754092E-4]   |\n",
      "|315    |2       |0.0       |[0.6889376977793729,0.2744105920739444,0.035965164458252756,4.5234370121470746E-4,2.3420198721510287E-4]  |\n",
      "|316    |2       |1.0       |[0.4422797817042373,0.5283028615494477,0.029192979500136156,6.981530560496064E-5,1.5456194057386865E-4]   |\n",
      "|317    |2       |1.0       |[0.390733804482118,0.573264940200854,0.035816806438151175,5.611412198988092E-5,1.2833475688718397E-4]     |\n",
      "|318    |2       |1.0       |[0.47105368219364163,0.49718030120559514,0.030175183153423907,0.0014005651857353805,1.9026826160383305E-4]|\n",
      "|319    |2       |1.0       |[0.4589121595423215,0.5106916214232802,0.029318489572821453,9.018155590035186E-4,1.7591390257348788E-4]   |\n",
      "|320    |2       |1.0       |[0.4609211064219027,0.506972562959553,0.03103359897084857,8.976196004286399E-4,1.7511204726718447E-4]     |\n",
      "|321    |2       |1.0       |[0.4609208939039102,0.5069727915569429,0.03103358296018554,8.976195672309963E-4,1.7511201173048914E-4]    |\n",
      "|322    |3       |1.0       |[0.33740534156716623,0.6382423040707219,0.02318531232023518,0.0010303304155045896,1.367116263721893E-4]   |\n",
      "|323    |2       |1.0       |[0.40921454345835634,0.5709669232451783,0.01854944383714726,0.0011168439791385866,1.5224548017944066E-4]  |\n",
      "|324    |2       |1.0       |[0.4381345838884306,0.5430807940193128,0.018224197241467087,3.9114748315039074E-4,1.6927736763921376E-4]  |\n",
      "|325    |2       |1.0       |[0.4057135493797298,0.5653946384916405,0.028677993439340525,7.905301084269038E-5,1.3476567844643265E-4]   |\n",
      "|326    |2       |0.0       |[0.6407498621660335,0.33296320562098114,0.023640042073799772,0.002401331294266552,2.4555884491904743E-4]  |\n",
      "|327    |2       |1.0       |[0.39706098365179415,0.5715488038380367,0.031134517344003815,1.3405767216860794E-4,1.2163749399650677E-4] |\n",
      "|328    |3       |1.0       |[0.397061396294686,0.5715483553346571,0.031134553115265275,1.3405769778243953E-4,1.2163755760945385E-4]   |\n",
      "|329    |3       |0.0       |[0.6654036487605954,0.2995836592819786,0.031179669331355294,0.0035813415144204334,2.5168111165014336E-4]  |\n",
      "|330    |2       |1.0       |[0.46355692871544274,0.5030468052610724,0.03239039141714322,8.34635142746647E-4,1.7123946359493496E-4]    |\n",
      "|331    |2       |1.0       |[0.45028318174644993,0.5185210114800602,0.030023909399333328,9.941499832764873E-4,1.7774739088013665E-4]  |\n",
      "|332    |2       |0.0       |[0.7068643423962137,0.2518072393062401,0.03541915148146879,0.005637219707230174,2.720471088472527E-4]     |\n",
      "|333    |2       |1.0       |[0.3020785598526215,0.6908469208992601,0.00487938661138534,0.002084676600445999,1.1045603628688997E-4]    |\n",
      "|334    |2       |1.0       |[0.20276503105491242,0.7896847687183599,0.007238438759893373,2.383710237993043E-4,7.33904430350085E-5]    |\n",
      "|335    |2       |1.0       |[0.1800865333428724,0.8101129606517784,0.009483154153187555,2.477265101455813E-4,6.962534201597709E-5]    |\n",
      "|336    |2       |1.0       |[0.25135364752478084,0.7407923781612051,0.00724701114999212,5.080254845837115E-4,9.893767943819272E-5]    |\n",
      "|337    |2       |1.0       |[0.4033382354563463,0.5765227344604293,0.019101845997624666,8.879830393779185E-4,1.4920104622178967E-4]   |\n",
      "|338    |2       |0.0       |[0.6522305702847092,0.32358568891034506,0.021747896734761178,0.0022054744231649556,2.3036964701964464E-4] |\n",
      "|339    |2       |0.0       |[0.6559998372621048,0.31873946926267066,0.02419472155816334,8.432922425017147E-4,2.2267967455961545E-4]   |\n",
      "|340    |2       |1.0       |[0.4016350337480385,0.5685341205092249,0.029505169267472403,1.9438807433356769E-4,1.3128840093062813E-4]  |\n",
      "|341    |2       |1.0       |[0.4067033397690873,0.5620850125756022,0.030939561968820434,1.379973310915068E-4,1.3408835539856006E-4]   |\n",
      "|342    |3       |1.0       |[0.3066159063121825,0.6856794555919878,0.007276296045886388,3.2865855287875044E-4,9.968349706444437E-5]   |\n",
      "|343    |3       |1.0       |[0.4167602646700546,0.5519165314925752,0.031071023276456142,1.1831843230927787E-4,1.338621286046457E-4]   |\n",
      "|344    |2       |1.0       |[0.306703598918866,0.6855894523842587,0.007278559693994069,3.286887711849066E-4,9.970023169635994E-5]     |\n",
      "|345    |2       |1.0       |[0.3101755581302432,0.6819810215999923,0.007457271345373001,2.867983160801034E-4,9.935060831137611E-5]    |\n",
      "|346    |2       |1.0       |[0.3656486939422816,0.5935936231149259,0.040466842655260876,1.671486716050638E-4,1.236916159264648E-4]    |\n",
      "|347    |2       |1.0       |[0.40520897028349345,0.5714771173182724,0.022979602922023216,1.9530539664525028E-4,1.3900407956559137E-4] |\n",
      "|348    |2       |1.0       |[0.40630245570722556,0.5706795642683604,0.02267681016195524,2.0306764263109615E-4,1.3810221982743645E-4]  |\n",
      "|349    |3       |0.0       |[0.6392646739012264,0.34110281456161795,0.01789849360954067,0.001487826599377039,2.46191328237957E-4]     |\n",
      "|350    |3       |0.0       |[0.6837022469810351,0.29324910735547643,0.021129163534811625,0.00166084380183355,2.5863832684338276E-4]   |\n",
      "|351    |2       |1.0       |[0.45797694365831254,0.5073737822959044,0.034357882950908095,1.3200790894744163E-4,1.593831859276442E-4]  |\n",
      "|352    |2       |1.0       |[0.4102010707273676,0.5679683778059849,0.021518599860753208,1.6886792391425994E-4,1.4308368198006713E-4]  |\n",
      "|353    |2       |1.0       |[0.40271457583370185,0.5816944411151574,0.015326012389283276,1.3410023806385737E-4,1.308704237934452E-4]  |\n",
      "|354    |2       |1.0       |[0.43942736542545346,0.5457834925056587,0.01451340110133291,1.3205887738711912E-4,1.4368209016763625E-4]  |\n",
      "|355    |2       |1.0       |[0.4442741980915669,0.5401792051497958,0.015348309485312465,6.65560949103349E-5,1.317311784146806E-4]     |\n",
      "|356    |2       |1.0       |[0.42950849035240757,0.5549156938535411,0.01539002453061261,6.261417932638311E-5,1.2317708411224845E-4]   |\n",
      "|357    |3       |1.0       |[0.4203531010666126,0.5566298790249211,0.022892533181121617,2.040356941465343E-6,1.2244637040330095E-4]   |\n",
      "|358    |2       |1.0       |[0.45659919288101253,0.5194685977596769,0.023779523560430442,1.7151129639100374E-5,1.3553466924111961E-4] |\n",
      "|359    |2       |1.0       |[0.39229336963585804,0.589272425321789,0.01822911037921044,9.12195687309683E-5,1.1387509441155627E-4]     |\n",
      "|360    |2       |1.0       |[0.3760555808845493,0.615459477170349,0.008188436534567342,1.7751537357810844E-4,1.1899003695633831E-4]   |\n",
      "|361    |2       |0.0       |[0.545564266366287,0.43168816117975195,0.020917340630236265,0.0016524705949395473,1.7776122878533017E-4]  |\n",
      "|362    |2       |1.0       |[0.30391684579225836,0.6799686906128598,0.015691304956898435,3.1393088950732947E-4,1.092277484759967E-4]  |\n",
      "|363    |2       |1.0       |[0.2915425125016329,0.6928611534173265,0.015271494862332816,2.1915069841101458E-4,1.0568852029688994E-4]  |\n",
      "|364    |2       |1.0       |[0.28266491193755133,0.7028019095681125,0.014287468747593327,1.4499849207818752E-4,1.0071125466458219E-4] |\n",
      "|365    |2       |1.0       |[0.2641713839774977,0.7200975147386542,0.015513838199258147,1.2959706549454502E-4,8.76660190952784E-5]    |\n",
      "|366    |2       |1.0       |[0.2904002529690154,0.693228109886553,0.01430088393640493,0.001970906626954914,9.984658107175376E-5]      |\n",
      "|367    |3       |1.0       |[0.29043211137422065,0.693194468840664,0.014302588905538352,0.0019709778169921337,9.98530625846695E-5]    |\n",
      "|368    |2       |1.0       |[0.2741224742664651,0.7089688870394323,0.014579948868862698,0.0022338708388443037,9.481898639579547E-5]   |\n",
      "|369    |2       |1.0       |[0.26791028181073734,0.7165116805339949,0.01370792073032312,0.0017790354431438224,9.108148180078722E-5]   |\n",
      "|370    |3       |1.0       |[0.2503085469228331,0.7360717219237822,0.012600870538724109,9.355080491306717E-4,8.335256552971156E-5]    |\n",
      "|371    |2       |1.0       |[0.2656428787499795,0.7186334518258359,0.014618941693160963,0.0010186851372038974,8.604259381959673E-5]   |\n",
      "|372    |2       |1.0       |[0.2656428787499795,0.7186334518258359,0.014618941693160963,0.0010186851372038974,8.604259381959673E-5]   |\n",
      "|373    |2       |1.0       |[0.2657211877380715,0.7185503512047325,0.014623612104539618,0.0010187909914612105,8.605796119498368E-5]   |\n",
      "|374    |2       |1.0       |[0.26313760045609075,0.7212800375802039,0.014576620602606035,9.212447848117564E-4,8.4496576287613E-5]     |\n",
      "|375    |2       |1.0       |[0.2655175822049675,0.7189373882814655,0.014638301058592696,8.233466431809904E-4,8.3381811793344E-5]      |\n",
      "|376    |2       |1.0       |[0.26331849338054547,0.7213766488840083,0.014554052995333805,6.68939041961422E-4,8.186569815087919E-5]    |\n",
      "|377    |2       |1.0       |[0.26544779308778327,0.719650572304866,0.014378787726741794,4.3626898372180445E-4,8.657789688699725E-5]   |\n",
      "|378    |2       |1.0       |[0.2749035542940257,0.7094162771069565,0.015150538775187142,4.435248231280825E-4,8.610500070266898E-5]    |\n",
      "|379    |2       |0.0       |[0.5643476055117793,0.40212938007202376,0.020100265309021117,0.013230011189229085,1.9273791794656707E-4]  |\n",
      "|380    |2       |1.0       |[0.30148080380219056,0.6810853343120475,0.015246947924092284,0.0020864376928101595,1.004762688594734E-4]  |\n",
      "|381    |2       |1.0       |[0.2689704906462108,0.7169961542000115,0.013001548478209888,9.444691275861805E-4,8.733754798166476E-5]    |\n",
      "|382    |2       |1.0       |[0.27618240665687,0.7093192044031345,0.013737178534386675,6.725005754851593E-4,8.870983012359213E-5]      |\n",
      "|383    |2       |0.0       |[0.5342851916482569,0.44247812769430417,0.02054201794928546,0.0025191407985790054,1.7552190957441976E-4]  |\n",
      "|384    |3       |1.0       |[0.29166859709328424,0.6839567764759807,0.02410805499897435,1.8054999434929258E-4,8.602143741146284E-5]   |\n",
      "|385    |3       |1.0       |[0.37660317838452284,0.5950000262614047,0.026826546808288613,0.001422538232816364,1.4771031296751996E-4]  |\n",
      "|386    |2       |1.0       |[0.3010783851051441,0.6820745391626113,0.01518176959652298,0.0015609673168436968,1.0433881887805928E-4]   |\n",
      "|387    |2       |1.0       |[0.29840521532919645,0.6849335487899124,0.015146306238754476,0.0014124099221003068,1.0251972003641156E-4] |\n",
      "|388    |2       |1.0       |[0.29834754728319074,0.6849945001698109,0.015143122119234497,0.001412322330739841,1.0250809702395631E-4]  |\n",
      "|389    |2       |1.0       |[0.29106596958391534,0.6927371642583773,0.014446183305544468,0.0016506636941591827,1.0001915800360898E-4] |\n",
      "|390    |3       |1.0       |[0.2556110514519306,0.7304556456834478,0.012496986859028421,0.0013504198486281572,8.589615696523004E-5]   |\n",
      "|391    |2       |1.0       |[0.2689055814073418,0.7151208186009725,0.014314722582402848,0.0015679989722377462,9.087843704499147E-5]   |\n",
      "|392    |3       |1.0       |[0.26549261160521315,0.7188465412056746,0.01437684327413301,0.0011958324191456756,8.8171495833409E-5]     |\n",
      "|393    |2       |1.0       |[0.2769183813430749,0.7080287061888532,0.014585711832080022,3.7743374471401625E-4,8.976689127798599E-5]   |\n",
      "|394    |2       |1.0       |[0.2769183813430749,0.7080287061888532,0.014585711832080022,3.7743374471401625E-4,8.976689127798599E-5]   |\n",
      "|395    |2       |1.0       |[0.2769183813430749,0.7080287061888532,0.014585711832080022,3.7743374471401625E-4,8.976689127798599E-5]   |\n",
      "|396    |3       |1.0       |[0.30579548245174,0.669069829544788,0.024809533332228413,2.307802306680578E-4,9.437444057564247E-5]       |\n",
      "|397    |2       |1.0       |[0.37782872636924947,0.5941923985844227,0.02604905121629767,0.0017836964189234478,1.4612741110679844E-4]  |\n",
      "|398    |2       |1.0       |[0.2926756641049214,0.6898976024477661,0.015151765848696135,0.0021708882713955685,1.0407932722069053E-4]  |\n",
      "|399    |2       |1.0       |[0.2745146473021415,0.7053855923126701,0.018760199592899124,0.0012323947894852464,1.0716600280420297E-4]  |\n",
      "|400    |3       |1.0       |[0.29113049929935514,0.6930929471389323,0.014446937187553032,0.0012271326061618355,1.0248376799776624E-4] |\n",
      "|401    |3       |1.0       |[0.2844406442614569,0.7001159224428658,0.014067833733786495,0.0012767661959028097,9.8833365988059E-5]     |\n",
      "|402    |3       |1.0       |[0.28444754338755024,0.7001086409629071,0.014068204267766254,0.0012767765880689747,9.883479370737224E-5]  |\n",
      "|403    |2       |1.0       |[0.2709490720335723,0.7144104210392221,0.013446938591288959,0.0011001553503036069,9.341298561299571E-5]   |\n",
      "|404    |2       |1.0       |[0.2698993176667399,0.7146370410690379,0.01407834340455204,0.001292853215631472,9.244464403859803E-5]     |\n",
      "|405    |2       |1.0       |[0.2673751894546593,0.7173260334595679,0.014038705298058438,0.0011692809633845835,9.079082432977714E-5]   |\n",
      "|406    |3       |1.0       |[0.23447419053765164,0.7529489652146999,0.011738966443809047,7.599706802639078E-4,7.790712357551122E-5]   |\n",
      "|407    |3       |1.0       |[0.2689325739865625,0.7164558739836091,0.013818268005580392,7.030359568207996E-4,9.024806742722919E-5]    |\n",
      "|408    |3       |1.0       |[0.2689325739865625,0.7164558739836091,0.013818268005580392,7.030359568207996E-4,9.024806742722919E-5]    |\n",
      "|409    |2       |1.0       |[0.2689325739865625,0.7164558739836091,0.013818268005580392,7.030359568207996E-4,9.024806742722919E-5]    |\n",
      "|410    |3       |1.0       |[0.27130111639221,0.7139139587467777,0.014115460552497672,5.794558668703523E-4,9.000844164418156E-5]      |\n",
      "|411    |3       |1.0       |[0.24642009960451514,0.7340205427195732,0.019068589971602375,4.043542642401394E-4,8.641344006936958E-5]   |\n",
      "|412    |2       |1.0       |[0.37508754525113286,0.597506757373578,0.026136811557119978,0.0011218299696232103,1.4705584854603074E-4]  |\n",
      "|413    |3       |1.0       |[0.37562228407047,0.5967974873539463,0.02643723420126643,0.0010006191451932076,1.423752291240957E-4]      |\n",
      "|414    |2       |1.0       |[0.32904412218615364,0.6533850133794904,0.016193974822105748,0.0012518836741632607,1.2500593808685069E-4] |\n",
      "|415    |3       |1.0       |[0.34110641050382884,0.6397521660970149,0.017328808709379553,0.001686199159048975,1.264155307276021E-4]   |\n",
      "|416    |3       |1.0       |[0.33278821552328275,0.6502478380821116,0.015428516887431541,0.0014063588398043837,1.2907066736991195E-4] |\n",
      "|417    |2       |1.0       |[0.3261114895758328,0.6536952863173187,0.01838714026432094,0.0016889636176144628,1.1712022491305753E-4]   |\n",
      "|418    |2       |1.0       |[0.2611119852163209,0.7234424000418438,0.013525721909484194,0.0018309709732422786,8.892185910881352E-5]   |\n",
      "|419    |2       |1.0       |[0.27066237373862434,0.6955873945697549,0.03338494986647027,2.7187503897532376E-4,9.340678617503267E-5]   |\n",
      "|420    |2       |1.0       |[0.317534152854106,0.662274033871557,0.01688069655713944,0.003193649552962534,1.1746716423491006E-4]      |\n",
      "|421    |2       |1.0       |[0.30022388571906344,0.6802820904490925,0.016788520618529517,0.0026039947797945984,1.0150843351999814E-4] |\n",
      "|422    |2       |1.0       |[0.3002098185175354,0.6802970552146838,0.016787664470583823,0.002603956141677545,1.0150565551941551E-4]   |\n",
      "|423    |2       |1.0       |[0.29759040687590416,0.6832023068128066,0.016751118486821825,0.00235641858832976,9.974923613755233E-5]    |\n",
      "|424    |3       |1.0       |[0.24036353516670558,0.7452089940826884,0.011825827617959885,0.0025154746860051804,8.616844664118886E-5]  |\n",
      "|425    |3       |1.0       |[0.2700024162911871,0.7137875287804728,0.013915892664910693,0.0022018674472593577,9.22948161701207E-5]    |\n",
      "|426    |2       |1.0       |[0.2700024162911871,0.7137875287804728,0.013915892664910693,0.0022018674472593577,9.22948161701207E-5]    |\n",
      "|427    |2       |1.0       |[0.2700024162911871,0.7137875287804728,0.013915892664910693,0.0022018674472593577,9.22948161701207E-5]    |\n",
      "|428    |2       |1.0       |[0.2573945353972217,0.7275496333158492,0.013899293890779008,0.0010703434399757948,8.61939561740666E-5]    |\n",
      "|429    |2       |0.0       |[0.5108809438644293,0.4615150071520411,0.021473669129562153,0.005966318219743204,1.6406163422426817E-4]   |\n",
      "|430    |2       |1.0       |[0.26476246450659485,0.7195284804806624,0.014923562793885814,7.011242848941618E-4,8.436793396262493E-5]   |\n",
      "|431    |2       |1.0       |[0.2604267410219354,0.7243256727345505,0.014565947637154091,5.9515933614835E-4,8.647927021172089E-5]      |\n",
      "|432    |2       |1.0       |[0.3634945471813738,0.6086539194159089,0.025919095292418457,0.0017941816182181626,1.3825649208079686E-4]  |\n",
      "|433    |2       |1.0       |[0.3634945471813738,0.6086539194159089,0.025919095292418457,0.0017941816182181626,1.3825649208079686E-4]  |\n",
      "|434    |3       |1.0       |[0.29083464798639613,0.6922844265491103,0.014746383981777357,0.0020289917343033217,1.0554974841284579E-4] |\n",
      "|435    |3       |1.0       |[0.27279102443262526,0.7122342249706777,0.013206416786265862,0.0016736571938790888,9.467661655211947E-5]  |\n",
      "|436    |2       |1.0       |[0.2658473211542639,0.7184970772347977,0.013847508581974114,0.0017171230542582495,9.096997470610793E-5]   |\n",
      "|437    |3       |1.0       |[0.25928907258052314,0.72523338950189,0.014437191497663678,9.563891839427624E-4,8.395723598036576E-5]     |\n",
      "|438    |3       |1.0       |[0.26980256007519754,0.7135939539836819,0.015698161409480004,8.196616588985787E-4,8.566287274206797E-5]   |\n",
      "|439    |2       |1.0       |[0.28524928044756215,0.6974674645295776,0.014851400533398444,0.002335084063333731,9.67704261278705E-5]    |\n",
      "|440    |2       |1.0       |[0.2563356586744325,0.7284771115792205,0.013507754600696465,0.0015934244569412743,8.60506887095185E-5]    |\n",
      "|441    |2       |1.0       |[0.3099813491995408,0.6729387995604234,0.015305828571973585,0.0016639822819128297,1.1004038614934497E-4]  |\n",
      "|442    |2       |1.0       |[0.26529055218376313,0.7198198724862065,0.013176787543964745,0.0016182079963365302,9.457978972902361E-5]  |\n",
      "|443    |3       |1.0       |[0.2706756868319662,0.7147032316669956,0.013338174987034657,0.0011915218196703735,9.138469433321553E-5]   |\n",
      "|444    |2       |0.0       |[0.5281191734105857,0.44636279600090395,0.01997148579850528,0.005370682745394813,1.7586204461020194E-4]   |\n",
      "|445    |2       |0.0       |[0.5281191734105857,0.44636279600090395,0.01997148579850528,0.005370682745394813,1.7586204461020194E-4]   |\n",
      "|446    |2       |1.0       |[0.26812002888362974,0.7160628568298366,0.015184130263225327,5.457857603605497E-4,8.71982629476079E-5]    |\n",
      "|447    |3       |1.0       |[0.27700633354676685,0.7009837533977114,0.021671616908132997,2.5040963305933666E-4,8.788651432942769E-5]  |\n",
      "|448    |2       |1.0       |[0.3102308053533859,0.6628136951616334,0.026596765340727153,2.609280200979062E-4,9.780612415568762E-5]    |\n",
      "|449    |2       |1.0       |[0.3007763809038893,0.6829372759526916,0.014778300284314143,0.0014091378164136665,9.890504269113847E-5]   |\n",
      "|450    |3       |1.0       |[0.2933095243867215,0.6919006070953497,0.014175658464797201,5.241672653772319E-4,9.004278775430676E-5]    |\n",
      "|451    |3       |1.0       |[0.3349939836900015,0.6379986189024943,0.026728814596045765,1.7506500991390175E-4,1.0351780154451237E-4]  |\n",
      "|452    |3       |1.0       |[0.3913577236353172,0.5769807261221455,0.030384597267881827,0.0011395546716032672,1.3739830305212845E-4]  |\n",
      "|453    |3       |1.0       |[0.2996177103960556,0.6831218118599803,0.01598061404186643,0.0011745148816859558,1.0534882041167632E-4]   |\n",
      "|454    |2       |1.0       |[0.31398306142934285,0.6688957915874636,0.015802005732264545,0.001214361172888818,1.0478007804014457E-4]  |\n",
      "|455    |2       |0.0       |[0.560505026307019,0.4132259003389905,0.01984894796073382,0.006238913901091577,1.8121149216521613E-4]     |\n",
      "|456    |3       |1.0       |[0.2983907113121842,0.6854107773829484,0.01528970891731132,8.185798703114009E-4,9.02225172446408E-5]      |\n",
      "|457    |2       |1.0       |[0.2957824862698534,0.6881317639542472,0.015256442442944934,7.406545173439296E-4,8.865281561046162E-5]    |\n",
      "|458    |3       |1.0       |[0.29724260656656326,0.6868508454450872,0.015340809501926636,4.7253535640817896E-4,9.320313001466507E-5]  |\n",
      "|459    |3       |1.0       |[0.292022781266512,0.691820362508723,0.015766731414841407,3.017432876426096E-4,8.838152228107079E-5]      |\n",
      "|460    |3       |1.0       |[0.3324969403803181,0.6414825067166211,0.025726647946936958,1.9017649396601245E-4,1.0372846215792333E-4]  |\n",
      "|461    |2       |1.0       |[0.29574079656845115,0.6695011453492964,0.034473446787074015,1.855694238941927E-4,9.904187128411596E-5]   |\n",
      "|462    |3       |1.0       |[0.39399571081277535,0.5773909072389544,0.02698442904881297,0.0014779947625178434,1.509581369393155E-4]   |\n",
      "|463    |3       |1.0       |[0.3572290605212917,0.6049672319876616,0.03608909910576388,0.0015683876638940483,1.4622072138867668E-4]   |\n",
      "|464    |3       |1.0       |[0.36704886495010614,0.6054216518023198,0.026113245785530557,0.0012818363286578288,1.3440113338588925E-4] |\n",
      "|465    |3       |1.0       |[0.36319328191864747,0.6095622853950999,0.026089857047810004,0.001020256856144968,1.343187822976764E-4]   |\n",
      "|466    |2       |1.0       |[0.38333926850604694,0.5869041227733768,0.02844228630643062,0.0011725686707354,1.4175374341026675E-4]     |\n",
      "|467    |2       |1.0       |[0.38333926850604694,0.5869041227733768,0.02844228630643062,0.0011725686707354,1.4175374341026675E-4]     |\n",
      "|468    |2       |0.0       |[0.6444763200295296,0.3151852240491795,0.034167766753550204,0.005933387211634405,2.3730195610630243E-4]   |\n",
      "|469    |3       |1.0       |[0.3188613148977046,0.6637792076589599,0.016136339856268705,0.001101977925816706,1.2115966125003393E-4]   |\n",
      "|470    |2       |1.0       |[0.3325294077840558,0.6495871185644889,0.016691823625069394,0.001078895827264488,1.1275419912137419E-4]   |\n",
      "|471    |2       |1.0       |[0.32569615946006897,0.6564284934093736,0.01651269550349582,0.001253875805742065,1.0877582131951945E-4]   |\n",
      "|472    |3       |1.0       |[0.28231503220303195,0.7030471164337714,0.013684322853822362,8.625897509821858E-4,9.093875839216214E-5]   |\n",
      "|473    |3       |1.0       |[0.2897920153936262,0.6957363964733443,0.013710170370159286,6.693990698833809E-4,9.201869298701133E-5]    |\n",
      "|474    |2       |1.0       |[0.28746567127209877,0.6965970842571388,0.015494923816821276,3.5169970631725075E-4,9.062094762394219E-5]  |\n",
      "|475    |3       |1.0       |[0.29707791415126683,0.6870875704536726,0.015413958830748893,3.270069986825973E-4,9.354956562906622E-5]   |\n",
      "|476    |2       |1.0       |[0.2969105854334725,0.687264431795218,0.015404516595583872,3.269475778505939E-4,9.3518597875044E-5]       |\n",
      "|477    |2       |1.0       |[0.3387972127402974,0.6327123973894896,0.02818737771467647,2.0435022325494693E-4,9.866193228153533E-5]    |\n",
      "|478    |3       |1.0       |[0.3387972127402974,0.6327123973894896,0.02818737771467647,2.0435022325494693E-4,9.866193228153533E-5]    |\n",
      "|479    |3       |1.0       |[0.32356799467752156,0.6605251656935535,0.01469496725117354,0.0010912257138885292,1.2064666386306524E-4]  |\n",
      "|480    |2       |1.0       |[0.29269475405452794,0.691406060533863,0.014894259437233862,9.127326834829746E-4,9.219329089234037E-5]    |\n",
      "|481    |2       |1.0       |[0.25851739789428363,0.727555838676552,0.012944280937688782,8.980620022101663E-4,8.442048926534656E-5]    |\n",
      "|482    |3       |1.0       |[0.28020953859233655,0.7059199084652965,0.013178369051463391,6.028155693681821E-4,8.936832153546034E-5]   |\n",
      "|483    |2       |1.0       |[0.2652406609710335,0.7148582209145425,0.019377506477088736,4.3382651735742584E-4,8.978511997758509E-5]   |\n",
      "|484    |2       |1.0       |[0.295553512680517,0.6893016464707378,0.014638156791616556,4.1303502704475355E-4,9.364903008389218E-5]    |\n",
      "|485    |3       |1.0       |[0.29390410696305497,0.689522316109148,0.01608032041419561,4.014021644178776E-4,9.185434918355707E-5]     |\n",
      "|486    |3       |1.0       |[0.2989577970525153,0.6785613302954443,0.022192946104941578,1.941779017382345E-4,9.374864536066508E-5]    |\n",
      "|487    |3       |1.0       |[0.3320403301784413,0.6407061361997677,0.026962533745166266,1.9082895738769433E-4,1.0017091923702524E-4]  |\n",
      "|488    |2       |1.0       |[0.3290128491813108,0.6452986257712439,0.025424811877441517,1.6132424184239324E-4,1.0238892816155939E-4]  |\n",
      "|489    |3       |1.0       |[0.34529540372589596,0.6287145687676791,0.025168199661368774,6.931904123330594E-4,1.286374327230797E-4]   |\n",
      "|490    |2       |0.0       |[0.5437630973025871,0.431673791194195,0.019065563133896245,0.005326001830931916,1.715465383897634E-4]     |\n",
      "|491    |2       |1.0       |[0.3011891612382484,0.6837546290688107,0.014520201349967898,4.441898596875777E-4,9.181848328550376E-5]    |\n",
      "|492    |3       |1.0       |[0.2981381246949382,0.6871196439517384,0.014324170286413945,3.27629346186081E-4,9.04317207234663E-5]      |\n",
      "|493    |2       |0.0       |[0.5988313850988178,0.36423368874306716,0.03491891446779906,0.0018277766757040217,1.8823501461188172E-4]  |\n",
      "|494    |2       |1.0       |[0.3969939950279192,0.5736054125627092,0.027606871798689137,0.001646785746532187,1.4693486415011834E-4]   |\n",
      "|495    |3       |1.0       |[0.3896217174088375,0.5824704850972225,0.02677621795690087,9.864416137369857E-4,1.4513792330197127E-4]    |\n",
      "|496    |2       |1.0       |[0.30055818264908973,0.6842767218678039,0.014523348458240664,5.468995608799267E-4,9.484746398594696E-5]   |\n",
      "|497    |2       |1.0       |[0.3845770704934909,0.5871552936239075,0.02682234207323162,0.0013033990113847389,1.418947979853507E-4]    |\n",
      "|498    |3       |1.0       |[0.33849554334575976,0.6421399253510575,0.017599431555090632,0.0016466312954783967,1.1846845261386854E-4] |\n",
      "|499    |2       |0.0       |[0.5890484766646031,0.3772142165902528,0.03215636163701124,0.0013999648745134043,1.8098023361949782E-4]   |\n",
      "|500    |3       |1.0       |[0.26995025061372635,0.714916949792477,0.013848138932730783,0.0011985814365257441,8.607922454011883E-5]   |\n",
      "|501    |2       |0.0       |[0.5346747319816906,0.43819528116080136,0.019421426738068372,0.007528645443872152,1.7991467556745816E-4]  |\n",
      "|502    |3       |1.0       |[0.2795243711574318,0.7054453041485527,0.01388923434101822,0.0010492021066151871,9.188824638225117E-5]    |\n",
      "|503    |3       |1.0       |[0.2900360067313939,0.6943465578245694,0.015037286164696403,4.887114315305591E-4,9.143784780982356E-5]    |\n",
      "|504    |2       |1.0       |[0.3324336495765817,0.6492267881508981,0.016849200068468365,0.0013667241610318855,1.236380430199127E-4]   |\n",
      "|505    |2       |1.0       |[0.33242892821820297,0.649231777475216,0.016848938592742498,0.0013667186608159175,1.236370530225641E-4]   |\n",
      "|506    |3       |1.0       |[0.310679765001417,0.6713315935853041,0.01667329929795465,0.001205286922790379,1.1005519253398644E-4]     |\n",
      "|507    |2       |1.0       |[0.32354018001876167,0.6586259589357149,0.01630271777060897,0.001422946847615419,1.0819642729912531E-4]   |\n",
      "|508    |2       |1.0       |[0.28874027664691326,0.6947631914804022,0.015246771971490401,0.001158485549448533,9.127435174561599E-5]   |\n",
      "|509    |2       |1.0       |[0.28749011332230484,0.6972008332348991,0.014277142681835471,9.377136795589113E-4,9.419708140160676E-5]   |\n",
      "|510    |2       |0.0       |[0.5373696155014848,0.43793519369625505,0.01965673065388948,0.004861074620942297,1.773855274282369E-4]    |\n",
      "|511    |2       |1.0       |[0.28080822413076784,0.7034569352074572,0.014946721973436201,6.97919658933812E-4,9.019902940501351E-5]    |\n",
      "|512    |2       |1.0       |[0.28698148195816114,0.6972719693989783,0.015155675107426821,5.000934568076174E-4,9.078007862622788E-5]   |\n",
      "|513    |2       |1.0       |[0.3416434062299301,0.6311299013006771,0.026885321676733062,2.3741864721827215E-4,1.0395214544142653E-4]  |\n",
      "|514    |2       |1.0       |[0.3178167806957463,0.6650005192831124,0.015864069840922177,0.001214347794045113,1.0428238617399071E-4]   |\n",
      "|515    |2       |1.0       |[0.3136917223597976,0.6694755523887387,0.01545040414168955,0.0012811401121817765,1.0118099759221783E-4]   |\n",
      "|516    |2       |1.0       |[0.29211039607134104,0.6924588545473047,0.0142316674542742,0.0011009679454589945,9.811398162110595E-5]    |\n",
      "|517    |2       |0.0       |[0.5410891737298702,0.4323191697256222,0.019617449068067606,0.006790764366230699,1.8344311020922083E-4]   |\n",
      "|518    |2       |1.0       |[0.38178175023166755,0.5889135580916134,0.027789487811318875,0.0013749425791581085,1.4026128624206922E-4] |\n",
      "|519    |2       |1.0       |[0.320028298201062,0.663150968007058,0.015389066791349566,0.0013200333119357715,1.1163368859461546E-4]    |\n",
      "|520    |2       |1.0       |[0.30110335433487545,0.6822434686753902,0.014893479312267236,0.0016620632371403838,9.763444032683042E-5]  |\n",
      "|521    |3       |1.0       |[0.2541140288782084,0.7324310646778832,0.012114980752987815,0.001256266665279153,8.365902564155494E-5]    |\n",
      "|522    |2       |1.0       |[0.28649764591484767,0.6992286274714242,0.013223916529883705,9.572912292420493E-4,9.251885460222114E-5]   |\n",
      "|523    |3       |1.0       |[0.2797123244919051,0.7060310264316929,0.013476884104500356,6.90181480184006E-4,8.958349171751616E-5]     |\n",
      "|524    |3       |1.0       |[0.2564737593632733,0.7304127044643243,0.01256211869348414,4.711181764124416E-4,8.029930250575569E-5]     |\n",
      "|525    |3       |1.0       |[0.29722818393189426,0.687835748191813,0.014405717604943871,4.390784525945578E-4,9.127181875431454E-5]    |\n",
      "|526    |3       |1.0       |[0.3068958321598969,0.6772732556701645,0.014118984173731168,0.0016066978088565024,1.0523018735084279E-4]  |\n",
      "|527    |3       |1.0       |[0.3068971589194695,0.677271860010704,0.01411905063251486,0.0016066999851801077,1.0523045213145933E-4]    |\n",
      "|528    |3       |1.0       |[0.26501487527654527,0.7165003668416032,0.017200820978334835,0.0011920643679057508,9.187253561090699E-5]  |\n",
      "|529    |3       |1.0       |[0.2915839000362302,0.6945214707021296,0.013050381820793254,7.520998359932583E-4,9.21476048537325E-5]     |\n",
      "|530    |3       |1.0       |[0.2732815093503761,0.7141431514785054,0.011841349198989187,6.475968105924589E-4,8.639316153665144E-5]    |\n",
      "|531    |3       |1.0       |[0.2887564395590639,0.6974643690808952,0.013008466548863975,6.802361898845194E-4,9.048862129261016E-5]    |\n",
      "|532    |2       |1.0       |[0.2887564395590639,0.6974643690808952,0.013008466548863975,6.802361898845194E-4,9.048862129261016E-5]    |\n",
      "|533    |2       |1.0       |[0.2887564395590639,0.6974643690808952,0.013008466548863975,6.802361898845194E-4,9.048862129261016E-5]    |\n",
      "|534    |2       |1.0       |[0.26590267633332126,0.7208677821807136,0.012641163874074922,5.069349257408993E-4,8.144268614934918E-5]   |\n",
      "|535    |2       |1.0       |[0.2968491529813545,0.6882221345902302,0.014402690640230106,4.390723392798195E-4,8.69494489052478E-5]     |\n",
      "|536    |2       |0.0       |[0.5845701232099964,0.38016728516136183,0.0338286782245089,0.0012546426127451103,1.7927079138776018E-4]   |\n",
      "|537    |2       |1.0       |[0.39368010489278316,0.5758262786679234,0.02884457726211965,0.0015057271770708375,1.4331200010304112E-4]  |\n",
      "|538    |2       |1.0       |[0.3192338175845247,0.6641648144878446,0.0153053440491614,0.0011893050634607606,1.0671881500850165E-4]    |\n",
      "|539    |3       |1.0       |[0.2807967713555274,0.7058764700349672,0.01207671134941005,0.0011562175840430001,9.382967605206669E-5]    |\n",
      "|540    |2       |1.0       |[0.2879409196991185,0.6980584741594242,0.012850844731686017,0.0010551943511059698,9.45670586652379E-5]    |\n",
      "|541    |2       |1.0       |[0.29124384478382886,0.6948408556888365,0.012888048758087741,9.339793046870933E-4,9.327146455973414E-5]   |\n",
      "|542    |2       |1.0       |[0.29124384478382886,0.6948408556888365,0.012888048758087741,9.339793046870933E-4,9.327146455973414E-5]   |\n",
      "|543    |2       |1.0       |[0.2914098960963864,0.6946666124303493,0.012896033316668802,9.341551926001285E-4,9.330296399533184E-5]    |\n",
      "|544    |2       |1.0       |[0.30184280596660623,0.6827556190320715,0.014786662152350171,5.251275559913378E-4,8.978529298086512E-5]   |\n",
      "|545    |3       |1.0       |[0.32599396902902905,0.6575512999904579,0.015109386368319695,0.0012326929639211567,1.126516482722882E-4]  |\n",
      "|546    |2       |0.0       |[0.5579234674801515,0.41932638289088436,0.01825882949288963,0.004311868029930735,1.7945210614365284E-4]   |\n",
      "|547    |3       |1.0       |[0.3806185332382409,0.589850921681899,0.028037807964183942,0.0013475735088852225,1.4516360679092785E-4]   |\n",
      "|548    |2       |0.0       |[0.5791346736427964,0.3880831011372526,0.022484049961827703,0.010109097279890447,1.8907797823297396E-4]   |\n",
      "|549    |2       |1.0       |[0.31339432966657427,0.6701375256840945,0.014771272018456528,0.0015922867470337383,1.0458588384100387E-4] |\n",
      "|550    |2       |1.0       |[0.299242397764355,0.6850948325574653,0.014392844544362267,0.001179468086289039,9.045704752835094E-5]     |\n",
      "|551    |2       |1.0       |[0.29767729812156113,0.6876105083416362,0.013847504009057364,7.731283365288907E-4,9.156119121639599E-5]   |\n",
      "|552    |2       |1.0       |[0.344371502241502,0.6288624910220257,0.0264183872399141,2.4701515485573675E-4,1.0060434170240373E-4]     |\n",
      "|553    |2       |1.0       |[0.32957968100146384,0.6419253021957922,0.028191964465537955,2.0706215212327225E-4,9.59901850827582E-5]   |\n",
      "|554    |2       |1.0       |[0.32957968100146384,0.6419253021957922,0.028191964465537955,2.0706215212327225E-4,9.59901850827582E-5]   |\n",
      "|555    |2       |1.0       |[0.39811731320203436,0.5709746694227813,0.02909711265390535,0.001670805654843325,1.4009906643569224E-4]   |\n",
      "|556    |3       |1.0       |[0.33659277399487547,0.6448347729005293,0.016944452838554405,0.0015074959988168569,1.2050426722387173E-4] |\n",
      "|557    |2       |1.0       |[0.32469421356015804,0.658344526681699,0.015038012460335431,0.0018183869271083048,1.0486037069925742E-4]  |\n",
      "|558    |2       |0.0       |[0.561555856709084,0.40702073783219644,0.018750448695503522,0.012480828425780872,1.9212833743516644E-4]   |\n",
      "|559    |2       |1.0       |[0.30450767614566293,0.6796778196936997,0.01390859187813154,0.001804131803258987,1.0178047924675942E-4]   |\n",
      "|560    |2       |1.0       |[0.3106398288902385,0.6741344401277248,0.013570331088489875,0.001556223330859533,9.917656268725736E-5]    |\n",
      "|561    |2       |1.0       |[0.28987527745721753,0.6956637700435612,0.013313505508644935,0.0010570993143242267,9.034767625220631E-5]  |\n",
      "|562    |2       |1.0       |[0.28744447386322364,0.697579806442626,0.01412471433537823,7.655145128183666E-4,8.549084595372937E-5]     |\n",
      "|563    |2       |1.0       |[0.35146518345386873,0.621092868888501,0.02711053031964588,2.307576361492884E-4,1.006597018351366E-4]     |\n",
      "|564    |2       |1.0       |[0.3167711817288721,0.6576897302553839,0.0252461069082759,1.9847730910222368E-4,9.450379836592248E-5]     |\n",
      "|565    |2       |1.0       |[0.35760410862972214,0.6145872147208661,0.02750740269698786,2.0153151250672919E-4,9.974243991716355E-5]   |\n",
      "|566    |2       |1.0       |[0.3217890531495533,0.6607640875236154,0.015119854727355208,0.002222259503124421,1.0474509635161052E-4]   |\n",
      "|567    |2       |1.0       |[0.30483264318678793,0.6789075083651174,0.014107133976264464,0.002053750618007074,9.896385382314941E-5]   |\n",
      "|568    |2       |1.0       |[0.3048264136626525,0.6789140661190193,0.014106820154426134,0.002053737389485133,9.896267441688031E-5]    |\n",
      "|569    |2       |1.0       |[0.30417949909652164,0.6793800942664854,0.01417482691913253,0.002168136603928016,9.744311393265298E-5]    |\n",
      "|570    |3       |1.0       |[0.25158542912099335,0.7349504256089208,0.011729890773842629,0.0016513261804567418,8.292831578652882E-5]  |\n",
      "|571    |2       |1.0       |[0.30003970902852717,0.6843481187523444,0.015031149093360887,4.943510051637462E-4,8.667212060373818E-5]   |\n",
      "|572    |2       |0.0       |[0.6005452111816801,0.3652450137190166,0.03186988029750155,0.002165785912511134,1.7410888929052378E-4]    |\n",
      "|573    |2       |1.0       |[0.3437718187585942,0.6299116198365052,0.02594455237598306,2.7635002382227176E-4,9.565900509516103E-5]    |\n",
      "|574    |3       |1.0       |[0.3257514236678168,0.658781967186464,0.013911570854394982,0.001447675102968905,1.0736318835542683E-4]    |\n",
      "|575    |2       |1.0       |[0.29712869594915015,0.6814403945939309,0.0190873908952401,0.002237845739231451,1.0567282244734712E-4]    |\n",
      "|576    |2       |1.0       |[0.3294989512219533,0.653921088117735,0.014349897828522445,0.002120371041139906,1.096917906493673E-4]     |\n",
      "|577    |3       |1.0       |[0.29512309296839834,0.6897652644058027,0.013091808402734896,0.0019242115339772354,9.562268908682598E-5]  |\n",
      "|578    |2       |0.0       |[0.5409380431194692,0.42405940428891287,0.018744901914746072,0.01607875257941846,1.7889809745336044E-4]   |\n",
      "|579    |2       |1.0       |[0.28820066757033563,0.6957611662137857,0.013661442493281306,0.0022836081524070786,9.311557019036223E-5]  |\n",
      "|580    |2       |1.0       |[0.2881944476770073,0.6957677239684191,0.013661122181874923,0.0022835917948862677,9.311437781241706E-5]   |\n",
      "|581    |2       |1.0       |[0.43950757676789715,0.5306426124169045,0.027939139887587013,0.0017667212405174774,1.4394968709393496E-4] |\n",
      "|582    |2       |1.0       |[0.35825061793950147,0.6227156918418356,0.017095723939048835,0.001825310103206454,1.1265617640762043E-4]  |\n",
      "|583    |3       |1.0       |[0.3001858250302968,0.6853226528511084,0.012330948047093752,0.0020585204117934716,1.0205365970767685E-4]  |\n",
      "|584    |3       |1.0       |[0.2987545209704527,0.683890953030196,0.014234030727593428,0.0030204025779940894,1.0009269376393289E-4]   |\n",
      "|585    |2       |0.0       |[0.5410343002467835,0.41893718591108625,0.01909067997677777,0.020751729779632605,1.8610408571979447E-4]   |\n",
      "|586    |3       |1.0       |[0.27253221560463653,0.7127830356851095,0.012313665745354401,0.0022778501602777443,9.323280462196104E-5]  |\n",
      "|587    |3       |1.0       |[0.287911617573422,0.695316369702232,0.013989102285433978,0.002687004811524597,9.590562738745172E-5]      |\n",
      "|588    |2       |0.0       |[0.5243080121358485,0.4429882617853312,0.01771641339865971,0.014809221594300067,1.780910858604303E-4]     |\n",
      "|589    |3       |1.0       |[0.23728468387451282,0.7500554399677436,0.011119491222225306,0.0014613384371481785,7.90464983702778E-5]   |\n",
      "|590    |2       |1.0       |[0.284785302287807,0.6997055865729064,0.013880202014822901,0.0015385722914578453,9.033683300586533E-5]    |\n",
      "|591    |2       |0.0       |[0.5393477808138591,0.43032941595309626,0.019216750679380567,0.010930925665433128,1.7512688823089826E-4]  |\n",
      "|592    |2       |1.0       |[0.27228703874787674,0.7139072080555687,0.012814872779527054,9.082940500499847E-4,8.258636697761466E-5]   |\n",
      "|593    |2       |0.0       |[0.5710274631104881,0.3982186938207643,0.027775677816712394,0.0028048444344086943,1.7332081762642856E-4]  |\n",
      "|594    |3       |1.0       |[0.3277571339070127,0.6501506105456587,0.021625019583679166,3.725137808490639E-4,9.472218280024129E-5]    |\n",
      "|595    |2       |1.0       |[0.3277571339070127,0.6501506105456587,0.021625019583679166,3.725137808490639E-4,9.472218280024129E-5]    |\n",
      "|596    |3       |1.0       |[0.3274347816069902,0.6555454582722038,0.015345048947524855,0.0015629972938713988,1.117138794097902E-4]   |\n",
      "|597    |2       |1.0       |[0.3309757666057492,0.6524182545855999,0.014397337910372399,0.002097298186574762,1.1134271170363414E-4]   |\n",
      "|598    |2       |1.0       |[0.280058240530759,0.704808858386284,0.01384715271880403,0.0011895134777221393,9.62348864307123E-5]       |\n",
      "|599    |3       |1.0       |[0.2807835569861015,0.703006437496774,0.015185903528410602,9.258385864891843E-4,9.826340222445913E-5]     |\n",
      "|600    |3       |1.0       |[0.3066981727460685,0.6569959543867369,0.03530594445566235,8.710045593874197E-4,1.2892385214484085E-4]    |\n",
      "|601    |3       |1.0       |[0.2625419535367099,0.7234364947408478,0.012336402032002073,0.0015981672044608213,8.698248597952568E-5]   |\n",
      "|602    |3       |1.0       |[0.25212769435958854,0.7342857628206405,0.01299603151953763,5.09230306797073E-4,8.128099343625732E-5]     |\n",
      "|603    |2       |1.0       |[0.2778412407358348,0.7068619980634794,0.014838025613782117,3.723850218319452E-4,8.635056507180528E-5]    |\n",
      "|604    |3       |1.0       |[0.30199811529127407,0.6761067530487567,0.021545641517672257,2.605774390473914E-4,8.891270324985755E-5]   |\n",
      "|605    |2       |0.0       |[0.5931182806473558,0.37068965309115953,0.03502437272740598,9.841522647281416E-4,1.835412693504607E-4]    |\n",
      "|606    |2       |1.0       |[0.33486439783172195,0.645975297767879,0.0163368183786207,0.002710820942882085,1.1266507889630628E-4]     |\n",
      "|607    |2       |1.0       |[0.28311861416280326,0.7010094911711053,0.014758069243955847,0.0010175449911175637,9.628043101800702E-5]  |\n",
      "|608    |2       |0.0       |[0.5387658149833072,0.43325219217533545,0.020530093875379192,0.007264347404301234,1.8755156167697307E-4]  |\n",
      "|609    |2       |0.0       |[0.538770628960981,0.4332471732460209,0.02053030264877887,0.007264342996047941,1.8755214817118277E-4]     |\n",
      "|610    |2       |1.0       |[0.2804551817894967,0.703337561371547,0.01530467079251159,8.130956500956343E-4,8.949039634932033E-5]      |\n",
      "|611    |3       |1.0       |[0.2865893813141728,0.6985604233419745,0.014071763811793946,6.884661115033661E-4,8.996542055546333E-5]    |\n",
      "|612    |2       |1.0       |[0.30928059638678207,0.6669804058554667,0.02344965415322619,2.0110856578136208E-4,8.823503874365414E-5]   |\n",
      "|613    |2       |1.0       |[0.3459111561565242,0.6342297243802251,0.017383976016514546,0.0023640122289836835,1.1113121775258935E-4]  |\n",
      "|614    |2       |1.0       |[0.3144671610967059,0.6685238971706099,0.01600146397184208,9.049036780684228E-4,1.0257408277361989E-4]    |\n",
      "|615    |2       |1.0       |[0.3066675408935185,0.6768320815095834,0.01529816161935908,0.0010994481408578794,1.0276783668109825E-4]   |\n",
      "|616    |2       |1.0       |[0.2834984862611383,0.7005092302715296,0.014856822659931745,0.0010412915162342257,9.416929116601797E-5]   |\n",
      "|617    |3       |1.0       |[0.27112349260982505,0.7137323987659265,0.01345957838846003,0.0015934117945007914,9.11184412875038E-5]    |\n",
      "|618    |3       |1.0       |[0.2632190122066662,0.722625966359484,0.013441681762125414,6.277174056880278E-4,8.562226603646014E-5]     |\n",
      "|619    |2       |0.0       |[0.5340883064550043,0.43963669081377227,0.021634056598760446,0.004471117127644772,1.6982900481827154E-4]  |\n",
      "|620    |3       |1.0       |[0.2783368449208438,0.7055327736246781,0.015422782995252458,6.211349515919174E-4,8.646350763355295E-5]    |\n",
      "|621    |2       |1.0       |[0.3403789537952922,0.6347782098287548,0.024403708723595884,3.43442427373907E-4,9.56852249831321E-5]      |\n",
      "|622    |2       |1.0       |[0.3297858136333707,0.6455863644271633,0.024215619859390265,3.117996892815211E-4,1.0040239079427698E-4]   |\n",
      "|623    |2       |1.0       |[0.3028754029062381,0.6808259689358882,0.014367386897439195,0.0018337222739757606,9.751898645865201E-5]   |\n",
      "|624    |2       |1.0       |[0.27493876469379336,0.7118728430096339,0.012392305746040885,7.093373206070186E-4,8.674922992470026E-5]   |\n",
      "|625    |2       |1.0       |[0.29717558123332266,0.6881238842824632,0.014106860282839721,5.024922403679259E-4,9.118196100644656E-5]   |\n",
      "|626    |3       |1.0       |[0.32353924677692064,0.6515486078220633,0.02454817561313163,2.673935149091351E-4,9.657627297516816E-5]    |\n",
      "|627    |3       |1.0       |[0.27521017431340294,0.7104668167684527,0.01226166021709606,0.0019647993062028354,9.654939484548044E-5]   |\n",
      "|628    |2       |0.0       |[0.5388151471948993,0.42290475315286147,0.020039385291087543,0.018051141837067915,1.8957252408377997E-4]  |\n",
      "|629    |2       |1.0       |[0.2743467478438149,0.7111594540809784,0.01291906934658673,0.0014822802446700584,9.244848394985211E-5]    |\n",
      "|630    |3       |1.0       |[0.23293522145672718,0.7545320877585713,0.011035774113664997,0.00141592020506767,8.099646596857499E-5]    |\n",
      "|631    |3       |1.0       |[0.26279222706269767,0.7240677675048423,0.011830541647438683,0.0012215320581215516,8.793172689993E-5]     |\n",
      "|632    |3       |1.0       |[0.2715506899868846,0.7146059579711176,0.012987458191410485,7.720879319744772E-4,8.380591861291448E-5]    |\n",
      "|633    |3       |1.0       |[0.28795230153523915,0.6960303909356703,0.01511929681967022,8.054950927231819E-4,9.25156166971272E-5]     |\n",
      "|634    |2       |1.0       |[0.33144542948969324,0.6438481714261863,0.02426380888566592,3.423749489993801E-4,1.0021524945500516E-4]   |\n",
      "|635    |2       |1.0       |[0.33644792814895896,0.6372424484268574,0.025778484215262788,4.2566447419197163E-4,1.0547473472905186E-4] |\n",
      "|636    |2       |1.0       |[0.33404888108196307,0.6421479940590182,0.023420909606880567,2.786499387872738E-4,1.0356531335075061E-4]  |\n",
      "|637    |2       |1.0       |[0.33404888108196307,0.6421479940590182,0.023420909606880567,2.786499387872738E-4,1.0356531335075061E-4]  |\n",
      "|638    |3       |1.0       |[0.2653033123825329,0.7046690315818214,0.02972874656598567,2.1269756580676762E-4,8.621190385319436E-5]    |\n",
      "|639    |3       |1.0       |[0.38400057555559997,0.587414296509725,0.0270425986364318,0.0013978699715092577,1.4465932673392206E-4]    |\n",
      "|640    |2       |1.0       |[0.3376824579492057,0.643163429867014,0.017293668864360713,0.0017448360415323095,1.1560727788727883E-4]   |\n",
      "|641    |2       |0.0       |[0.5661375301791818,0.4037063297403039,0.01894066480023784,0.011012985232249493,2.024900480269893E-4]     |\n",
      "|642    |2       |1.0       |[0.31047074512269435,0.6736911877273078,0.014545385091098614,0.0011845927329276829,1.0808932597143458E-4] |\n",
      "|643    |3       |1.0       |[0.28588939806393115,0.6899420796633836,0.022072124178305796,0.0019841627449315308,1.1223534944785826E-4] |\n",
      "|644    |3       |1.0       |[0.25311353697337796,0.726747805330761,0.018623336913485732,0.001423163020486508,9.215776188870263E-5]    |\n",
      "|645    |3       |1.0       |[0.39377420697384946,0.5774017106829277,0.027361803201304706,0.0013176347058259858,1.446444360922139E-4]  |\n",
      "|646    |2       |0.0       |[0.6077742492547723,0.35268502644477073,0.025259843188618368,0.014061656293139971,2.1922481869868503E-4]  |\n",
      "|647    |2       |1.0       |[0.3507476488600294,0.6286461790252309,0.018386779416202054,0.002094532138371401,1.2486056016627827E-4]   |\n",
      "|648    |2       |1.0       |[0.297924933691992,0.687368997969626,0.01373252685725335,8.801203934240905E-4,9.342108770445542E-5]       |\n",
      "|649    |3       |1.0       |[0.43327268100989424,0.5499059910881663,0.016507795375227445,2.011369846299882E-4,1.1239554208203022E-4]  |\n",
      "|650    |2       |1.0       |[0.48384291704648347,0.49076237565655845,0.02518827870004414,8.43588524927454E-5,1.2206974442130739E-4]   |\n",
      "|651    |2       |1.0       |[0.4594192196362042,0.5119702512539701,0.02840999832611413,7.637917439672012E-5,1.2415160931505536E-4]    |\n",
      "|652    |2       |0.0       |[0.7234893819119371,0.24742070428367832,0.02851611541947126,3.875254070660647E-4,1.8627297784702936E-4]   |\n",
      "|653    |2       |0.0       |[0.7255225592730673,0.24297342367069755,0.030953491349858368,3.7108859205517403E-4,1.794371143216706E-4]  |\n",
      "|654    |2       |1.0       |[0.46367270333397514,0.5072307211657979,0.028939228479955094,4.49147365093446E-5,1.124322837625181E-4]    |\n",
      "|655    |3       |1.0       |[0.4278733652090457,0.5544682760162619,0.01726722899265202,2.696331077350993E-4,1.214966743052749E-4]     |\n",
      "|656    |2       |0.0       |[0.7189386252116109,0.2608503588403145,0.01829319601209611,0.0017125430824465166,2.0527685353201569E-4]   |\n",
      "|657    |2       |1.0       |[0.44914628061827605,0.5351647043994185,0.015526803500675541,3.631273503167712E-5,1.2589874659805442E-4]  |\n",
      "|658    |2       |1.0       |[0.42581752092205655,0.559646072541721,0.014323141454605063,1.0027953060617596E-4,1.1298555101109828E-4]  |\n",
      "|659    |3       |1.0       |[0.4391781446643946,0.5460652811742558,0.014469681324212003,1.8010033345752536E-4,1.0679250368010888E-4]  |\n",
      "|660    |2       |0.0       |[0.5622561043188807,0.4069707753600435,0.03042686523085921,1.863138491628858E-4,1.5994124105370687E-4]    |\n",
      "|661    |3       |0.0       |[0.49518397218774407,0.48755243633907336,0.016927924099995974,2.036014796927776E-4,1.3206589349383827E-4] |\n",
      "|662    |3       |1.0       |[0.47216909798892875,0.5020016891862472,0.02564205747919152,7.629444118137118E-5,1.10860904451123E-4]     |\n",
      "|663    |3       |0.0       |[0.5458063412525738,0.42420432268841407,0.02956304972763468,2.7223943114395677E-4,1.540469002335729E-4]   |\n",
      "|664    |2       |0.0       |[0.7354084503666711,0.24195069360661905,0.02077772092641807,0.0016603264852111078,2.028086150806487E-4]   |\n",
      "|665    |2       |0.0       |[0.4898131500008847,0.4894029573758549,0.020358489187818473,2.9089502957564917E-4,1.3450840586640896E-4]  |\n",
      "|666    |2       |1.0       |[0.4661201853853453,0.5170873846304715,0.016353536569124513,3.089927537659357E-4,1.2990066129288703E-4]   |\n",
      "|667    |2       |1.0       |[0.48878753320492474,0.4919302596141504,0.018846888892562565,3.0988547057347063E-4,1.254328177889793E-4]  |\n",
      "|668    |3       |1.0       |[0.4476299297352849,0.5348238875644993,0.017057150896328824,3.588334730624462E-4,1.3019833082433016E-4]   |\n",
      "|669    |2       |0.0       |[0.704655433954282,0.2733867768628409,0.019629016875012326,0.00211897854362301,2.0979376424176035E-4]     |\n",
      "|670    |2       |0.0       |[0.7046439320162524,0.2733986453140054,0.019628625966383147,0.0021190028163953776,2.097938869637664E-4]   |\n",
      "|671    |2       |1.0       |[0.4183682771987249,0.5646496101884622,0.016594540881098418,2.705928333610444E-4,1.1697889835342495E-4]   |\n",
      "|672    |2       |1.0       |[0.4570028177996173,0.5123239269417685,0.030440911887325662,1.083381075742887E-4,1.2400526371404052E-4]   |\n",
      "|673    |2       |1.0       |[0.45587726035444337,0.5162423383952653,0.02765296767862541,9.743852415680215E-5,1.299950475088988E-4]    |\n",
      "|674    |2       |1.0       |[0.4464546041569853,0.5229613110592877,0.03036455816548494,8.866795223644671E-5,1.3085866600574185E-4]    |\n",
      "|675    |2       |1.0       |[0.463527437004878,0.5064718470893692,0.029808687718477663,6.721493452649322E-5,1.2481325274887582E-4]    |\n",
      "|676    |2       |0.0       |[0.7129337386713942,0.25296614897447534,0.03351579171061973,3.8781691815679997E-4,1.9650372535389319E-4]  |\n",
      "|677    |2       |1.0       |[0.41610701573786085,0.5437610894595342,0.039956410947551514,6.118890891399268E-5,1.1429494613956572E-4]  |\n",
      "|678    |2       |1.0       |[0.448575867502306,0.5324592677123035,0.01845614541396085,3.8047004788682384E-4,1.282493235427355E-4]     |\n",
      "|679    |2       |1.0       |[0.4434070092908937,0.5375276415660186,0.018586703132746338,3.5538718776695425E-4,1.232588225744055E-4]   |\n",
      "|680    |2       |1.0       |[0.4247607846270693,0.5576291153743648,0.017190300339125608,3.0152948601068995E-4,1.1827017342957102E-4]  |\n",
      "|681    |2       |0.0       |[0.520335835213494,0.44618182247050653,0.03302925777974844,2.922001570766783E-4,1.6088437917435844E-4]    |\n",
      "|682    |2       |1.0       |[0.4655091719011185,0.5153586383324468,0.018675422510407935,3.257658613252982E-4,1.3100139470162368E-4]   |\n",
      "|683    |2       |1.0       |[0.4574656320506106,0.5241799393059547,0.017894174863576317,3.3629116797123817E-4,1.2396261188702301E-4]  |\n",
      "|684    |2       |0.0       |[0.7030766600499727,0.27114390476251166,0.023050184138046288,0.0025172497622623895,2.120012872069979E-4]  |\n",
      "|685    |2       |1.0       |[0.4132107022716107,0.5698593817389942,0.016618878525210767,1.9821190989843687E-4,1.1282555428589101E-4]  |\n",
      "|686    |2       |1.0       |[0.41311068021543634,0.5699638741312554,0.016614430686962837,1.9820313867686352E-4,1.1281182766863144E-4] |\n",
      "|687    |3       |1.0       |[0.4194233546404323,0.5638351884931412,0.01649367412167877,1.3383524945142646E-4,1.1394749529644284E-4]   |\n",
      "|688    |2       |1.0       |[0.4686292647668066,0.5023430670916129,0.028828738526887297,7.855145473954963E-5,1.2037815995367982E-4]   |\n",
      "|689    |3       |1.0       |[0.4579769437038008,0.509041814231377,0.03278052092369594,7.52224958249594E-5,1.2549864530124428E-4]      |\n",
      "|690    |2       |1.0       |[0.42382772053774476,0.5364378959325717,0.039568972529872806,5.211667261190203E-5,1.1329432719874906E-4]  |\n",
      "|691    |3       |0.0       |[0.5281232429290074,0.44279371440659604,0.02872725508967885,2.0647558338175237E-4,1.4931199133599269E-4]  |\n",
      "|692    |2       |1.0       |[0.46789950124049695,0.5111431440645361,0.020438685006322573,3.776020647047577E-4,1.4106762393962909E-4]  |\n",
      "|693    |2       |1.0       |[0.4715389871408732,0.5090096871693083,0.01895735022670098,3.6233264846751773E-4,1.3164281464996477E-4]   |\n",
      "|694    |2       |0.0       |[0.7025822486415724,0.27746571433724504,0.017926543812409015,0.0018333300281665181,1.92163180606861E-4]   |\n",
      "|695    |2       |1.0       |[0.42344926172625014,0.5601499306899063,0.0160097717477531,2.7782022973155227E-4,1.1321560635899927E-4]   |\n",
      "|696    |3       |1.0       |[0.45797372219781196,0.5157291616198871,0.026105931907152487,7.318763920096507E-5,1.1799663594742707E-4]  |\n",
      "|697    |2       |1.0       |[0.423421401301367,0.5595910430862944,0.016474951429405752,3.888223723222259E-4,1.237818106105189E-4]     |\n",
      "|698    |2       |1.0       |[0.474445436066666,0.4943686056438367,0.030962108244255947,9.462287017003098E-5,1.292271750712553E-4]     |\n",
      "|699    |3       |1.0       |[0.4359861210128111,0.5366909589440784,0.027136262693989308,6.889351479328808E-5,1.1776383432787197E-4]   |\n",
      "|700    |2       |1.0       |[0.4412044982952437,0.529679185773347,0.028927637935191854,6.893550981144545E-5,1.1974248640597324E-4]    |\n",
      "|701    |3       |1.0       |[0.466107171706126,0.5157339202266777,0.017652091145225264,3.5446154743558653E-4,1.5235537453548595E-4]   |\n",
      "|702    |3       |1.0       |[0.43018087616998346,0.5544033956815861,0.014947811432103338,3.302105742034247E-4,1.377061421236408E-4]   |\n",
      "|703    |2       |0.0       |[0.7069360309041943,0.2680704824790084,0.021956847583159887,0.0028141834695806275,2.2245556405677328E-4]  |\n",
      "|704    |2       |1.0       |[0.4370114939460572,0.5457930845261624,0.016761298813417598,3.005277329639299E-4,1.3359498139880116E-4]   |\n",
      "|705    |2       |1.0       |[0.41108948577908727,0.5716254583956697,0.01682683997051637,3.362782242576432E-4,1.2193763046892771E-4]   |\n",
      "|706    |3       |1.0       |[0.4106739688386644,0.573329853922761,0.015504889129012043,3.584425106999387E-4,1.3284559886259594E-4]    |\n",
      "|707    |3       |1.0       |[0.4245595548025221,0.5566541373247834,0.018253487074855655,4.1215244290709896E-4,1.2066835493162702E-4]  |\n",
      "|708    |2       |1.0       |[0.39305832242241423,0.5902418837011488,0.016285915784884968,2.949735772559342E-4,1.1890451429603971E-4]  |\n",
      "|709    |3       |0.0       |[0.48999132259950096,0.48167344121061495,0.027549734644955106,6.213622678190076E-4,1.6413927711009832E-4] |\n",
      "|710    |2       |1.0       |[0.4543672163922315,0.5255532502053804,0.019344695052719355,5.824489877910525E-4,1.5238936187785955E-4]   |\n",
      "|711    |2       |1.0       |[0.4570260345233089,0.5236827726656444,0.01865231616431477,4.974922499248945E-4,1.413843968070523E-4]     |\n",
      "|712    |3       |1.0       |[0.3935578910914784,0.5830292454922832,0.02280996321336234,4.7235227503507874E-4,1.305479278409608E-4]    |\n",
      "|713    |2       |1.0       |[0.43539020388565025,0.5442945850527617,0.019538985034669092,6.323120213472948E-4,1.4391400557159267E-4]  |\n",
      "|714    |2       |1.0       |[0.4412800678366568,0.5396165175928797,0.018602783712356238,3.680386623345051E-4,1.3259219577269735E-4]   |\n",
      "|715    |2       |1.0       |[0.4115094349876726,0.5697166097182352,0.01842643492159554,2.2943764360375612E-4,1.1808272889282615E-4]   |\n",
      "|716    |2       |1.0       |[0.4115094349876726,0.5697166097182352,0.01842643492159554,2.2943764360375612E-4,1.1808272889282615E-4]   |\n",
      "|717    |2       |1.0       |[0.43736042341363807,0.5338873752952524,0.028517950430718527,1.1173185952017403E-4,1.22519000870714E-4]   |\n",
      "|718    |2       |0.0       |[0.5071445298278703,0.4630386973438177,0.029245441577042976,4.0995476596200717E-4,1.6137648530708228E-4]  |\n",
      "|719    |2       |0.0       |[0.7003786641682751,0.2759666547140888,0.021428535857285956,0.0020098933190343493,2.1625194131603265E-4]  |\n",
      "|720    |2       |1.0       |[0.4385999134299292,0.5433678264465892,0.01753692052555288,3.703017450569063E-4,1.2503785287186728E-4]    |\n",
      "|721    |2       |1.0       |[0.4016773759365175,0.5817271561191409,0.016287969634572328,1.881294532124935E-4,1.1936885655674562E-4]   |\n",
      "|722    |2       |1.0       |[0.4590727669719379,0.511633394586285,0.02906780324058415,1.0117139457411013E-4,1.2486380661897952E-4]    |\n",
      "|723    |2       |1.0       |[0.4559465990724594,0.5146973305719844,0.029171359353990553,6.363504232218156E-5,1.2107595924353271E-4]   |\n",
      "|724    |2       |1.0       |[0.4712014629533264,0.5098198801442768,0.018495398675206495,3.3734498806173535E-4,1.4591323912851802E-4]  |\n",
      "|725    |2       |1.0       |[0.461081771396289,0.5203253889592013,0.018013039280051404,4.376365308324045E-4,1.4216383362567882E-4]    |\n",
      "|726    |2       |1.0       |[0.41955189454844277,0.5625758617369858,0.017284829574088275,4.723767038481822E-4,1.1503743663484969E-4]  |\n",
      "|727    |2       |1.0       |[0.41955200202496884,0.5625757493231801,0.01728483447590284,4.723767246920476E-4,1.1503745125634035E-4]   |\n",
      "|728    |2       |1.0       |[0.4549663381398754,0.5157001355403524,0.029115760412519398,9.256988918380241E-5,1.2519601806882869E-4]   |\n",
      "|729    |3       |1.0       |[0.45463007753164697,0.5180712032514059,0.027082557478167808,8.842321025029594E-5,1.277385285290712E-4]   |\n",
      "|730    |2       |1.0       |[0.4341661032772634,0.5412165634012185,0.024408632506021505,7.713820949437353E-5,1.315626060022248E-4]    |\n",
      "|731    |2       |1.0       |[0.4312072876207143,0.5441898145261221,0.02440347178436063,6.991678610515967E-5,1.2950928269770583E-4]    |\n",
      "|732    |2       |0.0       |[0.6908407390519022,0.27567016286210505,0.03281372505207664,4.6331753632691326E-4,2.120554975890231E-4]   |\n",
      "|733    |3       |1.0       |[0.40406922016392705,0.5731435466549903,0.022616230528552363,4.819030126398192E-5,1.228123512661694E-4]   |\n",
      "|734    |3       |0.0       |[0.49838034694080013,0.4753664630221272,0.025633386745955153,4.415967528401745E-4,1.7820653827732225E-4]  |\n",
      "|735    |3       |0.0       |[0.4983798082515581,0.4753670330191459,0.025633355524917915,4.4159674371710005E-4,1.7820646066100688E-4]  |\n",
      "|736    |2       |0.0       |[0.49837959277581284,0.4753672610180025,0.025633343036502644,4.4159674006766795E-4,1.7820642961442472E-4] |\n",
      "|737    |2       |0.0       |[0.5068217811588775,0.4658883540390865,0.026726496762898996,3.846750250898245E-4,1.7869301404716868E-4]   |\n",
      "|738    |3       |1.0       |[0.47815108845514404,0.4977524352084821,0.023489652839055512,4.340683115124104E-4,1.7275518580595178E-4]  |\n",
      "|739    |2       |1.0       |[0.4749668408574428,0.5009822013886216,0.02348718584977346,3.9364623710437443E-4,1.7012566705773208E-4]   |\n",
      "|740    |2       |1.0       |[0.47496716559813057,0.500981858588173,0.02348720384036122,3.9364625583383404E-4,1.7012571750147775E-4]   |\n",
      "|741    |2       |1.0       |[0.47925636530307164,0.4947710982801466,0.025392248333029496,4.108766685230102E-4,1.6941141522916134E-4]  |\n",
      "|742    |2       |1.0       |[0.4763442263618731,0.4967566121811525,0.026347224132953498,3.858472469724565E-4,1.6609007704828854E-4]   |\n",
      "|743    |2       |1.0       |[0.4318370189561111,0.5501054394393003,0.017320061247712706,5.899296138647804E-4,1.4755074301111686E-4]   |\n",
      "|744    |2       |1.0       |[0.41896862251355516,0.5633053936997146,0.017185631780324832,4.114778502746287E-4,1.2887415613089672E-4]  |\n",
      "|745    |3       |1.0       |[0.4268237266963921,0.5559522456345813,0.01673083489209332,3.6511542148965015E-4,1.280773554436439E-4]    |\n",
      "|746    |2       |1.0       |[0.4053677642468287,0.5788080685875243,0.015384060121523865,3.1845463314456756E-4,1.2165241097846864E-4]  |\n",
      "|747    |3       |1.0       |[0.36127234648893164,0.6237957643713824,0.014690748952480853,1.3315725682401828E-4,1.0798293038111661E-4] |\n",
      "|748    |2       |1.0       |[0.47220152676905597,0.50474287968376,0.0229102815687573,2.0071531775759516E-5,1.2524044665097685E-4]     |\n",
      "|749    |2       |0.0       |[0.5580297152050513,0.41712684746678347,0.024497983257864134,1.8954650463795167E-4,1.5590756566311072E-4] |\n",
      "|750    |2       |0.0       |[0.5255264525660399,0.45099631397463885,0.023204174029168648,1.2347781659171806E-4,1.4958161356075104E-4] |\n",
      "|751    |2       |0.0       |[0.7791291159879389,0.19062335828696514,0.02912382395019984,8.938965207909074E-4,2.2980525410522455E-4]   |\n",
      "|752    |3       |1.0       |[0.4696560634005997,0.5149960233674362,0.015081968115216287,1.4800674370298568E-4,1.1793837304475853E-4]  |\n",
      "|753    |2       |0.0       |[0.5026394931760335,0.4738584332374081,0.023331086674334017,4.381584807897728E-5,1.271710641453078E-4]    |\n",
      "|754    |2       |0.0       |[0.49064793554522157,0.4854363054162918,0.023766386129680135,3.6962054977469994E-5,1.1241085382898866E-4] |\n",
      "|755    |2       |1.0       |[0.41266817515339815,0.5734289691783241,0.013685846018122193,1.1122980886291023E-4,1.0577984129252615E-4] |\n",
      "|756    |2       |1.0       |[0.40959643043656513,0.5765248644012769,0.013673848001492768,1.0077828209623604E-4,1.0407887856896472E-4] |\n",
      "|757    |2       |1.0       |[0.4508502446811324,0.5342233913851694,0.014716260458492466,9.689801654530188E-5,1.1320545866054077E-4]   |\n",
      "|758    |2       |1.0       |[0.47858952533346266,0.4987702346564956,0.022492005171041198,3.817436948146682E-5,1.1006046951899611E-4]  |\n",
      "|759    |2       |1.0       |[0.4785889825181243,0.49877080612258523,0.022491976576918246,3.8174366657411326E-5,1.10060415714873E-4]   |\n",
      "|760    |2       |1.0       |[0.4843345351028632,0.4933460739512838,0.02216466620667762,3.7272441671603586E-5,1.1745229750369452E-4]   |\n",
      "|761    |3       |1.0       |[0.4696906925165675,0.5083474132849117,0.021632181828960196,2.0528216073185487E-4,1.2443020882861713E-4]  |\n",
      "|762    |3       |0.0       |[0.5080222078963365,0.47579989449861887,0.015862205501224105,1.8971275652791285E-4,1.2597934729268915E-4] |\n",
      "|763    |3       |1.0       |[0.4623407093614111,0.5157966887128864,0.02149612351599044,2.4134405901981613E-4,1.2513435069212727E-4]   |\n",
      "|764    |2       |1.0       |[0.4255369866191044,0.5598570344082219,0.01426292050092048,2.3421150222168588E-4,1.0884696953146307E-4]   |\n",
      "|765    |2       |1.0       |[0.4797740774283235,0.49705823301348184,0.023006920319069167,4.836879113840842E-5,1.124004479872126E-4]   |\n",
      "|766    |2       |0.0       |[0.5470976315282781,0.42777820246914133,0.024729821550105113,2.45484696466002E-4,1.4885975600941741E-4]   |\n",
      "|767    |2       |0.0       |[0.5470952910300918,0.4277806637996642,0.024729700836922826,2.454847914530916E-4,1.4885954186824102E-4]   |\n",
      "|768    |3       |1.0       |[0.4777564079099724,0.505427493750176,0.01634968722634797,3.3866312410381355E-4,1.2774798939982977E-4]    |\n",
      "|769    |2       |0.0       |[0.6996804562818943,0.2766812004538501,0.0215305015916796,0.0018785089749160644,2.2933269765982017E-4]    |\n",
      "|770    |3       |1.0       |[0.3809390798440285,0.60374091140272,0.015015915618197736,1.801675299780883E-4,1.239256050755801E-4]      |\n",
      "|771    |2       |1.0       |[0.4010853401796419,0.5839333561189067,0.014755634031711944,1.1910610854335155E-4,1.0656356119614217E-4]  |\n",
      "|772    |2       |1.0       |[0.44544674659347205,0.5302250613937005,0.024143799292984793,6.64531354347533E-5,1.179395844079798E-4]    |\n",
      "|773    |2       |1.0       |[0.4311239228902165,0.5384952670976499,0.030198670239016706,5.9089476314664966E-5,1.2305029680205885E-4]  |\n",
      "|774    |2       |1.0       |[0.44212313778200274,0.5356368104654963,0.022106647013692516,1.6332654592354485E-5,1.1707208421614595E-4] |\n",
      "|775    |3       |1.0       |[0.41360207757115475,0.55753408236782,0.02868991922088258,5.5614832999579626E-5,1.1830600714332832E-4]    |\n",
      "|776    |3       |1.0       |[0.46177116315458117,0.5120095271048033,0.025603433313728528,4.524914246518716E-4,1.6338500223499658E-4]  |\n",
      "|777    |3       |0.0       |[0.7337620845234812,0.2357207445350011,0.02852604606004626,0.00175068796374699,2.4043691772419177E-4]     |\n",
      "|778    |3       |1.0       |[0.43692670489668506,0.5415032332456631,0.02111647814673267,2.9096037823613387E-4,1.6262333268315436E-4]  |\n",
      "|779    |3       |1.0       |[0.47983287336486696,0.4948698394768372,0.024739870097987997,3.788632046024375E-4,1.78553855705425E-4]    |\n",
      "|780    |3       |1.0       |[0.46777474666601976,0.5051645335526161,0.02678271165471502,1.2878103002903456E-4,1.4922709662011405E-4]  |\n",
      "|781    |3       |0.0       |[0.71493067097207,0.2540427015018409,0.03012109078551355,6.740892130989209E-4,2.3144752747666705E-4]      |\n",
      "|782    |3       |1.0       |[0.46459193812658534,0.5083683569094827,0.02677599921296919,1.1677144960698617E-4,1.4693430135595347E-4]  |\n",
      "|783    |3       |1.0       |[0.4734373518859131,0.5010189294080641,0.025189536399027615,2.0795597922267393E-4,1.462263277723366E-4]   |\n",
      "|784    |2       |0.0       |[0.7254098881131447,0.24312403311513656,0.029701831993994288,0.0015088323368321157,2.554144408922935E-4]  |\n",
      "|785    |2       |1.0       |[0.42599877980651857,0.5583158889281449,0.015306503878782386,2.4916499225276955E-4,1.296623943014441E-4]  |\n",
      "|786    |3       |1.0       |[0.3975533049010065,0.5871070183305336,0.014839437168530029,3.611813339544515E-4,1.390582659754607E-4]    |\n",
      "|787    |3       |1.0       |[0.39866139711424375,0.5639535628871761,0.03716696910728046,9.883776655321227E-5,1.1923312474631954E-4]   |\n",
      "|788    |2       |0.0       |[0.7010723364711042,0.2670421940937831,0.031182224624628753,4.99336378302754E-4,2.0390843218113524E-4]    |\n",
      "|789    |2       |1.0       |[0.4419829232266624,0.5293495809324207,0.028473976657955966,7.201381998455681E-5,1.2150536297627548E-4]   |\n",
      "|790    |2       |0.0       |[0.6940488300368951,0.2724232523420214,0.033030334640677075,3.06768021837241E-4,1.9081495856925067E-4]    |\n",
      "|791    |2       |0.0       |[0.5164454241174212,0.4533638081968192,0.02957021568600586,4.575071244087475E-4,1.6304487534482426E-4]    |\n",
      "|792    |2       |0.0       |[0.5034549760141727,0.4684026982474458,0.02764471507722601,3.362759205169202E-4,1.613347406386609E-4]     |\n",
      "|793    |2       |1.0       |[0.39045233038700156,0.592343090709293,0.016883903258826757,2.048955030760996E-4,1.1578014180258329E-4]   |\n",
      "|794    |2       |1.0       |[0.3904509614341991,0.5923445252420385,0.01688383804412129,2.0489534950689133E-4,1.1577993013424338E-4]   |\n",
      "|795    |3       |1.0       |[0.431764254819083,0.5431947969969464,0.024822009183789788,8.610295318699286E-5,1.32836046993893E-4]      |\n",
      "|796    |2       |1.0       |[0.3910083174563223,0.5861545607515707,0.02268604976641001,3.7672842519445385E-5,1.1339918317765675E-4]   |\n",
      "|797    |2       |1.0       |[0.4575560077523363,0.5192360830961991,0.023062535700668393,1.996071916419241E-5,1.2541273163218326E-4]   |\n",
      "|798    |2       |0.0       |[0.7412918682174685,0.22831460024943567,0.027925219403371743,0.0022058864113188754,2.6242571840521736E-4] |\n",
      "|799    |2       |0.0       |[0.5117622347686351,0.4613888382040302,0.026240095448104732,4.305124894378395E-4,1.7831908979213808E-4]   |\n",
      "|800    |2       |0.0       |[0.5107772116714694,0.4665153308550525,0.02240918619454577,1.3584800635226568E-4,1.6242327258015216E-4]   |\n",
      "|801    |3       |0.0       |[0.5058392204584651,0.4652145549862989,0.028364777722599946,4.1551914694652097E-4,1.6592768568960895E-4]  |\n",
      "|802    |3       |1.0       |[0.46142706052495386,0.5222683899357766,0.016010841779267624,1.7013880935459576E-4,1.235689506473704E-4]  |\n",
      "|803    |3       |1.0       |[0.461427938203541,0.522267478148337,0.016010875745666037,1.7013884435905787E-4,1.2356905809702507E-4]    |\n",
      "|804    |2       |0.0       |[0.5073442483512385,0.4672916748650037,0.025199736484941955,3.777046647253082E-5,1.2656983234335994E-4]   |\n",
      "|805    |2       |0.0       |[0.51282701141498,0.45821844719306715,0.028785602473175343,4.990514215586863E-5,1.1903377662156657E-4]    |\n",
      "|806    |3       |0.0       |[0.4971605274158086,0.4747740899510982,0.027898625657424164,4.735484365615082E-5,1.1940213201306891E-4]   |\n",
      "|807    |2       |1.0       |[0.4712559247952427,0.5043067631234853,0.024289624112365113,3.537422306860781E-5,1.1231374583836052E-4]   |\n",
      "|808    |2       |1.0       |[0.47115758489996085,0.504410788451459,0.024283949484123254,3.537366636245326E-5,1.1230349809443481E-4]   |\n",
      "|809    |3       |1.0       |[0.465478834505267,0.5103325343774859,0.02404718317690495,3.126816926569993E-5,1.1017977107636082E-4]     |\n",
      "|810    |2       |0.0       |[0.7110723476879098,0.26146085699866967,0.027167134263716648,1.3281156781321868E-4,1.6684948189065276E-4] |\n",
      "|811    |3       |0.0       |[0.5110594978628799,0.46654190546110597,0.022085695451973766,1.7156154582522392E-4,1.4133967821515958E-4] |\n",
      "|812    |3       |0.0       |[0.5320571106410517,0.4436666078427413,0.023937639718120657,1.9085743969903958E-4,1.4778435838739485E-4]  |\n",
      "|813    |3       |0.0       |[0.5535934405798028,0.41945220259915067,0.026636201585496736,1.6244027344704213E-4,1.557149621028653E-4]  |\n",
      "|814    |3       |1.0       |[0.40177068707556285,0.5848390290017428,0.01309971909364471,1.805173345062771E-4,1.1004749454327825E-4]   |\n",
      "|815    |2       |1.0       |[0.44464881640370946,0.5376073222900443,0.017478677978471625,1.5177189962631345E-4,1.1341142814824119E-4] |\n",
      "|816    |3       |1.0       |[0.40197122963788345,0.5842828459228262,0.013311493577149227,3.2127555715983627E-4,1.1315530498127907E-4] |\n",
      "|817    |2       |1.0       |[0.4347846680566471,0.5494324963626226,0.015295889439445225,3.656313068846598E-4,1.2131483440027005E-4]   |\n",
      "|818    |2       |0.0       |[0.6820108553846752,0.2989513181930698,0.01737271118063753,0.0014831608637083159,1.8195437790906777E-4]   |\n",
      "|819    |2       |1.0       |[0.41919859265353737,0.565842061463856,0.01460707463825544,2.4301291363914253E-4,1.0925833071218903E-4]   |\n",
      "|820    |2       |0.0       |[0.7204913652055952,0.2532065373285065,0.025837314179367524,2.855571585176602E-4,1.7922612801304496E-4]   |\n",
      "|821    |3       |0.0       |[0.5540674222885634,0.41781038654817715,0.02768524498701036,2.8137250477702725E-4,1.5557367147205208E-4]  |\n",
      "|822    |2       |1.0       |[0.4672012755288734,0.5173222077994253,0.014996467143250689,3.531901421582761E-4,1.268593862923977E-4]    |\n",
      "|823    |3       |1.0       |[0.43617586533051617,0.5498160133421911,0.013614557645088081,2.729416630006161E-4,1.206220192040216E-4]   |\n",
      "|824    |3       |1.0       |[0.47895335761588453,0.49404224754951415,0.026827813527286738,6.359653461012511E-5,1.1298477270440999E-4] |\n",
      "|825    |2       |0.0       |[0.7602656647083339,0.20796419901920177,0.029287499759075252,0.0022533626549987055,2.2927385839046068E-4] |\n",
      "|826    |2       |0.0       |[0.5273925667009629,0.44424347159157845,0.0277918967832512,4.1668817996464607E-4,1.553767442429334E-4]    |\n",
      "|827    |2       |0.0       |[0.5062211546677412,0.4674351801080908,0.02582719940532709,3.673102946650671E-4,1.4915552417587587E-4]    |\n",
      "|828    |3       |0.0       |[0.527380533361328,0.44425622596429,0.027791176552550995,4.1668865668535054E-4,1.5537546514575437E-4]     |\n",
      "|829    |2       |1.0       |[0.42621934938671024,0.5560120345894675,0.017279965464719188,3.555917006323366E-4,1.3305885847088775E-4]  |\n",
      "|830    |3       |1.0       |[0.4171718719673712,0.5662291469692842,0.016115703598151686,3.5707970672120874E-4,1.2619775847162216E-4]  |\n",
      "|831    |3       |1.0       |[0.4522817188345251,0.5193458541847474,0.02812221910157247,1.2083758159870683E-4,1.293702975561614E-4]    |\n",
      "|832    |2       |1.0       |[0.4412510899666321,0.5287984317499046,0.02974241370948569,8.74388313573361E-5,1.2062574262026948E-4]     |\n",
      "|833    |2       |1.0       |[0.4396985369280561,0.5425171958615824,0.01729480332646844,3.555657585982638E-4,1.338981252947278E-4]     |\n",
      "|834    |2       |1.0       |[0.41610429199688237,0.5655046448047971,0.017841950267490386,4.1088838380983237E-4,1.3822454702025933E-4] |\n",
      "|835    |2       |1.0       |[0.4161097560704272,0.5654989196308012,0.017842209510617064,4.10889335904746E-4,1.3822545224973068E-4]    |\n",
      "|836    |2       |0.0       |[0.6750419398027689,0.300906247058206,0.021814091395284837,0.0020157349525094597,2.2198679123070349E-4]   |\n",
      "|837    |3       |1.0       |[0.4683932081483035,0.5043596675803069,0.027062434600682462,5.484044102395836E-5,1.2984922968320548E-4]   |\n",
      "|838    |2       |1.0       |[0.4765525296016948,0.49612247082668093,0.02714851061343489,4.868862399996772E-5,1.2780033418929686E-4]   |\n",
      "|839    |2       |0.0       |[0.7200235735326429,0.24676644763995667,0.03281743767882261,2.0880295181065182E-4,1.837381967672686E-4]   |\n",
      "|840    |2       |1.0       |[0.47436157134801377,0.4856897045032539,0.03827237324978169,0.0014932081733547758,1.831427255959229E-4]   |\n",
      "|841    |3       |1.0       |[0.36786222780835764,0.6113783449682787,0.02006087018383235,5.789164012990758E-4,1.1964063823207162E-4]   |\n",
      "|842    |3       |1.0       |[0.4168942968468552,0.5497258642159529,0.032850575103807976,3.916085445622054E-4,1.3765528882176824E-4]   |\n",
      "|843    |3       |1.0       |[0.37754906147436096,0.5882188672417837,0.033811108824363376,2.9602632455498696E-4,1.249361349368357E-4]  |\n",
      "|844    |2       |1.0       |[0.41108734920628653,0.5495514871191227,0.038853677625791776,3.714762972914052E-4,1.3600975150765423E-4]  |\n",
      "|845    |3       |1.0       |[0.38469997275326306,0.5829290846100429,0.03199641046866471,2.554618933108332E-4,1.190702747185236E-4]    |\n",
      "|846    |3       |1.0       |[0.4018378683979101,0.5630260314015461,0.03474747927555774,2.653181097482958E-4,1.2330281523790438E-4]    |\n",
      "|847    |2       |1.0       |[0.40438039696037376,0.5719526426821697,0.021770809843767412,0.001749413580642672,1.4673693304636908E-4]  |\n",
      "|848    |2       |1.0       |[0.3956323611572241,0.5812022807984892,0.02156108355613309,0.0014663927318060196,1.3788175634737738E-4]   |\n",
      "|849    |3       |1.0       |[0.3692321739502648,0.5995666109570008,0.03061687604523786,4.537465527024332E-4,1.3059249479416396E-4]    |\n",
      "|850    |2       |0.0       |[0.6627220508305829,0.28936422242071985,0.04535187995516494,0.0023359981066291007,2.2584868690328906E-4]  |\n",
      "|851    |2       |0.0       |[0.6360339842910288,0.3279192935445505,0.02611827720722082,0.009706593209850988,2.2185174734882756E-4]    |\n",
      "|852    |2       |1.0       |[0.4111917750010877,0.5572545062614874,0.031009782023119378,3.9288359011196213E-4,1.5105312419353572E-4]  |\n",
      "|853    |3       |1.0       |[0.46679280287485275,0.5006701063943068,0.02950570038920102,0.002824101529081882,2.0728881255755417E-4]   |\n",
      "|854    |2       |1.0       |[0.387165454183729,0.5924423271721314,0.017843986807483912,0.0023843278259215827,1.6390401073414508E-4]   |\n",
      "|855    |2       |1.0       |[0.36696248555433864,0.6096820055733791,0.020684656631098825,0.002521386056045406,1.4946618513809307E-4]  |\n",
      "|856    |2       |1.0       |[0.31970547555437967,0.6518780430196748,0.02692047181928839,0.0013756616616042582,1.2034794505289009E-4]  |\n",
      "|857    |2       |1.0       |[0.3910129708422857,0.5750502164928784,0.0334168562997008,3.8990016231033926E-4,1.3005620282510358E-4]    |\n",
      "|858    |2       |0.0       |[0.653511206987598,0.31696168585993695,0.021820822275686665,0.007490869295671873,2.1541558110645473E-4]   |\n",
      "|859    |2       |1.0       |[0.4186729491257151,0.5529799094146239,0.028032705784767114,1.9020582706071287E-4,1.242298478333017E-4]   |\n",
      "|860    |2       |1.0       |[0.45739049891535655,0.5136054207174945,0.02792829164234699,9.045603388441624E-4,1.7122838595778268E-4]   |\n",
      "|861    |2       |1.0       |[0.39286639445569105,0.5854443455391853,0.01962379975956658,0.0019138646600109154,1.5159558554626184E-4]  |\n",
      "|862    |2       |1.0       |[0.3899547314837372,0.5885547449254147,0.01960737315098289,0.001733992678217538,1.4915776164747403E-4]    |\n",
      "|863    |3       |1.0       |[0.40500373868086886,0.5724023759152874,0.020646903448653723,0.0018141273397968834,1.3285461539306204E-4] |\n",
      "|864    |2       |1.0       |[0.44905629331060887,0.5137025532309452,0.03489083841847817,0.0021752201399667112,1.7509490000090438E-4]  |\n",
      "|865    |2       |1.0       |[0.4462012230030033,0.5167543147605622,0.03489954529340658,0.0019724762291622898,1.7244071386554878E-4]   |\n",
      "|866    |2       |0.0       |[0.7026292479782921,0.24258079938056362,0.04272729803542883,0.011790052073523103,2.7260253219241486E-4]   |\n",
      "|867    |3       |1.0       |[0.42409960501889177,0.5404977551886901,0.03409914121585848,0.0011472900478716188,1.5620852868808479E-4]  |\n",
      "|868    |3       |1.0       |[0.388797882224202,0.5867041749220863,0.021888494908279855,0.00247171455961146,1.377333858205248E-4]      |\n",
      "|869    |3       |1.0       |[0.3884435266279516,0.587383054468077,0.021146472150611554,0.0028916258229283237,1.353209304313944E-4]    |\n",
      "|870    |2       |1.0       |[0.39718737116151254,0.5700224429097749,0.03235941166970759,3.0990712375597666E-4,1.2086713524902316E-4]  |\n",
      "|871    |2       |1.0       |[0.4476886944359993,0.5173622171047882,0.0329004711364027,0.001854770076319615,1.9384724649029053E-4]     |\n",
      "|872    |2       |1.0       |[0.3416196595967993,0.6369420423854897,0.019896267329990797,0.0014192131107175871,1.228175770027688E-4]   |\n",
      "|873    |2       |1.0       |[0.39624412638129974,0.5704086126929332,0.03269959538550958,5.164161154843054E-4,1.312494247733211E-4]    |\n",
      "|874    |3       |1.0       |[0.3822175883905619,0.5852786340280289,0.0319473236969243,4.321972955689327E-4,1.2425658891587315E-4]     |\n",
      "|875    |3       |1.0       |[0.4494882627685614,0.5170333287773012,0.031837891291841454,0.0014587307574147556,1.817864048811574E-4]   |\n",
      "|876    |3       |1.0       |[0.36532726793205655,0.6129876428613638,0.019975727450245725,0.0015743739498427622,1.3498780649109031E-4] |\n",
      "|877    |2       |1.0       |[0.3970508966791973,0.5679096195963029,0.03448263986008728,4.1609853064740785E-4,1.40745333764961E-4]     |\n",
      "|878    |2       |1.0       |[0.4389047198529548,0.5268766871736578,0.03195586714336157,0.0020843182177515606,1.7840761227435974E-4]   |\n",
      "|879    |2       |1.0       |[0.33916568601306973,0.6286611768337226,0.028426895966976996,0.00360296159401842,1.4327959221233113E-4]   |\n",
      "|880    |3       |1.0       |[0.31494317273670724,0.6663770567859234,0.016853007098251434,0.0016984691864363375,1.2829419268155532E-4] |\n",
      "|881    |3       |1.0       |[0.325394741831426,0.6532132725028097,0.020342529084441945,9.315159240463128E-4,1.179406572760516E-4]     |\n",
      "|882    |3       |1.0       |[0.40483763247724047,0.571103917124386,0.02118334262866377,0.0027167213096830154,1.5838646002675212E-4]   |\n",
      "|883    |2       |1.0       |[0.3842058209462667,0.5938223751193898,0.019439500871425965,0.0023768191237028904,1.554839392146602E-4]   |\n",
      "|884    |2       |1.0       |[0.4343704587128894,0.5472152617146278,0.01797329655098184,2.8966749421975027E-4,1.5131552728126497E-4]   |\n",
      "|885    |2       |1.0       |[0.3630908990384484,0.6200141678619158,0.016301060787908654,4.6104390461778694E-4,1.3282840710949508E-4]  |\n",
      "|886    |3       |1.0       |[0.35896455861553755,0.6195761289619789,0.020069545193778036,0.001263550679587017,1.2621654911868603E-4]  |\n",
      "|887    |2       |1.0       |[0.37622514213477176,0.6004580725362374,0.02186888539821029,0.0013167537456093808,1.311461851712134E-4]   |\n",
      "|888    |2       |1.0       |[0.3656263362248889,0.612843946690491,0.020140240186179548,0.001258640482455239,1.3083641598530843E-4]    |\n",
      "|889    |2       |1.0       |[0.36562613207227995,0.6128441634014914,0.02014022783607374,0.0012586403134502945,1.3083637670447703E-4]  |\n",
      "|890    |3       |1.0       |[0.31190886162744363,0.662281169661477,0.024536643611762273,0.0011540737016725328,1.1925139764453541E-4]  |\n",
      "|891    |2       |1.0       |[0.36562521338614695,0.6128451386003422,0.020140172260644792,0.001258639552925541,1.30836199940596E-4]    |\n",
      "|892    |2       |1.0       |[0.34551001648156277,0.6348476318948559,0.018426400908171776,0.0010922991772357186,1.2365153817378346E-4] |\n",
      "|893    |2       |1.0       |[0.36775375140022803,0.6105126632065098,0.020315785171238873,0.001286481967117621,1.3131825490578578E-4]  |\n",
      "|894    |2       |1.0       |[0.36775324000175547,0.6105132061643785,0.020315754134737258,0.0012864815420894084,1.3131815703928165E-4] |\n",
      "|895    |2       |1.0       |[0.3530413838361389,0.6270905561549472,0.019043948372783386,6.969153752733603E-4,1.2719626085715128E-4]   |\n",
      "|896    |2       |1.0       |[0.3994040794331691,0.5684574358744228,0.03155738398462542,4.401690203617935E-4,1.4093168742085907E-4]    |\n",
      "|897    |2       |1.0       |[0.39940387297566454,0.5684576604523583,0.03155736594152357,4.401689795028438E-4,1.4093165095078404E-4]   |\n",
      "|898    |2       |1.0       |[0.39322283072305925,0.5723535408890197,0.03391147661677646,3.803780489847362E-4,1.317737221600393E-4]    |\n",
      "|899    |2       |1.0       |[0.3931330147406857,0.572451950067288,0.03390291466394685,3.8036197410603787E-4,1.317585539734569E-4]     |\n",
      "|900    |2       |1.0       |[0.39322272842410505,0.5723536529756722,0.033911466864652344,3.803780306843864E-4,1.3177370488616657E-4]  |\n",
      "|901    |2       |1.0       |[0.39322262612520836,0.5723537650622615,0.033911457112533704,3.803780123840241E-4,1.3177368761230646E-4]  |\n",
      "|902    |2       |0.0       |[0.6524849012254968,0.3062649395821268,0.03870895040868656,0.002313641797321577,2.2756698636819191E-4]    |\n",
      "|903    |2       |0.0       |[0.6498174583820316,0.3090491591729262,0.038805084326043564,0.0021036150328736167,2.2468308612494153E-4]  |\n",
      "|904    |3       |1.0       |[0.32806205274536937,0.6314904861481081,0.04000368043105181,3.1990426212105455E-4,1.2387641334956064E-4]  |\n",
      "|905    |3       |1.0       |[0.4213889839170969,0.5453312568678115,0.031177514560096695,0.0019301076726809313,1.721369823139417E-4]   |\n",
      "|906    |2       |1.0       |[0.46452124185330934,0.4984685584948928,0.035376619206275356,0.0014603363120207495,1.7324413350168643E-4] |\n",
      "|907    |2       |0.0       |[0.6235898249715394,0.3540176555081978,0.020315429710216923,0.0018513799430962725,2.257098669496014E-4]   |\n",
      "|908    |2       |1.0       |[0.3918044417636666,0.5807683902232851,0.0271684405032586,1.318018862088987E-4,1.269256235806441E-4]      |\n",
      "|909    |2       |1.0       |[0.3937696681995522,0.5776609562503263,0.028216217127672118,2.2834870720081773E-4,1.2480971524862978E-4]  |\n",
      "|910    |2       |1.0       |[0.40449159779843974,0.5653839255436997,0.029771467687214198,2.2746644086451615E-4,1.2554252978197857E-4] |\n",
      "|911    |2       |1.0       |[0.3935802470148383,0.5776553997766944,0.028400020327016603,2.4030545814014568E-4,1.2402742331056347E-4]  |\n",
      "|912    |2       |1.0       |[0.39166691266052056,0.5778132240650444,0.03029763569536428,9.436425274585154E-5,1.278633263248128E-4]    |\n",
      "|913    |3       |1.0       |[0.390712972921773,0.5905408205107309,0.017043433224904387,0.0015648259088801592,1.3794743371154237E-4]   |\n",
      "|914    |2       |1.0       |[0.47405584717326965,0.4907181341014321,0.032851659422596206,0.0022020670536218472,1.7229224908010354E-4] |\n",
      "|915    |3       |1.0       |[0.48045656602956316,0.48238697436374295,0.03509493175868317,0.0018912383552733545,1.7028949273725162E-4] |\n",
      "|916    |3       |1.0       |[0.3863792102015823,0.591039602322906,0.020381195175689454,0.0020586695530088557,1.4132274681354697E-4]   |\n",
      "|917    |3       |1.0       |[0.34951016702030757,0.6227320391218718,0.025506490900639937,0.0021178688140508456,1.334341431298199E-4]  |\n",
      "|918    |2       |0.0       |[0.6396882144572287,0.3220683583613823,0.025595225756579198,0.012429640926769276,2.1856049804050683E-4]   |\n",
      "|919    |3       |1.0       |[0.35003050945462605,0.6328663745282862,0.015917689612197687,0.001068111353220233,1.1731505166982278E-4]  |\n",
      "|920    |3       |1.0       |[0.38247962373305716,0.5975061503974985,0.018532766716633106,0.0013516782259802892,1.297809268308375E-4]  |\n",
      "|921    |3       |1.0       |[0.34573363965593007,0.6357591367502079,0.017025902307283895,0.001360734223424119,1.2058706315419416E-4]  |\n",
      "|922    |2       |1.0       |[0.3843432215303069,0.5961085327472188,0.01892094168531866,5.059438244139083E-4,1.2136021274181606E-4]    |\n",
      "|923    |2       |0.0       |[0.4851482109960502,0.4783699934062566,0.03499015037019398,0.0013209847694128742,1.7066045808645323E-4]   |\n",
      "|924    |2       |1.0       |[0.41109292916119217,0.5672900485563407,0.01985689368475467,0.0016321803050163986,1.2794829269611955E-4]  |\n",
      "|925    |2       |1.0       |[0.39000031851732536,0.5881636481196321,0.019849078955405008,0.0018453121912610812,1.416422163764708E-4]  |\n",
      "|926    |2       |1.0       |[0.39000115487127124,0.5881627638607473,0.019849125875887042,0.0018453130182187276,1.416423738757407E-4]  |\n",
      "|927    |2       |1.0       |[0.35722702308786813,0.6124670850588383,0.02828848008731332,0.001887034827211525,1.303769387687598E-4]    |\n",
      "|928    |2       |1.0       |[0.3544388308179889,0.6154703368538382,0.028253605433737863,0.001708997830843202,1.2822906359180185E-4]   |\n",
      "|929    |3       |1.0       |[0.33787405639530393,0.6441128457750295,0.01699763590874789,8.983237781792745E-4,1.1713814273939427E-4]   |\n",
      "|930    |3       |1.0       |[0.3875103203848981,0.5805326751937259,0.03154539229866207,2.808816008407162E-4,1.3073052187333162E-4]    |\n",
      "|931    |3       |1.0       |[0.3995292660397427,0.57691905638832,0.021568176762643984,0.0018231053724849522,1.6039543680830372E-4]    |\n",
      "|932    |2       |1.0       |[0.3843955547093436,0.5920486229347509,0.02119723244891874,0.002217127218513182,1.4146268847349086E-4]    |\n",
      "|933    |3       |1.0       |[0.30931333886186707,0.6639169074186654,0.02535730771184299,0.001285317337820576,1.271286698040602E-4]    |\n",
      "|934    |2       |1.0       |[0.3747120911077749,0.6062855945168444,0.017702204329411577,0.0011697101478594044,1.3039989810950526E-4]  |\n",
      "|935    |2       |1.0       |[0.37457576723486213,0.6028265499297283,0.021431004470554574,0.0010347369939976685,1.3194137085755406E-4] |\n",
      "|936    |3       |1.0       |[0.41092969108035154,0.5576919243077056,0.030786606394906903,4.544321296109555E-4,1.373460874248781E-4]   |\n",
      "|937    |2       |1.0       |[0.4641185526657554,0.49768097426580354,0.035354673805523686,0.002663949361197601,1.8184990171966905E-4]  |\n",
      "|938    |3       |1.0       |[0.3933936787659185,0.5861676844027592,0.018725529613989482,0.0015512557310038188,1.618514863288761E-4]   |\n",
      "|939    |3       |1.0       |[0.3942330829092904,0.5819001745065648,0.021817157916856737,0.001891036731920193,1.585479353679079E-4]    |\n",
      "|940    |3       |1.0       |[0.3717077110748598,0.6100929870035686,0.01686348177031082,0.0011797130775878435,1.5610707367287333E-4]   |\n",
      "|941    |2       |1.0       |[0.3568597488492935,0.6258608632960317,0.01590144178049958,0.0012268600903278133,1.510859838473701E-4]    |\n",
      "|942    |2       |1.0       |[0.3507155372137084,0.6324071231710984,0.01567490191999363,0.0010544712275682194,1.4796646763134491E-4]   |\n",
      "|943    |2       |1.0       |[0.35071523400194576,0.6324074415355713,0.01567488707877709,0.0010544709867063946,1.4796639699974203E-4]  |\n",
      "|944    |3       |1.0       |[0.36748364848639914,0.6136796019297678,0.01779718241109107,8.870132344925509E-4,1.5255393824953787E-4]   |\n",
      "|945    |3       |1.0       |[0.36031721433519964,0.618473513853005,0.01944178239744629,0.0016300795357204352,1.3740987862854564E-4]   |\n",
      "|946    |2       |1.0       |[0.3563058743936157,0.6238485853267564,0.01845096764258688,0.0012631850762062041,1.313875608348755E-4]    |\n",
      "|947    |3       |1.0       |[0.3364416532410683,0.6454708681923879,0.01686824080349,0.0010951765108365831,1.2406125221726128E-4]      |\n",
      "|948    |2       |1.0       |[0.3564010816731289,0.6237478672379271,0.018456373578811813,0.0012632707853954476,1.3140672473658128E-4]  |\n",
      "|949    |2       |1.0       |[0.3563091153278846,0.6238451568025216,0.018451151661554015,0.0012631879947692503,1.3138821327057828E-4]  |\n",
      "|950    |2       |1.0       |[0.36591038143471677,0.6145107831811466,0.018285935108919862,0.0011604609580848132,1.3243931713184686E-4] |\n",
      "|951    |2       |1.0       |[0.36581582941167984,0.6146106142369634,0.018280747304784004,0.0011603881777406118,1.3242086883225612E-4] |\n",
      "|952    |2       |1.0       |[0.3759112009246682,0.6039221202625441,0.019056749542662964,9.772483876250156E-4,1.3268088249976357E-4]   |\n",
      "|953    |2       |1.0       |[0.3730092011664312,0.6069402843349836,0.019034881317493042,8.851266388944501E-4,1.3050654219789223E-4]   |\n",
      "|954    |2       |1.0       |[0.3844297004279631,0.5679791356200945,0.04698937280920881,4.6860174441009473E-4,1.3318939832331883E-4]   |\n",
      "|955    |2       |0.0       |[0.6645651583828491,0.2943933794707018,0.039099593298268856,0.0017244916682451585,2.1737717993506662E-4]  |\n",
      "|956    |2       |1.0       |[0.3758941724987684,0.5901232817612723,0.03364007608500547,2.1147882646250796E-4,1.3099082849123107E-4]   |\n",
      "|957    |2       |1.0       |[0.38318444242514793,0.5817237320984164,0.03475820378321053,2.0292468168542834E-4,1.3069701153951225E-4]  |\n",
      "|958    |3       |1.0       |[0.48240189464756705,0.48388718789186436,0.03193721444058755,0.0015924651074293236,1.8123791255158115E-4] |\n",
      "|959    |2       |1.0       |[0.47451799277588474,0.4924578054093169,0.03154195853416762,0.0013044445791346635,1.7779870149600344E-4]  |\n",
      "|960    |2       |1.0       |[0.47451692990140193,0.4924589479083632,0.03154187923420948,0.001304444423300787,1.7779853272458236E-4]   |\n",
      "|961    |2       |1.0       |[0.4456774000648452,0.5191191807271687,0.03397361309402551,0.0010537178851201384,1.7608822884042857E-4]   |\n",
      "|962    |2       |1.0       |[0.45206444219376485,0.5113924855491244,0.03538589176790268,9.825282324799406E-4,1.746522567281531E-4]    |\n",
      "|963    |3       |1.0       |[0.3689659707200067,0.5904908695175989,0.04005313137913142,3.670008447791205E-4,1.2302753848385213E-4]    |\n",
      "|964    |3       |1.0       |[0.37962589646404454,0.598299089438156,0.020329460650504054,0.0016002603928002052,1.4529305449507675E-4]  |\n",
      "|965    |3       |1.0       |[0.3672885593571155,0.6146048200412617,0.016969923040616522,9.909451199315382E-4,1.4575244107477994E-4]   |\n",
      "|966    |3       |1.0       |[0.36728876505128266,0.614604603733983,0.01696993347501109,9.90945254646255E-4,1.4575248507723082E-4]     |\n",
      "|967    |2       |1.0       |[0.36727498363721117,0.6146190962201113,0.01696923437742667,9.909362283713548E-4,1.4574953687929208E-4]   |\n",
      "|968    |2       |1.0       |[0.3449457849155685,0.6390131423044986,0.015216711507903725,6.854068253367672E-4,1.3895444669254176E-4]   |\n",
      "|969    |3       |1.0       |[0.35982947018267514,0.6219793347234734,0.017395743367949794,6.54940985662921E-4,1.405107402386555E-4]    |\n",
      "|970    |2       |1.0       |[0.357248146321772,0.6251340676053907,0.016842535935349956,6.344344856714408E-4,1.408156518158051E-4]     |\n",
      "|971    |2       |1.0       |[0.3572505877208164,0.6251314983888847,0.01684266211971386,6.344355926453406E-4,1.4081617793957094E-4]    |\n",
      "|972    |2       |1.0       |[0.3479489934788915,0.6342369639278954,0.016933783051175173,7.422815893088099E-4,1.3797795272888362E-4]   |\n",
      "|973    |3       |1.0       |[0.3154032733540032,0.669491845504291,0.014324408619741354,6.534435756128857E-4,1.2702894635154032E-4]    |\n",
      "|974    |2       |1.0       |[0.3349321215593862,0.6484659596834904,0.01571150364380646,7.555403208914131E-4,1.3487479242541486E-4]    |\n",
      "|975    |2       |1.0       |[0.3756201736024831,0.6010707988170022,0.021908450811162345,0.0012634190358374743,1.37157733514966E-4]    |\n",
      "|976    |2       |1.0       |[0.45335719617689185,0.5142404817044668,0.031224593523947294,0.0010055540667465337,1.721745279475378E-4]  |\n",
      "|977    |3       |1.0       |[0.40980010927946353,0.5652569106289422,0.022935506856307505,0.001857179861350403,1.502933739362855E-4]   |\n",
      "|978    |3       |1.0       |[0.33525685903618413,0.6397297217122213,0.023657719088721423,0.001208477982493829,1.4722218037928095E-4]  |\n",
      "|979    |3       |1.0       |[0.33614765474042013,0.6533714258858642,0.009347186017442088,9.954197929158695E-4,1.383135633576958E-4]   |\n",
      "|980    |2       |1.0       |[0.40000580366085625,0.5774837902583003,0.02085692674877452,0.0015056800003663252,1.4779933170255363E-4]  |\n",
      "|981    |2       |1.0       |[0.36214505593747776,0.6169505376744945,0.020615890245367013,1.475249005773736E-4,1.409912420835736E-4]   |\n",
      "|982    |3       |1.0       |[0.3320721151712938,0.6454273706332955,0.021877594395191872,4.969156183340779E-4,1.2600418188471484E-4]   |\n",
      "|983    |2       |1.0       |[0.3827387433142903,0.5888040894884163,0.028027068050406227,2.840105296196009E-4,1.460886172675846E-4]    |\n",
      "|984    |3       |0.0       |[0.6539216706084057,0.31071697746966637,0.027295219297939426,0.007838478185124037,2.2765443886456907E-4]  |\n",
      "|985    |3       |1.0       |[0.3958064847250837,0.5817155908777818,0.02105284843776417,0.0012866804046699118,1.383955547003859E-4]    |\n",
      "|986    |3       |1.0       |[0.3958067993553234,0.5817152575198635,0.021052866904723504,0.0012866806089643784,1.383956111250382E-4]   |\n",
      "|987    |2       |1.0       |[0.3928672320440336,0.5847959467366174,0.021034918538613646,0.0011657347945443276,1.3616788619118887E-4]  |\n",
      "|988    |3       |1.0       |[0.3794877213464542,0.6012842788646247,0.01868847902730708,4.071460022239614E-4,1.3237475939025908E-4]    |\n",
      "|989    |2       |1.0       |[0.3850447700522155,0.5936132606186039,0.020475270919221005,7.294457416949826E-4,1.3725266826472216E-4]   |\n",
      "|990    |2       |1.0       |[0.37162763658999404,0.6073637188954363,0.0205605676975897,3.238237363094908E-4,1.242530806706386E-4]     |\n",
      "|991    |3       |1.0       |[0.3472873661800851,0.6329386248388384,0.01892154446917961,7.322585001575643E-4,1.2020601173908886E-4]    |\n",
      "|992    |2       |1.0       |[0.34704990970135147,0.635139118318774,0.017274873475555712,4.0866683127736E-4,1.2743167304148494E-4]     |\n",
      "|993    |2       |1.0       |[0.372030903172199,0.6066516106258923,0.020554579136662008,6.424056428921367E-4,1.205014223543539E-4]     |\n",
      "|994    |2       |1.0       |[0.3597828647855304,0.6235239898723032,0.01636274330031195,2.0233703129526442E-4,1.2806501055918898E-4]   |\n",
      "|995    |2       |1.0       |[0.4312201002609105,0.5355159047107237,0.0328333955001033,2.9319682105203E-4,1.3740270721034287E-4]       |\n",
      "|996    |2       |1.0       |[0.43024686214546126,0.5339820502896985,0.03540033210801028,2.4180702886421486E-4,1.28948427965843E-4]    |\n",
      "|997    |3       |1.0       |[0.37220160114425876,0.5963863812886148,0.03107360133966789,2.1106952373327925E-4,1.2734670372547044E-4]  |\n",
      "|998    |3       |1.0       |[0.39241580026232525,0.5761897354928016,0.031131687863525933,1.4227287915990678E-4,1.2050350218714003E-4] |\n",
      "|999    |2       |1.0       |[0.42333823842267193,0.5517458508457006,0.023438208018486666,0.001328453848339636,1.4924886480119744E-4]  |\n",
      "+-------+--------+----------+----------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %% [python]\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, unix_timestamp, when,\n",
    "    hour, dayofweek, month, year,\n",
    "    monotonically_increasing_id\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import StringIndexerModel, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from pyspark.ml import Pipeline\n",
    "import os\n",
    "\n",
    "# 1) Spark başlat\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LR Batch Predict 100 Rows (full)\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\",\"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2) df_no_na’yı baştan oluştur\n",
    "df = spark.read.csv(\"US_Accidents_March23.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 2.1) İstenmeyen sütunları düş\n",
    "drop_cols = [\n",
    "    \"ID\",\"Source\",\"Zipcode\",\"Timezone\",\"Airport_Code\",\"Amenity\",\"Bump\",\n",
    "    \"Give_Way\",\"No_Exit\",\"Railway\",\"Description\",\"County\",\"Roundabout\",\n",
    "    \"Station\",\"Stop\",\"Nautical_Twilight\",\"Astronomical_Twilight\",\"Country\"\n",
    "]\n",
    "df2 = df.drop(*drop_cols)\n",
    "\n",
    "# 2.2) Süre & tarih/saat özellikleri\n",
    "df3 = (\n",
    "    df2\n",
    "    .withColumn(\"Start_TS\", to_timestamp(col(\"Start_Time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .withColumn(\"End_TS\",   to_timestamp(col(\"End_Time\"),   \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .withColumn(\"Duration\",\n",
    "        ((unix_timestamp(col(\"End_TS\")) - unix_timestamp(col(\"Start_TS\"))) / 60)\n",
    "        .cast(DoubleType())\n",
    "    )\n",
    "    .withColumn(\"HourOfDay\", hour(col(\"Start_TS\")))\n",
    "    .withColumn(\"DayOfWeek\", dayofweek(col(\"Start_TS\")))\n",
    "    .withColumn(\"Month\",    month(col(\"Start_TS\")))\n",
    "    .withColumn(\"Year\",     year(col(\"Start_TS\")))\n",
    "    .drop(\"Start_TS\",\"End_TS\",\"Start_Time\",\"End_Time\")\n",
    ")\n",
    "\n",
    "# 2.3) City/Street cardinality düşürme\n",
    "def clean_column(df, column_name, top_n=32):\n",
    "    top_vals = [\n",
    "        r[column_name] for r in\n",
    "        df.groupBy(column_name).count()\n",
    "          .orderBy(col(\"count\").desc())\n",
    "          .limit(top_n)\n",
    "          .collect()\n",
    "    ]\n",
    "    return df.withColumn(\n",
    "        f\"{column_name}_Cleaned\",\n",
    "        when(col(column_name).isin(top_vals), col(column_name)).otherwise(\"Other\")\n",
    "    )\n",
    "\n",
    "for c in [\"City\",\"Street\"]:\n",
    "    df3 = clean_column(df3, c, top_n=32)\n",
    "\n",
    "# 2.4) İlgili sütunları seç ve dropna\n",
    "selected_cols = [\n",
    "    \"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\", \"Precipitation(in)\", \"Wind_Chill(F)\", \"Traffic_Signal\",\n",
    "    \"Weather_Condition\",\"Wind_Direction\",\"Civil_Twilight\",\"Sunrise_Sunset\",\n",
    "    \"State\",\"City_Cleaned\",\"Street_Cleaned\",\"Junction\",\"Duration\",\"Severity\",\n",
    "    \"HourOfDay\",\"DayOfWeek\",\"Month\",\"Year\"\n",
    "]\n",
    "df_no_na = df3.select(*selected_cols).dropna().cache()\n",
    "\n",
    "# 3) 100 satırlık örnek + row_id\n",
    "sample100 = df_no_na.limit(1000) \\\n",
    "                   .withColumn(\"_row_id\", monotonically_increasing_id())\n",
    "\n",
    "# 4) Küçük‐model’den indexer aşamaları (00…07) yükle\n",
    "base = \"models/us_accidents_dt_final_last_full_spark/stages\"\n",
    "indexer_paths = [os.path.join(base, d) for d in sorted(os.listdir(base)) if d.startswith(\"0\") and \"StringIndexer\" in d]\n",
    "prep_indexers = [StringIndexerModel.load(p) for p in indexer_paths]\n",
    "\n",
    "# 5) VectorAssembler’ı yeniden tanımla\n",
    "feature_cols = [\n",
    "    \"Temperature(F)\",\"Humidity(%)\",\"Pressure(in)\",\"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\",\"Precipitation(in)\",\"Wind_Chill(F)\",\"Traffic_Signal\",\n",
    "    \"Junction\",\"Duration\",\n",
    "    \"HourOfDay\",\"DayOfWeek\",\"Month\",\"Year\"\n",
    "] + [c + \"_Idx\" for c in [\n",
    "    \"Weather_Condition\",\"Wind_Direction\",\"Civil_Twilight\",\n",
    "    \"Sunrise_Sunset\",\"State\",\"City_Cleaned\",\"Street_Cleaned\"\n",
    "]]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "\n",
    "# 6) Son aşama olarak final LR modelini yükle\n",
    "lr_stage = \"models/us_accidents_lr_final_full_spark/stages/09_LogisticRegression_ea33fdabd04f\"\n",
    "lr_final = LogisticRegressionModel.load(lr_stage)\n",
    "\n",
    "# 7) Pipeline’ı oluştur ve tahmin yap\n",
    "pipe_full = Pipeline(stages = prep_indexers + [assembler, lr_final])\n",
    "predictions = pipe_full.fit(df_no_na).transform(sample100) \\\n",
    "                       .select(\"_row_id\",\"Severity\",\"prediction\",\"probability\")\n",
    "\n",
    "# 8) Sonuçları göster\n",
    "predictions.show(1000, truncate=False)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2634ffa",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3359811427.py, line 98)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[20], line 98\u001b[1;36m\u001b[0m\n\u001b[1;33m    Row(ID=\"test1\"\"Start_Time\": \"2023-12-10 07:00:00\",\u001b[0m\n\u001b[1;37m                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, unix_timestamp, when,\n",
    "    hour, dayofweek, month, year,\n",
    "    monotonically_increasing_id\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import StringIndexerModel, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from pyspark.ml import Pipeline\n",
    "import os\n",
    "from pyspark.sql import Row\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "# 1) Spark başlat\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LR Batch Predict Dynamic Columns\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\",\"16g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2) df_no_na’yı baştan oluştur (model eğitimi için gerekli)\n",
    "# Orijinal büyük veri setini okuma\n",
    "# Lütfen \"US_Accidents_March23.csv\" dosyasının bu kodun çalıştığı dizinde olduğundan emin olun.\n",
    "df = spark.read.csv(\"US_Accidents_March23.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 2.1) İstenmeyen sütunları düş\n",
    "drop_cols = [\n",
    "    \"ID\",\"Source\",\"Zipcode\",\"Timezone\",\"Airport_Code\",\"Amenity\",\"Bump\",\n",
    "    \"Give_Way\",\"No_Exit\",\"Railway\",\"Description\",\"County\",\"Roundabout\",\n",
    "    \"Station\",\"Stop\",\"Nautical_Twilight\",\"Astronomical_Twilight\",\"Country\"\n",
    "]\n",
    "df2 = df.drop(*drop_cols)\n",
    "\n",
    "# 2.2) Süre & tarih/saat özellikleri\n",
    "df3 = (\n",
    "    df2\n",
    "    .withColumn(\"Start_TS\", to_timestamp(col(\"Start_Time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .withColumn(\"End_TS\",   to_timestamp(col(\"End_Time\"),   \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .withColumn(\"Duration\",\n",
    "        ((unix_timestamp(col(\"End_TS\")) - unix_timestamp(col(\"Start_TS\"))) / 60)\n",
    "        .cast(DoubleType())\n",
    "    )\n",
    "    .withColumn(\"HourOfDay\", hour(col(\"Start_TS\")))\n",
    "    .withColumn(\"DayOfWeek\", dayofweek(col(\"Start_TS\")))\n",
    "    .withColumn(\"Month\",    month(col(\"Start_TS\")))\n",
    "    .withColumn(\"Year\",     year(col(\"Start_TS\")))\n",
    "    .drop(\"Start_TS\",\"End_TS\",\"Start_Time\",\"End_Time\")\n",
    ")\n",
    "\n",
    "# 2.3) City/Street cardinality düşürme fonksiyonu\n",
    "def clean_column(df_input, column_name, top_n=32, top_vals_cache=None):\n",
    "    if top_vals_cache is None:\n",
    "        print(f\"'{column_name}' için top {top_n} değerler hesaplanıyor (Eğitim Verisi)...\")\n",
    "        top_vals = [\n",
    "            r[column_name] for r in\n",
    "            df_input.groupBy(column_name).count()\n",
    "              .orderBy(col(\"count\").desc())\n",
    "              .limit(top_n)\n",
    "              .collect()\n",
    "        ]\n",
    "        return df_input.withColumn(\n",
    "            f\"{column_name}_Cleaned\",\n",
    "            when(col(column_name).isin(top_vals), col(column_name)).otherwise(\"Other\")\n",
    "        ), top_vals\n",
    "    else:\n",
    "        print(f\"'{column_name}' için önbelleklenmiş top değerler kullanılıyor (Test Verisi)...\")\n",
    "        return df_input.withColumn(\n",
    "            f\"{column_name}_Cleaned\",\n",
    "            when(col(column_name).isin(top_vals_cache), col(column_name)).otherwise(\"Other\")\n",
    "        ), top_vals_cache\n",
    "\n",
    "# Eğitim verisi için City/Street temizliği ve top değerleri önbellekleme\n",
    "df3, city_top_vals = clean_column(df3, \"City\", top_n=32)\n",
    "df3, street_top_vals = clean_column(df3, \"Street\", top_n=32)\n",
    "\n",
    "\n",
    "# 2.4) İlgili sütunları seç ve dropna (Eğitim verisi için)\n",
    "selected_cols_training = [\n",
    "    \"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\", \"Precipitation(in)\", \"Wind_Chill(F)\", \"Traffic_Signal\",\n",
    "    \"Weather_Condition\",\"Wind_Direction\",\"Civil_Twilight\",\"Sunrise_Sunset\",\n",
    "    \"State\",\"City_Cleaned\",\"Street_Cleaned\",\"Junction\",\"Duration\",\"Severity\",\n",
    "    \"HourOfDay\",\"DayOfWeek\",\"Month\",\"Year\"\n",
    "]\n",
    "df_no_na = df3.select(*selected_cols_training).dropna().cache()\n",
    "\n",
    "\n",
    "# --- Yeni Veri Yükleme ve Hazırlama Kısmı ---\n",
    "# Örnek yeni veri oluşturma. Sütun adları Kaggle veri setindeki orijinal isimlerle eşleşiyor.\n",
    "new_raw_data = [\n",
    "    Row(ID=\"test1\", Source=\"MapQuest\", Zipcode=\"10001\", Timezone=\"US/Eastern\", Airport_Code=\"KJFK\", Amenity=False, Bump=False, Give_Way=False, No_Exit=False, Railway=False, Description=\"Test description 1\", County=\"New York\", Roundabout=False, Station=False, Stop=False, Nautical_Twilight=\"Day\", Astronomical_Twilight=\"Day\", Country=\"US\",\n",
    "        Start_Time=\"2025-07-22 10:00:00\", End_Time=\"2025-07-22 10:30:00\",\n",
    "        **{'Temperature(F)': 75.0, 'Humidity(%)': 60.0, 'Pressure(in)': 29.9, 'Visibility(mi)': 10.0,\n",
    "           'Wind_Speed(mph)': 5.0, 'Precipitation(in)': 0.0, 'Wind_Chill(F)': 70.0},\n",
    "        Traffic_Signal=True, Weather_Condition=\"Clear\", Wind_Direction=\"NW\", Civil_Twilight=\"Day\", Sunrise_Sunset=\"Day\",\n",
    "        State=\"NY\", City=\"New York\", Street=\"Broadway\", Junction=False, Severity=2,\n",
    "        Start_Lat=40.7, Start_Lng=-74.0, End_Lat=40.7, End_Lng=-74.0),\n",
    "    Row(ID=\"test2\", Source=\"MapQuest\", Zipcode=\"90210\", Timezone=\"US/Pacific\", Airport_Code=\"KLAX\", Amenity=False, Bump=False, Give_Way=False, No_Exit=False, Railway=False, Description=\"Test description 2\", County=\"Los Angeles\", Roundabout=False, Station=False, Stop=False, Nautical_Twilight=\"Night\", Astronomical_Twilight=\"Night\", Country=\"US\",\n",
    "        Start_Time=\"2025-07-22 18:00:00\", End_Time=\"2025-07-22 18:45:00\",\n",
    "        **{'Temperature(F)': 80.0, 'Humidity(%)': 50.0, 'Pressure(in)': 30.0, 'Visibility(mi)': 8.0,\n",
    "           'Wind_Speed(mph)': 10.0, 'Precipitation(in)': 0.0, 'Wind_Chill(F)': 78.0},\n",
    "        Traffic_Signal=False, Weather_Condition=\"Fair\", Wind_Direction=\"SW\", Civil_Twilight=\"Night\", Sunrise_Sunset=\"Sunset\",\n",
    "        State=\"CA\", City=\"Beverly Hills\", Street=\"Wilshire Blvd\", Junction=True, Severity=3,\n",
    "        Start_Lat=34.0, Start_Lng=-118.2, End_Lat=34.0, End_Lng=-118.2),\n",
    "    Row(ID=\"test3\", Source=\"MapQuest\", Zipcode=\"60601\", Timezone=\"US/Central\", Airport_Code=\"KORD\", Amenity=True, Bump=False, Give_Way=False, No_Exit=False, Railway=False, Description=\"Test description 3\", County=\"Cook\", Roundabout=False, Station=True, Stop=False, Nautical_Twilight=\"Day\", Astronomical_Twilight=\"Day\", Country=\"US\",\n",
    "        Start_Time=\"2025-07-22 13:00:00\", End_Time=\"2025-07-22 14:15:00\",\n",
    "        **{'Temperature(F)': 68.0, 'Humidity(%)': 70.0, 'Pressure(in)': 29.8, 'Visibility(mi)': 7.0,\n",
    "           'Wind_Speed(mph)': 8.0, 'Precipitation(in)': 0.1, 'Wind_Chill(F)': 65.0},\n",
    "        Traffic_Signal=True, Weather_Condition=\"Rain\", Wind_Direction=\"E\", Civil_Twilight=\"Day\", Sunrise_Sunset=\"Day\",\n",
    "        State=\"IL\", City=\"Chicago\", Street=\"Michigan Ave\", Junction=False, Severity=4,\n",
    "        Start_Lat=41.8, Start_Lng=-87.6, End_Lat=41.8, End_Lng=-87.6)\n",
    "]\n",
    "new_data = spark.createDataFrame(new_raw_data)\n",
    "\n",
    "# Yeni veriye de aynı dönüşümleri uygula (drop_cols, zaman özellikleri)\n",
    "new_data_cleaned_step1 = new_data.drop(*drop_cols)\n",
    "new_data_cleaned_step2 = (\n",
    "    new_data_cleaned_step1\n",
    "    .withColumn(\"Start_TS\", to_timestamp(col(\"Start_Time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .withColumn(\"End_TS\",   to_timestamp(col(\"End_Time\"),   \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .withColumn(\"Duration\",\n",
    "        ((unix_timestamp(col(\"End_TS\")) - unix_timestamp(col(\"Start_TS\"))) / 60)\n",
    "        .cast(DoubleType())\n",
    "    )\n",
    "    .withColumn(\"HourOfDay\", hour(col(\"Start_TS\")))\n",
    "    .withColumn(\"DayOfWeek\", dayofweek(col(\"Start_TS\")))\n",
    "    .withColumn(\"Month\",    month(col(\"Start_TS\")))\n",
    "    .withColumn(\"Year\",     year(col(\"Start_TS\")))\n",
    "    .drop(\"Start_TS\",\"End_TS\",\"Start_Time\",\"End_Time\")\n",
    ")\n",
    "\n",
    "# Yeni test verisi için City/Street temizliği - BU SATIRLARI AKTİF HALE GETİRMELİSİNİZ!\n",
    "new_data_cleaned_step2, _ = clean_column(new_data_cleaned_step2, \"City\", top_vals_cache=city_top_vals)\n",
    "new_data_cleaned_step2, _ = clean_column(new_data_cleaned_step2, \"Street\", top_vals_cache=street_top_vals)\n",
    "\n",
    "\n",
    "# Yeni veriden model için gerekli sütunları seç ve NaN değerleri düşür\n",
    "selected_cols_prediction = [\n",
    "    \"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\", \"Precipitation(in)\", \"Wind_Chill(F)\", \"Traffic_Signal\",\n",
    "    \"Weather_Condition\",\"Wind_Direction\",\"Civil_Twilight\",\"Sunrise_Sunset\",\n",
    "    \"State\",\"City_Cleaned\",\"Street_Cleaned\",\"Junction\",\"Duration\",\"Severity\", # City_Cleaned ve Street_Cleaned burada mevcut olmalı\n",
    "    \"HourOfDay\",\"DayOfWeek\",\"Month\",\"Year\"\n",
    "]\n",
    "\n",
    "# select işlemi için doğru sütun listesini kullanın\n",
    "new_data_for_prediction = new_data_cleaned_step2.select(*selected_cols_prediction).dropna() \\\n",
    "                                                .withColumn(\"_row_id\", monotonically_increasing_id())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0b1cf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline eğitim verisi üzerinde fit ediliyor...\n",
      "Pipeline başarıyla fit edildi.\n",
      "\n",
      "Yeni veri üzerinde tahminler yapılıyor...\n",
      "\n",
      "Tahmin sonuçları:\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o4608.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 185.0 failed 1 times, most recent failure: Lost task 0.0 in stage 185.0 (TID 1214) (MSI executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.io.EOFException\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 26 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.io.EOFException\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 26 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# 8) Sonuçları göster\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTahmin sonuçları:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 43\u001b[0m \u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m spark\u001b[38;5;241m.\u001b[39mstop()\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSpark oturumu durduruldu.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\dataframe.py:978\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    971\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    972\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    975\u001b[0m         },\n\u001b[0;32m    976\u001b[0m     )\n\u001b[1;32m--> 978\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o4608.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 185.0 failed 1 times, most recent failure: Lost task 0.0 in stage 185.0 (TID 1214) (MSI executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.io.EOFException\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 26 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.io.EOFException\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 26 more\r\n"
     ]
    }
   ],
   "source": [
    "# 4) Küçük‐model’den indexer aşamaları (00…07) yükle\n",
    "base = \"models/us_accidents_dt_final_last_full_spark/stages\"\n",
    "if not os.path.exists(base):\n",
    "    print(f\"HATA: Model aşamalarının bulunduğu '{base}' dizini bulunamadı. Lütfen model yolunu kontrol edin.\")\n",
    "    spark.stop()\n",
    "    exit()\n",
    "\n",
    "indexer_paths = [os.path.join(base, d) for d in sorted(os.listdir(base)) if d.startswith(\"0\") and \"StringIndexer\" in d]\n",
    "prep_indexers = [StringIndexerModel.load(p) for p in indexer_paths]\n",
    "\n",
    "# 5) VectorAssembler’ı yeniden tanımla\n",
    "feature_cols = [\n",
    "    \"Temperature(F)\",\"Humidity(%)\",\"Pressure(in)\",\"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\",\"Precipitation(in)\",\"Wind_Chill(F)\",\"Traffic_Signal\",\n",
    "    \"Junction\",\"Duration\",\n",
    "    \"HourOfDay\",\"DayOfWeek\",\"Month\",\"Year\"\n",
    "] + [c + \"_Idx\" for c in [\n",
    "    \"Weather_Condition\",\"Wind_Direction\",\"Civil_Twilight\",\n",
    "    \"Sunrise_Sunset\",\"State\",\"City_Cleaned\",\"Street_Cleaned\" # Bu sütunlar artık mevcut olacak\n",
    "]]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "\n",
    "# 6) Son aşama olarak final LR modelini yükle\n",
    "lr_stage = \"models/us_accidents_lr_final_full_spark/stages/09_LogisticRegression_ea33fdabd04f\"\n",
    "if not os.path.exists(lr_stage):\n",
    "    print(f\"HATA: Logistic Regression modelinin bulunduğu '{lr_stage}' dizini bulunamadı. Lütfen model yolunu kontrol edin.\")\n",
    "    spark.stop()\n",
    "    exit()\n",
    "lr_final = LogisticRegressionModel.load(lr_stage)\n",
    "\n",
    "# 7) Pipeline’ı oluştur ve tahmin yap\n",
    "pipe_full = Pipeline(stages = prep_indexers + [assembler, lr_final])\n",
    "print(\"\\nPipeline eğitim verisi üzerinde fit ediliyor...\")\n",
    "model_fitted = pipe_full.fit(df_no_na)\n",
    "print(\"Pipeline başarıyla fit edildi.\")\n",
    "\n",
    "print(\"\\nYeni veri üzerinde tahminler yapılıyor...\")\n",
    "predictions = model_fitted.transform(new_data_for_prediction) \\\n",
    "                          .select(\"_row_id\",\"Severity\",\"prediction\",\"probability\")\n",
    "\n",
    "# 8) Sonuçları göster\n",
    "print(\"\\nTahmin sonuçları:\")\n",
    "predictions.show(truncate=False)\n",
    "\n",
    "spark.stop()\n",
    "print(\"\\nSpark oturumu durduruldu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7b586ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Kaydedilmiş PipelineModel yükleniyor: c:\\Users\\aslay\\Desktop\\BİL 401\\project\\models\\us_accidents_dt_final_last_full_spark\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 44) (MSI executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.io.EOFException\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.io.EOFException\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 32 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 125\u001b[0m\n\u001b[0;32m    122\u001b[0m     sys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[DEBUG] Kaydedilmiş PipelineModel yükleniyor: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(saved_pipeline_model_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 125\u001b[0m loaded_pipeline_model \u001b[38;5;241m=\u001b[39m \u001b[43mPipelineModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43msaved_pipeline_model_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# --- Yeni Gelecek Veriler (Örnek) ---\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# Bu veri formatı, orijinal CSV dosyanızın başlık satırı ile aynı olmalıdır.\u001b[39;00m\n\u001b[0;32m    131\u001b[0m veri_yeni \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    132\u001b[0m     {\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA-12345\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSource\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSource_A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeverity\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m# Severity'yi None veya boş bırakın\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# İstediğiniz kadar yeni veri ekleyebilirsiniz\u001b[39;00m\n\u001b[0;32m    163\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\pyspark\\ml\\util.py:369\u001b[0m, in \u001b[0;36mMLReadable.load\u001b[1;34m(cls, path)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RL:\n\u001b[0;32m    368\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\pyspark\\ml\\pipeline.py:282\u001b[0m, in \u001b[0;36mPipelineModelReader.load\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipelineModel\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 282\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mDefaultParamsReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadMetadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m JavaMLReader(cast(Type[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJavaMLReadable[PipelineModel]\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls))\u001b[38;5;241m.\u001b[39mload(path)\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\pyspark\\ml\\util.py:579\u001b[0m, in \u001b[0;36mDefaultParamsReader.loadMetadata\u001b[1;34m(path, sc, expectedClassName)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;124;03mLoad metadata saved using :py:meth:`DefaultParamsWriter.saveMetadata`\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;124;03m    If non empty, this is checked against the loaded metadata.\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    578\u001b[0m metadataPath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 579\u001b[0m metadataStr \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadataPath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    580\u001b[0m loadedVals \u001b[38;5;241m=\u001b[39m DefaultParamsReader\u001b[38;5;241m.\u001b[39m_parseMetaData(metadataStr, expectedClassName)\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loadedVals\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\pyspark\\rdd.py:2888\u001b[0m, in \u001b[0;36mRDD.first\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2862\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m   2863\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2864\u001b[0m \u001b[38;5;124;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[0;32m   2865\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2886\u001b[0m \u001b[38;5;124;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[0;32m   2887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2888\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2889\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[0;32m   2890\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\pyspark\\rdd.py:2855\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   2852\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2854\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[1;32m-> 2855\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2857\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[0;32m   2858\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\pyspark\\context.py:2510\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   2508\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[0;32m   2509\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2510\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 44) (MSI executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.io.EOFException\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.io.EOFException\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 32 more\r\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, unix_timestamp, when,\n",
    "    hour, dayofweek, month, year,\n",
    "    monotonically_increasing_id\n",
    ")\n",
    "from pyspark.sql.types import DoubleType, StringType # StringType'ı da ekleyelim\n",
    "from pyspark.ml.feature import StringIndexerModel, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from pyspark.ml import PipelineModel # Pipeline'ı direkt PipelineModel olarak yükleyeceğimiz için\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Ortam değişkenlerini ayarlayın (Python worker hataları için önemli)\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "# 1) Spark başlat\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LR Batch Predict New Data\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\",\"8g\") \\\n",
    "    .config(\"spark.pyspark.python\", sys.executable)\\\n",
    "    .config(\"spark.pyspark.driver.python\", sys.executable)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# --- Mevcut Veri Okuma ve Ön İşleme Kısmı (Modelleme için kullanılmaz, sadece pipeline'ın nasıl oluşturulduğunu gösterir) ---\n",
    "# Bu kısım, PipelineModel'in nasıl eğitildiğini ve StringIndexerModel'lerin nasıl kaydedildiğini gösterir.\n",
    "# Yeni veri tahmininde bu adımlara doğrudan ihtiyacımız olmayacak,\n",
    "# çünkü PipelineModel bu dönüşümleri zaten içinde barındırıyor.\n",
    "# Ancak, clean_column fonksiyonunu kullanmaya devam edeceğiz.\n",
    "\n",
    "# 2.3) City/Street cardinality düşürme fonksiyonu\n",
    "# Bu fonksiyon, PipelineModel'in dışındaki ham veri ön işleme adımlarının bir parçasıdır\n",
    "# ve yeni veriler için de uygulanması gerekir.\n",
    "def clean_column(df, column_name, top_n=32):\n",
    "    \"\"\"\n",
    "    Belirli bir sütundaki en sık geçen 'top_n' değeri tutar, diğerlerini 'Other' yapar.\n",
    "    \"\"\"\n",
    "    # collect() sırasında oluşabilecek hatalara karşı StringType'a dönüştürme ve null doldurma\n",
    "    df = df.withColumn(column_name, col(column_name).cast(StringType())).na.fill({column_name: \"\"})\n",
    "\n",
    "    top_vals = [\n",
    "        r[column_name] for r in\n",
    "        df.groupBy(column_name).count()\n",
    "          .orderBy(col(\"count\").desc())\n",
    "          .limit(top_n)\n",
    "          .collect()\n",
    "    ]\n",
    "    return df.withColumn(\n",
    "        f\"{column_name}_Cleaned\",\n",
    "        when(col(column_name).isin(top_vals), col(column_name)).otherwise(\"Other\")\n",
    "    )\n",
    "\n",
    "# --- Yeni Veri İçin Hazırlık Fonksiyonu ---\n",
    "def prepare_new_data(new_df, top_n=32):\n",
    "    \"\"\"\n",
    "    Yeni gelen veriyi, modelin beklediği formata (raw features) hazırlar.\n",
    "    Bu kısım, modelin kendisi tarafından yapılacak StringIndexer ve VectorAssembler\n",
    "    ön işleme adımlarından önceki veriyi hazırlar.\n",
    "    \"\"\"\n",
    "    # 2.2) Süre & tarih/saat özellikleri\n",
    "    processed_df = (\n",
    "        new_df\n",
    "        .withColumn(\"Start_TS\", to_timestamp(col(\"Start_Time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "        .withColumn(\"End_TS\",   to_timestamp(col(\"End_Time\"),   \"yyyy-MM-dd HH:mm:ss\"))\n",
    "        .withColumn(\"Duration\",\n",
    "            ((unix_timestamp(col(\"End_TS\")) - unix_timestamp(col(\"Start_TS\"))) / 60)\n",
    "            .cast(DoubleType())\n",
    "        )\n",
    "        .withColumn(\"HourOfDay\", hour(col(\"Start_TS\")))\n",
    "        .withColumn(\"DayOfWeek\", dayofweek(col(\"Start_TS\")))\n",
    "        .withColumn(\"Month\",    month(col(\"Start_TS\")))\n",
    "        .withColumn(\"Year\",     year(col(\"Start_TS\")))\n",
    "        .drop(\"Start_TS\",\"End_TS\",\"Start_Time\",\"End_Time\")\n",
    "    )\n",
    "\n",
    "    # 2.3) City/Street cardinality düşürme\n",
    "    for c in [\"City\",\"Street\"]:\n",
    "        processed_df = clean_column(processed_df, c, top_n=top_n)\n",
    "\n",
    "    # 2.4) İlgili sütunları seç ve dropna\n",
    "    # Tahmin yapacağımız için \"Severity\" sütununu dahil etmiyoruz\n",
    "    # Ancak eğer test verinizde Severity varsa, yine de dışarıda bırakmanız gerekir.\n",
    "    selected_cols_for_prediction = [\n",
    "        \"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\",\n",
    "        \"Wind_Speed(mph)\", \"Precipitation(in)\", \"Wind_Chill(F)\", \"Traffic_Signal\",\n",
    "        \"Weather_Condition\",\"Wind_Direction\",\"Civil_Twilight\",\"Sunrise_Sunset\",\n",
    "        \"State\",\"City_Cleaned\",\"Street_Cleaned\",\"Junction\",\"Duration\",\n",
    "        \"HourOfDay\",\"DayOfWeek\",\"Month\",\"Year\"\n",
    "    ]\n",
    "    # Sadece tahmin yapacağımız için Severity'yi düşürelim (eğer varsa)\n",
    "    if \"Severity\" in processed_df.columns:\n",
    "        processed_df = processed_df.drop(\"Severity\")\n",
    "\n",
    "    # Tüm gerekli sütunların var olduğundan emin olun, yoksa hata verebilir.\n",
    "    # Burada yalnızca modelin beklediği sütunları seçiyoruz.\n",
    "    # Eğer yeni verinizde bir sütun eksikse, burada hata alırsınız.\n",
    "    # Modelin beklediği tüm sütunların yeni veride de olduğundan emin olun.\n",
    "    final_df_for_pipeline = processed_df.select(*selected_cols_for_prediction).dropna()\n",
    "    \n",
    "    return final_df_for_pipeline\n",
    "\n",
    "\n",
    "### Mevcut PipelineModel'i Yükle\n",
    "\n",
    "\n",
    "# 7) Eğitilmiş PipelineModel'i yükle\n",
    "# Pipeline'ın tamamının kaydedildiği ana yolu belirtin.\n",
    "# Muhtemelen \"models/us_accidents_lr_optimized_small_spark\" veya \"models/us_accidents_lr_final_full_spark\"\n",
    "# Kayıtlı PipelineModel'iniz Logistic Regression'ı zaten içeriyor olmalı.\n",
    "# Bu örnekte, 'lr_optimized_small_spark' modelini kullanıyorum varsayımıyla devam ediyorum.\n",
    "# Eğer final full spark modelini kullanıyorsanız yolu ona göre değiştirin.\n",
    "saved_pipeline_model_path = \"models/us_accidents_dt_final_last_full_spark\" # Veya models/us_accidents_lr_final_full_spark\n",
    "\n",
    "if not os.path.isdir(os.path.abspath(saved_pipeline_model_path)):\n",
    "    print(f\"HATA: Kaydedilmiş PipelineModel klasörü bulunamadı: {os.path.abspath(saved_pipeline_model_path)}\")\n",
    "    spark.stop()\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"[DEBUG] Kaydedilmiş PipelineModel yükleniyor: {os.path.abspath(saved_pipeline_model_path)}\")\n",
    "loaded_pipeline_model = PipelineModel.load(os.path.abspath(saved_pipeline_model_path))\n",
    "\n",
    "\n",
    "\n",
    "# --- Yeni Gelecek Veriler (Örnek) ---\n",
    "# Bu veri formatı, orijinal CSV dosyanızın başlık satırı ile aynı olmalıdır.\n",
    "veri_yeni = [\n",
    "    {\n",
    "        \"ID\": \"A-12345\", \"Source\": \"Source_A\", \"Severity\": None, # Severity'yi None veya boş bırakın\n",
    "        \"Start_Time\": \"2024-07-22 15:30:00\", \"End_Time\": \"2024-07-22 16:00:00\",\n",
    "        \"Start_Lat\": 34.0522, \"Start_Lng\": -118.2437, \"End_Lat\": 34.0530, \"End_Lng\": -118.2440,\n",
    "        \"Distance(mi)\": 0.05, \"Description\": \"Traffic accident on Main St.\",\n",
    "        \"Street\": \"Main St\", \"City\": \"Los Angeles\", \"County\": \"Los Angeles\", \"State\": \"CA\",\n",
    "        \"Zipcode\": \"90012\", \"Country\": \"US\", \"Timezone\": \"US/Pacific\", \"Airport_Code\": \"KLAX\",\n",
    "        \"Weather_Timestamp\": \"2024-07-22 15:00:00\", \"Temperature(F)\": 90.0, \"Wind_Chill(F)\": 88.0,\n",
    "        \"Humidity(%)\": 40.0, \"Pressure(in)\": 29.9, \"Visibility(mi)\": 10.0, \"Wind_Direction\": \"SW\",\n",
    "        \"Wind_Speed(mph)\": 15.0, \"Precipitation(in)\": 0.0, \"Weather_Condition\": \"Clear\",\n",
    "        \"Amenity\": False, \"Bump\": False, \"Crossing\": True, \"Give_Way\": False, \"Junction\": False,\n",
    "        \"No_Exit\": False, \"Railway\": False, \"Roundabout\": False, \"Station\": False, \"Stop\": False,\n",
    "        \"Traffic_Calming\": False, \"Traffic_Signal\": True, \"Turning_Loop\": False, \"Sunrise_Sunset\": \"Day\",\n",
    "        \"Civil_Twilight\": \"Day\", \"Nautical_Twilight\": \"Day\", \"Astronomical_Twilight\": \"Day\"\n",
    "    },\n",
    "    {\n",
    "        \"ID\": \"A-12346\", \"Source\": \"Source_B\", \"Severity\": None,\n",
    "        \"Start_Time\": \"2024-07-22 08:00:00\", \"End_Time\": \"2024-07-22 08:15:00\",\n",
    "        \"Start_Lat\": 38.9072, \"Start_Lng\": -77.0370, \"End_Lat\": 38.9075, \"End_Lng\": -77.0372,\n",
    "        \"Distance(mi)\": 0.02, \"Description\": \"Minor collision on K Street.\",\n",
    "        \"Street\": \"K Street NW\", \"City\": \"Washington\", \"County\": \"District of Columbia\", \"State\": \"DC\",\n",
    "        \"Zipcode\": \"20005\", \"Country\": \"US\", \"Timezone\": \"US/Eastern\", \"Airport_Code\": \"KDCA\",\n",
    "        \"Weather_Timestamp\": \"2024-07-22 07:45:00\", \"Temperature(F)\": 75.0, \"Wind_Chill(F)\": 72.0,\n",
    "        \"Humidity(%)\": 70.0, \"Pressure(in)\": 30.1, \"Visibility(mi)\": 8.0, \"Wind_Direction\": \"E\",\n",
    "        \"Wind_Speed(mph)\": 5.0, \"Precipitation(in)\": 0.0, \"Weather_Condition\": \"Partly Cloudy\",\n",
    "        \"Amenity\": False, \"Bump\": False, \"Crossing\": False, \"Give_Way\": False, \"Junction\": True,\n",
    "        \"No_Exit\": False, \"Railway\": False, \"Roundabout\": False, \"Station\": True, \"Stop\": False,\n",
    "        \"Traffic_Calming\": False, \"Traffic_Signal\": False, \"Turning_Loop\": False, \"Sunrise_Sunset\": \"Day\",\n",
    "        \"Civil_Twilight\": \"Day\", \"Nautical_Twilight\": \"Day\", \"Astronomical_Twilight\": \"Day\"\n",
    "    },\n",
    "    # İstediğiniz kadar yeni veri ekleyebilirsiniz\n",
    "]\n",
    "\n",
    "# Yeni veriyi Spark DataFrame'ine dönüştür\n",
    "df_new_data_raw = spark.createDataFrame(veri_yeni)\n",
    "\n",
    "# Yeni veriyi modelin beklediği ham özellik formatına hazırla\n",
    "df_new_data_prepared = prepare_new_data(df_new_data_raw, top_n=32)\n",
    "\n",
    "# --- Tahmin Yap ---\n",
    "print(\"\\n--- Yeni Veri Üzerinde Tahmin Yapılıyor ---\")\n",
    "if loaded_pipeline_model:\n",
    "    predictions_new_data = loaded_pipeline_model.transform(df_new_data_prepared) \\\n",
    "                                                .select(\"City_Cleaned\", \"Street_Cleaned\", \"prediction\", \"probability\") # İstenen çıktılar\n",
    "    \n",
    "    print(\"\\nYeni Veri Tahmin Sonuçları:\")\n",
    "    predictions_new_data.show(truncate=False)\n",
    "else:\n",
    "    print(\"HATA: Pipeline modeli yüklenemediği için tahmin yapılamadı.\")\n",
    "\n",
    "# 8) Spark'ı durdur\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7044a0",
   "metadata": {},
   "source": [
    "## Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96c4f3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+------------------+-------------------+\n",
      "|Start_Lat|Start_Lng| End_Lat|           End_Lng|        distance_mi|\n",
      "+---------+---------+--------+------------------+-------------------+\n",
      "| 40.10891|-83.09286|40.11206|         -83.03187| 3.2302598454049494|\n",
      "| 39.86542| -84.0628|39.86501|         -84.04873|  0.746718600847293|\n",
      "| 39.10266|-84.52468|39.10209|         -84.52396|0.05514922259749175|\n",
      "| 39.10148|-84.52341|39.09841|         -84.52241|0.21879110549002928|\n",
      "| 41.06213|-81.53784|41.06217|-81.53546999999998|0.12350028305173481|\n",
      "+---------+---------+--------+------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, radians, sin, cos, sqrt, atan2\n",
    "\n",
    "# 1) Spark’ı başlat\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Compute Distance\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2) Ham CSV’yi oku ve orijinal mesafe sütununu at\n",
    "df = spark.read.csv(\"data/traffic_data/US_Accidents_March23.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 3) Koordinatlardan herhangi biri boşsa o satırı at\n",
    "df_clean = df.dropna(subset=[\"Start_Lat\", \"Start_Lng\", \"End_Lat\", \"End_Lng\"])\n",
    "\n",
    "# 4) Haversine ile yeni distance_mi hesapla\n",
    "from pyspark.sql.functions import col, radians, sin, cos, sqrt, atan2\n",
    "\n",
    "df_dist = df_clean.withColumn(\"lat1\", radians(col(\"Start_Lat\"))) \\\n",
    "    .withColumn(\"lon1\", radians(col(\"Start_Lng\"))) \\\n",
    "    .withColumn(\"lat2\", radians(col(\"End_Lat\"))) \\\n",
    "    .withColumn(\"lon2\", radians(col(\"End_Lng\"))) \\\n",
    "    .withColumn(\"dlat\", col(\"lat2\") - col(\"lat1\")) \\\n",
    "    .withColumn(\"dlon\", col(\"lon2\") - col(\"lon1\")) \\\n",
    "    .withColumn(\"a\", sin(col(\"dlat\")/2)**2 + cos(col(\"lat1\"))*cos(col(\"lat2\"))*sin(col(\"dlon\")/2)**2) \\\n",
    "    .withColumn(\"c\", 2*atan2(sqrt(col(\"a\")), sqrt(1-col(\"a\")))) \\\n",
    "    .withColumn(\"distance_mi\", col(\"c\") * 3958.8) \\\n",
    "    .drop(\"lat1\",\"lon1\",\"lat2\",\"lon2\",\"dlat\",\"dlon\",\"a\",\"c\")\n",
    "\n",
    "# 5) Kontrol\n",
    "df_dist.select(\"Start_Lat\",\"Start_Lng\",\"End_Lat\",\"End_Lng\",\"distance_mi\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef06a658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Source: string (nullable = true)\n",
      " |-- Severity: integer (nullable = true)\n",
      " |-- Start_Time: timestamp (nullable = true)\n",
      " |-- End_Time: timestamp (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Street: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Zipcode: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Timezone: string (nullable = true)\n",
      " |-- Airport_Code: string (nullable = true)\n",
      " |-- Weather_Timestamp: timestamp (nullable = true)\n",
      " |-- Temperature(F): double (nullable = true)\n",
      " |-- Wind_Chill(F): double (nullable = true)\n",
      " |-- Humidity(%): double (nullable = true)\n",
      " |-- Pressure(in): double (nullable = true)\n",
      " |-- Visibility(mi): double (nullable = true)\n",
      " |-- Wind_Direction: string (nullable = true)\n",
      " |-- Wind_Speed(mph): double (nullable = true)\n",
      " |-- Precipitation(in): double (nullable = true)\n",
      " |-- Weather_Condition: string (nullable = true)\n",
      " |-- Amenity: boolean (nullable = true)\n",
      " |-- Bump: boolean (nullable = true)\n",
      " |-- Crossing: boolean (nullable = true)\n",
      " |-- Give_Way: boolean (nullable = true)\n",
      " |-- Junction: boolean (nullable = true)\n",
      " |-- No_Exit: boolean (nullable = true)\n",
      " |-- Railway: boolean (nullable = true)\n",
      " |-- Roundabout: boolean (nullable = true)\n",
      " |-- Station: boolean (nullable = true)\n",
      " |-- Stop: boolean (nullable = true)\n",
      " |-- Traffic_Calming: boolean (nullable = true)\n",
      " |-- Traffic_Signal: boolean (nullable = true)\n",
      " |-- Turning_Loop: boolean (nullable = true)\n",
      " |-- Sunrise_Sunset: string (nullable = true)\n",
      " |-- Civil_Twilight: string (nullable = true)\n",
      " |-- Nautical_Twilight: string (nullable = true)\n",
      " |-- Astronomical_Twilight: string (nullable = true)\n",
      " |-- distance_mi: double (nullable = true)\n",
      "\n",
      "+-------------------+\n",
      "|        distance_mi|\n",
      "+-------------------+\n",
      "| 3.2302598454049494|\n",
      "|  0.746718600847293|\n",
      "|0.05514922259749175|\n",
      "|0.21879110549002928|\n",
      "|0.12350028305173481|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ... önceki adımlar: df_dist hazırlandı\n",
    "\n",
    "# 6) Artık eğitimde kullanmayacağımız koordinat sütunlarını at\n",
    "cols_to_drop = [\"Start_Lat\", \"Start_Lng\", \"End_Lat\", \"End_Lng\",\"Distance(mi)\"]\n",
    "df_final = df_dist.drop(*cols_to_drop)\n",
    "# Kontrol\n",
    "df_final.printSchema()\n",
    "df_final.select(\"distance_mi\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "689d1fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE = 1.810\n",
      "Test MAE  = 0.880\n",
      "Test R2   = 0.023\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# 1) %10 örnek al\n",
    "#sample_df = df_final.sample(withReplacement=False, seed=42).cache()\n",
    "\n",
    "# 2) Kullancağımız sayısal feature’lar\n",
    "numeric_feats = [\n",
    "    \"Temperature(F)\",\n",
    "    \"Humidity(%)\",\n",
    "    \"Pressure(in)\",\n",
    "    \"Visibility(mi)\",\n",
    "    \"Wind_Speed(mph)\"\n",
    "]\n",
    "\n",
    "# 3) Train/test split\n",
    "train_df, test_df = df_final.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 4) Pipeline adımları\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=numeric_feats,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"   # eksik değer içeren satırları at\n",
    ")\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"distance_mi\",\n",
    "    numTrees=100,\n",
    "    maxDepth=7\n",
    ")\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "# 5) Modeli eğit (null’ları assembler zaten atıyor)\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# 6) Test set’inde tahmin & değerlendirme\n",
    "pred = model.transform(test_df)\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"distance_mi\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "print(f\"Test RMSE = {evaluator.evaluate(pred):.3f}\")\n",
    "print(f\"Test MAE  = {evaluator.setMetricName('mae').evaluate(pred):.3f}\")\n",
    "print(f\"Test R2   = {evaluator.setMetricName('r2').evaluate(pred):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7981fe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour, dayofweek, month, when\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 1) Zaman özellikleri ekleyelim\n",
    "df2 = df_dist \\\n",
    "    .withColumn(\"hour\", hour(\"Start_Time\")) \\\n",
    "    .withColumn(\"day_of_week\", dayofweek(\"Start_Time\")) \\\n",
    "    .withColumn(\"month\", month(\"Start_Time\")) \\\n",
    "    .withColumn(\"is_weekend\", when(dayofweek(\"Start_Time\").isin([1,7]), 1).otherwise(0))\n",
    "\n",
    "# 2) Kategorikleri sayısallaştır: Weather_Condition, City, State\n",
    "idx_weather = StringIndexer(inputCol=\"Weather_Condition\", outputCol=\"weather_idx\", handleInvalid=\"keep\")\n",
    "enc_weather = OneHotEncoder(inputCol=\"weather_idx\", outputCol=\"weather_vec\")\n",
    "\n",
    "idx_city    = StringIndexer(inputCol=\"City\",              outputCol=\"city_idx\",    handleInvalid=\"keep\")\n",
    "enc_city    = OneHotEncoder(inputCol=\"city_idx\",          outputCol=\"city_vec\")\n",
    "\n",
    "idx_state   = StringIndexer(inputCol=\"State\",             outputCol=\"state_idx\",   handleInvalid=\"keep\")\n",
    "enc_state   = OneHotEncoder(inputCol=\"state_idx\",         outputCol=\"state_vec\")\n",
    "\n",
    "# 3) Assemble edilecek feature’lar\n",
    "numeric_feats = [\"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\",\n",
    "                 \"Visibility(mi)\", \"Wind_Speed(mph)\", \"distance_mi\",\n",
    "                 \"hour\", \"day_of_week\", \"month\", \"is_weekend\"]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=numeric_feats + [\"weather_vec\", \"city_vec\", \"state_vec\"],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# 4) Regresyon modelinizi yeniden kurun\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor(labelCol=\"distance_mi\", featuresCol=\"features\",\n",
    "                           numTrees=50, maxDepth=10, seed=42)\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    idx_weather, enc_weather,\n",
    "    idx_city,    enc_city,\n",
    "    idx_state,   enc_state,\n",
    "    assembler, rf\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8344d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 1.6668868671419903\n",
      "MAE  = 0.8987651630937321\n",
      "R2   = 0.01991324161864938\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sin, cos, atan2, sqrt, radians\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# 1) SparkSession (gerekirse timeout ve worker reuse ayarları da eklenebilir)\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"DistancePrediction_NoUDF\")\n",
    "         .config(\"spark.python.worker.reuse\", \"true\")\n",
    "         .config(\"spark.executor.memory\", \"8g\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# 2) Veri yükle\n",
    "df = spark.read.csv(\"data/traffic_data/US_Accidents_March23.CSV\", header=True, inferSchema=True)\n",
    "\n",
    "# 3) Haversine’i direkt Spark SQL ile\n",
    "R = 3958.8\n",
    "df2 = (df.dropna(subset=[\"Start_Lat\",\"Start_Lng\",\"End_Lat\",\"End_Lng\"])\n",
    "          .withColumn(\"φ1\", radians(col(\"Start_Lat\")))\n",
    "          .withColumn(\"φ2\", radians(col(\"End_Lat\")))\n",
    "          .withColumn(\"Δφ\", radians(col(\"End_Lat\") - col(\"Start_Lat\")))\n",
    "          .withColumn(\"Δλ\", radians(col(\"End_Lng\") - col(\"Start_Lng\")))\n",
    "          .withColumn(\"a\",\n",
    "               sin(col(\"Δφ\")/2) * sin(col(\"Δφ\")/2) +\n",
    "               cos(col(\"φ1\")) * cos(col(\"φ2\")) *\n",
    "               sin(col(\"Δλ\")/2) * sin(col(\"Δλ\")/2)\n",
    "          )\n",
    "          .withColumn(\"distance_mi\", 2 * R * atan2(sqrt(col(\"a\")), sqrt(1-col(\"a\"))))\n",
    "          # Gerekmeyen sütunları at\n",
    "          .drop(\"φ1\",\"φ2\",\"Δφ\",\"Δλ\",\"a\",\"Distance(mi)\",\n",
    "                \"Start_Lat\",\"Start_Lng\",\"End_Lat\",\"End_Lng\")\n",
    ")\n",
    "\n",
    "# 4) Özellikleri seç ve boşları at\n",
    "num_feats = [\"Temperature(F)\",\"Wind_Chill(F)\",\"Humidity(%)\",\n",
    "             \"Pressure(in)\",\"Visibility(mi)\",\"Wind_Speed(mph)\",\n",
    "             \"Precipitation(in)\"]\n",
    "df_final = df2.select(*(num_feats + [\"distance_mi\"])).na.drop()\n",
    "\n",
    "# 5) %10 ile örnek al, train/test split\n",
    "df_sample = df_final.sample(False, 0.1, seed=42)\n",
    "train, test   = df_sample.randomSplit([0.8,0.2], seed=42)\n",
    "\n",
    "# 6) Pipeline tanımı\n",
    "assembler = VectorAssembler(inputCols=num_feats, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "rf        = RandomForestRegressor(labelCol=\"distance_mi\", featuresCol=\"features\",\n",
    "                                  numTrees=10, maxDepth=5)\n",
    "pipeline  = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "# 7) Eğit ve değerlendir\n",
    "model = pipeline.fit(train)\n",
    "pred  = model.transform(test)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"distance_mi\", predictionCol=\"prediction\")\n",
    "print(\"RMSE =\", evaluator.evaluate(pred, {evaluator.metricName:\"rmse\"}))\n",
    "print(\"MAE  =\", evaluator.evaluate(pred, {evaluator.metricName:\"mae\"}))\n",
    "print(\"R2   =\", evaluator.evaluate(pred, {evaluator.metricName:\"r2\"}))\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "189b1c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE = 1.825742950402607\n",
      "Test MAE  = 0.9041405026802174\n",
      "Test R2   = 0.02143761243157094\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, radians, sin, cos, atan2, sqrt\n",
    ")\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, OneHotEncoder, VectorAssembler\n",
    ")\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# 1) SparkSession oluştur\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"DistancePrediction_NoTimeFeatures\")\n",
    "         .config(\"spark.python.worker.reuse\", \"true\")\n",
    "         .config(\"spark.executor.memory\", \"8g\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# 2) CSV’i oku (header ve tipleri kendine göre ayarla)\n",
    "df = (spark.read\n",
    "      .option(\"header\", True)\n",
    "      .option(\"inferSchema\", True)\n",
    "      .csv(\"data/traffic_data/US_Accidents_March23.csv\"))\n",
    "\n",
    "# 3) Koordinat boşlarını at\n",
    "df = df.dropna(subset=[\"Start_Lat\",\"Start_Lng\",\"End_Lat\",\"End_Lng\"])\n",
    "\n",
    "# 4) Haversine ile distance hesapla (mile cinsinden)\n",
    "#    R = 3958.8 mile\n",
    "df = (df\n",
    "      .withColumn(\"lat1\", radians(col(\"Start_Lat\")))\n",
    "      .withColumn(\"lon1\", radians(col(\"Start_Lng\")))\n",
    "      .withColumn(\"lat2\", radians(col(\"End_Lat\")))\n",
    "      .withColumn(\"lon2\", radians(col(\"End_Lng\")))\n",
    "      .withColumn(\"dlat\", col(\"lat2\")-col(\"lat1\"))\n",
    "      .withColumn(\"dlon\", col(\"lon2\")-col(\"lon1\"))\n",
    "      .withColumn(\"a\",\n",
    "          sin(col(\"dlat\")/2)**2 +\n",
    "          cos(col(\"lat1\")) * cos(col(\"lat2\")) * sin(col(\"dlon\")/2)**2\n",
    "      )\n",
    "      .withColumn(\"c\", 2 * atan2(sqrt(col(\"a\")), sqrt(1-col(\"a\"))))\n",
    "      .withColumn(\"distance_mi\", col(\"c\")*3958.8)\n",
    ")\n",
    "\n",
    "# 5) Artık eğitimde kullanmayacağımız sütunları at\n",
    "df_final = (df\n",
    "    .drop(\n",
    "      \"Distance(mi)\",\n",
    "      \"Start_Lat\",\"Start_Lng\",\"End_Lat\",\"End_Lng\",\n",
    "      \"lat1\",\"lat2\",\"lon1\",\"lon2\",\"dlat\",\"dlon\",\"a\",\"c\",\n",
    "      \"Start_Time\",\"End_Time\",\"Weather_Timestamp\"  # zaman sütunları\n",
    "    )\n",
    "    .na.drop(subset=[\"distance_mi\"])  # label boş kalmasın\n",
    ")\n",
    "\n",
    "# 6) %10 örnek al\n",
    "df_sample = df_final.sample(False, 0.1, seed=42)\n",
    "\n",
    "# 7) Pipeline aşamaları\n",
    "#   - Kategorik: Severity, Weather_Condition\n",
    "cats = [\"Severity\",\"Weather_Condition\"]\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid=\"keep\")\n",
    "    for c in cats\n",
    "]\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=c+\"_idx\", outputCol=c+\"_vec\")\n",
    "    for c in cats\n",
    "]\n",
    "\n",
    "#   - Numerik feature’lar\n",
    "num_feats = [\n",
    "    \"Temperature(F)\",\"Humidity(%)\",\"Pressure(in)\",\n",
    "    \"Wind_Speed(mph)\",\"Visibility(mi)\"\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=num_feats + [c+\"_vec\" for c in cats],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    labelCol=\"distance_mi\",\n",
    "    featuresCol=\"features\",\n",
    "    numTrees=50,\n",
    "    maxDepth=8\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, rf])\n",
    "\n",
    "# 8) Train/Test split + eğitim\n",
    "train, test = df_sample.randomSplit([0.8,0.2], seed=42)\n",
    "model = pipeline.fit(train.na.drop())\n",
    "\n",
    "# 9) Tahmin & değerlendirme\n",
    "pred = model.transform(test.na.drop())\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"distance_mi\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "print(\"Test RMSE =\", evaluator.evaluate(pred, {evaluator.metricName:\"rmse\"}))\n",
    "print(\"Test MAE  =\", evaluator.evaluate(pred, {evaluator.metricName:\"mae\"}))\n",
    "print(\"Test R2   =\", evaluator.evaluate(pred, {evaluator.metricName:\"r2\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7de824a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aslay\\bert_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\aslay\\AppData\\Local\\Temp\\ipykernel_30772\\1778831836.py\", line 109, in <module>\n",
      "    model = pipe.fit(train)\n",
      "            ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aslay\\bert_env\\Lib\\site-packages\\pyspark\\ml\\base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aslay\\bert_env\\Lib\\site-packages\\pyspark\\ml\\pipeline.py\", line 134, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aslay\\bert_env\\Lib\\site-packages\\pyspark\\ml\\base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aslay\\bert_env\\Lib\\site-packages\\pyspark\\ml\\wrapper.py\", line 381, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aslay\\bert_env\\Lib\\site-packages\\pyspark\\ml\\wrapper.py\", line 378, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aslay\\bert_env\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aslay\\bert_env\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aslay\\bert_env\\Lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aslay\\bert_env\\Lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aslay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] Varolan bir bağlantı uzaktaki bir ana bilgisayar tarafından zorla kapatıldı\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aslay\\bert_env\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aslay\\bert_env\\Lib\\site-packages\\py4j\\clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aslay\\bert_env\\Lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aslay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] Varolan bir bağlantı uzaktaki bir ana bilgisayar tarafından zorla kapatıldı\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aslay\\bert_env\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aslay\\bert_env\\Lib\\site-packages\\py4j\\clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] Hedef makine etkin olarak reddettiğinden bağlantı kurulamadı",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[1], line 109\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# 9c) Modeli küçük örnekle eğit\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m pred   \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(test)\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\pyspark\\ml\\pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(10061, 'Hedef makine etkin olarak reddettiğinden bağlantı kurulamadı', None, 10061, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2179\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2176\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[0;32m   2177\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2179\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[0;32m   2181\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[0;32m   2182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\ipykernel\\zmqshell.py:559\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[1;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[0;32m    553\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    554\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    556\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    560\u001b[0m }\n\u001b[0;32m    562\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[0;32m    563\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\py4j\\protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[1;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\py4j\\java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\py4j\\clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\py4j\\clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mc:\\Users\\aslay\\bert_env\\Lib\\site-packages\\py4j\\clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[1;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] Hedef makine etkin olarak reddettiğinden bağlantı kurulamadı"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, radians, sin, cos, atan2, sqrt\n",
    ")\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, OneHotEncoder, VectorAssembler\n",
    ")\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# 1) SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"DistancePrediction_FeatsOnly\")\n",
    "         .config(\"spark.python.worker.reuse\", \"true\")\n",
    "         .config(\"spark.executor.memory\", \"16g\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# 2) Veriyi oku (path’i kendi dosyana göre değiştir)\n",
    "df = (spark.read\n",
    "      .option(\"header\", True)\n",
    "      .option(\"inferSchema\", True)\n",
    "      .csv(\"data/traffic_data/US_Accidents_March23.csv\"))\n",
    "\n",
    "# 3) Boş koordinatları at ve Haversine formülü ile distance_mi hesapla\n",
    "#    Earth radius in miles ~3958.8\n",
    "df_dist = (df.dropna(subset=[\"Start_Lat\",\"Start_Lng\",\"End_Lat\",\"End_Lng\"])\n",
    "    .withColumn(\"lat1\", radians(col(\"Start_Lat\")))\n",
    "    .withColumn(\"lon1\", radians(col(\"Start_Lng\")))\n",
    "    .withColumn(\"lat2\", radians(col(\"End_Lat\")))\n",
    "    .withColumn(\"lon2\", radians(col(\"End_Lng\")))\n",
    "    .withColumn(\"dlat\", col(\"lat2\") - col(\"lat1\"))\n",
    "    .withColumn(\"dlon\", col(\"lon2\") - col(\"lon1\"))\n",
    "    .withColumn(\"a\",\n",
    "        sin(col(\"dlat\")/2)**2 +\n",
    "        cos(col(\"lat1\")) * cos(col(\"lat2\")) * sin(col(\"dlon\")/2)**2\n",
    "    )\n",
    "    .withColumn(\"c\", 2 * atan2(sqrt(col(\"a\")), sqrt(1-col(\"a\"))))\n",
    "    .withColumn(\"distance_mi\", col(\"c\") * 3958.8)\n",
    "    .drop(\"lat1\",\"lat2\",\"lon1\",\"lon2\",\"dlat\",\"dlon\",\"a\",\"c\")\n",
    ")\n",
    "\n",
    "# 4) Başlangıç/bitiş kümeleme\n",
    "assembler_k1 = VectorAssembler(\n",
    "    inputCols=[\"Start_Lat\",\"Start_Lng\"], outputCol=\"start_coord\")\n",
    "k1 = KMeans(k=50, featuresCol=\"start_coord\",\n",
    "            predictionCol=\"start_cluster\", seed=42)\n",
    "assembler_k2 = VectorAssembler(\n",
    "    inputCols=[\"End_Lat\",\"End_Lng\"], outputCol=\"end_coord\")\n",
    "k2 = KMeans(k=50, featuresCol=\"end_coord\",\n",
    "            predictionCol=\"end_cluster\", seed=42)\n",
    "\n",
    "km_pipe = Pipeline(stages=[assembler_k1, k1, assembler_k2, k2])\n",
    "df_feat = km_pipe.fit(df_dist).transform(df_dist)\n",
    "\n",
    "# 5) Koordinat sütunlarını at\n",
    "df_feat = df_feat.drop(\n",
    "    \"Start_Lat\",\"Start_Lng\",\"End_Lat\",\"End_Lng\",\n",
    "    \"start_coord\",\"end_coord\"\n",
    ")\n",
    "\n",
    "# 6) Kategorik → index + one‑hot\n",
    "cats = [\"Severity\",\"Weather_Condition\",\"start_cluster\",\"end_cluster\"]\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid=\"keep\")\n",
    "    for c in cats\n",
    "]\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=c+\"_idx\", outputCol=c+\"_vec\")\n",
    "    for c in cats\n",
    "]\n",
    "\n",
    "# 7) Sayısal feature’lar + ohe kolonları\n",
    "num_feats = [\n",
    "    \"Temperature(F)\",\"Humidity(%)\",\"Pressure(in)\",\n",
    "    \"Wind_Speed(mph)\",\"Visibility(mi)\",\n",
    "    \"distance_mi\"  # hedefi de features’a koyuyoruz, ama RF label olarak ayrıştıracak\n",
    "]\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=num_feats + [c+\"_vec\" for c in cats],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "# 8) Model\n",
    "rf = RandomForestRegressor(\n",
    "    labelCol=\"distance_mi\",\n",
    "    featuresCol=\"features\",\n",
    "    numTrees=50,\n",
    "    maxDepth=8,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "pipe = Pipeline(stages=indexers + encoders + [assembler, rf])\n",
    "# 9a) %10 örnek al\n",
    "df_small = df_feat \\\n",
    "    .na.drop(subset=[\"distance_mi\"]) \\\n",
    "    .sample(withReplacement=False, fraction=0.1, seed=42) \\\n",
    "    .cache()\n",
    "\n",
    "# 9b) Örnekten split\n",
    "train, test = df_small.randomSplit([0.8,0.2], seed=42)\n",
    "\n",
    "# 9c) Modeli küçük örnekle eğit\n",
    "model = pipe.fit(train)\n",
    "pred   = model.transform(test)\n",
    "\n",
    "\n",
    "# 10) Değerlendir\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"distance_mi\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "print(\"RMSE =\", evaluator.evaluate(pred, {evaluator.metricName:\"rmse\"}))\n",
    "print(\"MAE  =\", evaluator.evaluate(pred, {evaluator.metricName:\"mae\"}))\n",
    "print(\"R2   =\", evaluator.evaluate(pred, {evaluator.metricName:\"r2\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1564a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample üzerinde Accuracy = 0.2522689101526988\n",
      "Sample üzerinde F1 Score = 0.19301675454757888\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, radians, sin, cos, atan2, sqrt\n",
    "from pyspark.ml.feature import VectorAssembler, Bucketizer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# --- 0) SparkSession başlat ---\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"DistanceClassification_SampleTest\")\n",
    "         .config(\"spark.executor.memory\", \"8g\")\n",
    "         .config(\"spark.driver.memory\",   \"4g\")\n",
    "         .config(\"spark.python.worker.reuse\", \"true\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# --- 1) Veriyi yükle ---\n",
    "df = (spark.read\n",
    "      .option(\"header\", True)\n",
    "      .option(\"inferSchema\", True)\n",
    "      .csv(\"data/traffic_data/US_Accidents_March23.csv\"))\n",
    "\n",
    "# --- 2) Haversine ile distance_mi hesapla ---\n",
    "R = 3958.8\n",
    "df2 = (df.dropna(subset=[\"Start_Lat\",\"Start_Lng\",\"End_Lat\",\"End_Lng\"])\n",
    "         .withColumn(\"φ1\", radians(col(\"Start_Lat\")))\n",
    "         .withColumn(\"φ2\", radians(col(\"End_Lat\")))\n",
    "         .withColumn(\"Δφ\", radians(col(\"End_Lat\") - col(\"Start_Lat\")))\n",
    "         .withColumn(\"Δλ\", radians(col(\"End_Lng\") - col(\"Start_Lng\")))\n",
    "         .withColumn(\"a\",\n",
    "             sin(col(\"Δφ\")/2) * sin(col(\"Δφ\")/2) +\n",
    "             cos(col(\"φ1\")) * cos(col(\"φ2\")) *\n",
    "             sin(col(\"Δλ\")/2) * sin(col(\"Δλ\")/2)\n",
    "         )\n",
    "         .withColumn(\"distance_mi\", 2 * R * atan2(sqrt(col(\"a\")), sqrt(1-col(\"a\"))))\n",
    "         .drop(\"φ1\",\"φ2\",\"Δφ\",\"Δλ\",\"a\",\n",
    "               \"Start_Lat\",\"Start_Lng\",\"End_Lat\",\"End_Lng\",\"Distance(mi)\"))\n",
    "\n",
    "# --- 3) IQR ile outlier temizleme ---\n",
    "q1, q3 = df2.approxQuantile(\"distance_mi\", [0.25, 0.75], 1e-4)\n",
    "iqr    = q3 - q1\n",
    "lower, upper = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "df_clean = df2.filter((col(\"distance_mi\") >= lower) & (col(\"distance_mi\") <= upper))\n",
    "\n",
    "# --- 4) Tüm verinin %10’u ile örnek al ---\n",
    "df_sample = df_clean.sample(withReplacement=False, fraction=0.1, seed=42)\n",
    "\n",
    "# --- 5) Sample üzerinde train/test split ---\n",
    "train_sample, test_sample = df_sample.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# --- 6) Quintile bazlı bucket eşikleri (sadece train_sample’da) ---\n",
    "splits = train_sample.approxQuantile(\"distance_mi\",\n",
    "                                     [0.0,0.2,0.4,0.6,0.8,1.0],\n",
    "                                     1e-4)\n",
    "splits[0], splits[-1] = float(\"-inf\"), float(\"inf\")\n",
    "bucketizer = Bucketizer(inputCol=\"distance_mi\",\n",
    "                        outputCol=\"distance_class\",\n",
    "                        splits=splits)\n",
    "\n",
    "# --- 7) Özellikleri birleştir ---\n",
    "num_feats = [\n",
    "    \"Temperature(F)\",\"Wind_Chill(F)\",\"Humidity(%)\",\n",
    "    \"Pressure(in)\",\"Visibility(mi)\",\"Wind_Speed(mph)\",\n",
    "    \"Precipitation(in)\"\n",
    "]\n",
    "assembler = VectorAssembler(inputCols=num_feats,\n",
    "                            outputCol=\"features\",\n",
    "                            handleInvalid=\"skip\")\n",
    "\n",
    "# --- 8) Hafif Random Forest Classifier ---\n",
    "rf = (RandomForestClassifier(labelCol=\"distance_class\",\n",
    "                             featuresCol=\"features\")\n",
    "      .setNumTrees(20)\n",
    "      .setMaxDepth(5)\n",
    "      .setSubsamplingRate(0.7)\n",
    "      .setSeed(42))\n",
    "\n",
    "# --- 9) Pipeline oluştur ---\n",
    "pipe = Pipeline(stages=[bucketizer, assembler, rf])\n",
    "\n",
    "# --- 10) Model eğit & tahmin ---\n",
    "model = pipe.fit(train_sample)\n",
    "pred  = model.transform(test_sample)\n",
    "\n",
    "# --- 11) Değerlendirme ---\n",
    "evaluator_acc = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"distance_class\", predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\")\n",
    "evaluator_f1  = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"distance_class\", predictionCol=\"prediction\",\n",
    "    metricName=\"f1\")\n",
    "\n",
    "print(\"Sample üzerinde Accuracy =\", evaluator_acc.evaluate(pred))\n",
    "print(\"Sample üzerinde F1 Score =\", evaluator_f1.evaluate(pred))\n",
    "\n",
    "# --- 12) SparkSession’ı kapat ---\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3553b48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Toplam temize düşen satır sayısı: 3892385\n",
      "+-------+-------------------+\n",
      "|summary|        distance_mi|\n",
      "+-------+-------------------+\n",
      "|  count|            3892385|\n",
      "|   mean| 0.4257840179197078|\n",
      "| stddev|0.49089888749527927|\n",
      "|    min|                0.0|\n",
      "|    max| 2.1313033514150135|\n",
      "+-------+-------------------+\n",
      "\n",
      "+-------+------------------+------------------+------------------+-----------------+-----------------+-----------------+--------------------+\n",
      "|summary|    Temperature(F)|     Wind_Chill(F)|       Humidity(%)|     Pressure(in)|   Visibility(mi)|  Wind_Speed(mph)|   Precipitation(in)|\n",
      "+-------+------------------+------------------+------------------+-----------------+-----------------+-----------------+--------------------+\n",
      "|  count|           3799394|           3392355|           3793813|          3812577|          3794595|          3704724|             3313976|\n",
      "|   mean|61.366199425487395|59.328386474882244| 63.57699206576603|29.44533834726443|9.119675472613036|7.555216825868907|0.006447466728786...|\n",
      "| stddev|19.128881968710566| 21.74442357801775|22.962789608364655|1.070173303934003| 2.63460756941167|5.560057447083292|  0.0797273851633318|\n",
      "|    min|             -89.0|             -89.0|               1.0|              0.0|              0.0|              0.0|                 0.0|\n",
      "|    max|             207.0|             207.0|             100.0|            58.63|            140.0|           1087.0|                24.0|\n",
      "+-------+------------------+------------------+------------------+-----------------+-----------------+-----------------+--------------------+\n",
      "\n",
      "🔹 distance_mi quantile değerleri: [0.0, 0.03740954217444517, 0.13621531977348844, 0.34871502099701324, 0.7921222251169631, 2.1313033514150135]\n",
      "+-------------------+------+\n",
      "|distance_class_temp|count |\n",
      "+-------------------+------+\n",
      "|0.0                |778410|\n",
      "|1.0                |778483|\n",
      "|2.0                |778424|\n",
      "|3.0                |778446|\n",
      "|4.0                |778622|\n",
      "+-------------------+------+\n",
      "\n",
      "🔹 Sayısal sütunlardaki eksikler:\n",
      "+--------------+-------------+-----------+------------+--------------+---------------+-----------------+-----------+\n",
      "|Temperature(F)|Wind_Chill(F)|Humidity(%)|Pressure(in)|Visibility(mi)|Wind_Speed(mph)|Precipitation(in)|distance_mi|\n",
      "+--------------+-------------+-----------+------------+--------------+---------------+-----------------+-----------+\n",
      "|92991         |500030       |98572      |79808       |97790         |187661         |578409           |0          |\n",
      "+--------------+-------------+-----------+------------+--------------+---------------+-----------------+-----------+\n",
      "\n",
      "🔹 Diğer sütunlardaki eksikler:\n",
      "+---+------+--------+----------+--------+-----------+------+----+------+-----+-------+-------+--------+------------+-----------------+--------------+-----------------+-------+----+--------+--------+--------+-------+-------+----------+-------+----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+\n",
      "|ID |Source|Severity|Start_Time|End_Time|Description|Street|City|County|State|Zipcode|Country|Timezone|Airport_Code|Weather_Timestamp|Wind_Direction|Weather_Condition|Amenity|Bump|Crossing|Give_Way|Junction|No_Exit|Railway|Roundabout|Station|Stop|Traffic_Calming|Traffic_Signal|Turning_Loop|Sunrise_Sunset|Civil_Twilight|Nautical_Twilight|Astronomical_Twilight|\n",
      "+---+------+--------+----------+--------+-----------+------+----+------+-----+-------+-------+--------+------------+-----------------+--------------+-----------------+-------+----+--------+--------+--------+-------+-------+----------+-------+----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+\n",
      "|0  |0     |0       |0         |0       |0          |8689  |143 |0     |0    |1075   |0      |4323    |13609       |69465            |105620        |95164            |0      |0   |0       |0       |0       |0      |0      |0         |0      |0   |0              |0             |0           |17980         |17980         |17980            |17980                |\n",
      "+---+------+--------+----------+--------+-----------+------+----+------+-----+-------+-------+--------+------------+-----------------+--------------+-----------------+-------+----+--------+--------+--------+-------+-------+----------+-------+----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, count, isnan,  # isnan eklendi\n",
    "    radians, sin, cos, atan2, sqrt\n",
    ")\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.sql.types import DoubleType, FloatType\n",
    "\n",
    "# --- A) SparkSession başlat ---\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"DistanceClassification_EDA_FixMissing\")\n",
    "         .config(\"spark.executor.memory\", \"8g\")\n",
    "         .config(\"spark.driver.memory\",   \"4g\")\n",
    "         .config(\"spark.python.worker.reuse\", \"true\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# --- B) Veri hazırlığı ---\n",
    "df = (spark.read\n",
    "      .option(\"header\", True)\n",
    "      .option(\"inferSchema\", True)\n",
    "      .csv(\"data/traffic_data/US_Accidents_March23.csv\"))\n",
    "\n",
    "R = 3958.8\n",
    "df2 = (df.dropna(subset=[\"Start_Lat\",\"Start_Lng\",\"End_Lat\",\"End_Lng\"])\n",
    "         .withColumn(\"φ1\", radians(col(\"Start_Lat\")))\n",
    "         .withColumn(\"φ2\", radians(col(\"End_Lat\")))\n",
    "         .withColumn(\"Δφ\", radians(col(\"End_Lat\") - col(\"Start_Lat\")))\n",
    "         .withColumn(\"Δλ\", radians(col(\"End_Lng\") - col(\"Start_Lng\")))\n",
    "         .withColumn(\"a\",\n",
    "             sin(col(\"Δφ\")/2) * sin(col(\"Δφ\")/2) +\n",
    "             cos(col(\"φ1\")) * cos(col(\"φ2\")) *\n",
    "             sin(col(\"Δλ\")/2) * sin(col(\"Δλ\")/2)\n",
    "         )\n",
    "         .withColumn(\"distance_mi\", 2 * R * atan2(sqrt(col(\"a\")), sqrt(1-col(\"a\"))))\n",
    "         .drop(\"φ1\",\"φ2\",\"Δφ\",\"Δλ\",\"a\",\n",
    "               \"Start_Lat\",\"Start_Lng\",\"End_Lat\",\"End_Lng\",\"Distance(mi)\"))\n",
    "\n",
    "# IQR ile outlier’ları çıkar\n",
    "q1, q3 = df2.approxQuantile(\"distance_mi\", [0.25, 0.75], 1e-4)\n",
    "iqr    = q3 - q1\n",
    "lower, upper = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "df_clean = df2.filter((col(\"distance_mi\") >= lower) & (col(\"distance_mi\") <= upper))\n",
    "\n",
    "# --- C) Genel istatistikler ---\n",
    "print(\"🔹 Toplam temize düşen satır sayısı:\", df_clean.count())\n",
    "df_clean.select(\"distance_mi\").describe().show()\n",
    "\n",
    "num_feats = [\n",
    "    \"Temperature(F)\",\"Wind_Chill(F)\",\"Humidity(%)\",\n",
    "    \"Pressure(in)\",\"Visibility(mi)\",\"Wind_Speed(mph)\",\n",
    "    \"Precipitation(in)\"\n",
    "]\n",
    "df_clean.select(num_feats).describe().show()\n",
    "\n",
    "# --- D) distance_mi quantile ve binleme ---\n",
    "quantiles = df_clean.approxQuantile(\"distance_mi\",\n",
    "                                    [0.0,0.2,0.4,0.6,0.8,1.0],\n",
    "                                    1e-4)\n",
    "print(\"🔹 distance_mi quantile değerleri:\", quantiles)\n",
    "\n",
    "splits = quantiles[:]\n",
    "splits[0], splits[-1] = float(\"-inf\"), float(\"inf\")\n",
    "bucketizer = Bucketizer(inputCol=\"distance_mi\",\n",
    "                        outputCol=\"distance_class_temp\",\n",
    "                        splits=splits)\n",
    "df_binned = bucketizer.transform(df_clean)\n",
    "df_binned.groupBy(\"distance_class_temp\") \\\n",
    "         .count() \\\n",
    "         .orderBy(\"distance_class_temp\") \\\n",
    "         .show(truncate=False)\n",
    "\n",
    "# --- E) Eksik değer analizi (tip bazlı) ---\n",
    "from pyspark.sql.types import DoubleType, FloatType\n",
    "\n",
    "# Sayısal sütunlar\n",
    "num_cols = [f.name for f in df_clean.schema.fields \n",
    "            if isinstance(f.dataType, (DoubleType, FloatType))]\n",
    "# Diğerleri\n",
    "other_cols = [c for c in df_clean.columns if c not in num_cols]\n",
    "\n",
    "print(\"🔹 Sayısal sütunlardaki eksikler:\")\n",
    "df_clean.select([\n",
    "    count(when(col(c).isNull() | isnan(col(c)), c)).alias(c)\n",
    "    for c in num_cols\n",
    "]).show(truncate=False)\n",
    "\n",
    "print(\"🔹 Diğer sütunlardaki eksikler:\")\n",
    "df_clean.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c)\n",
    "    for c in other_cols\n",
    "]).show(truncate=False)\n",
    "\n",
    "# --- F) Spark’ı kapat ---\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb71ddb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impute+Scale Accuracy = 0.2701567450499026\n",
      "Impute+Scale F1 = 0.24786260192762977\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, radians, sin, cos, atan2, sqrt\n",
    "from pyspark.sql.types import DoubleType, FloatType\n",
    "from pyspark.ml.feature import (\n",
    "    Bucketizer, Imputer, VectorAssembler, StandardScaler\n",
    ")\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# --- 0) SparkSession ---\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"DistanceClassification_WithImputeScale\")\n",
    "         .config(\"spark.executor.memory\", \"12g\")\n",
    "         .config(\"spark.driver.memory\",   \"8g\")\n",
    "         .config(\"spark.python.worker.reuse\", \"true\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# --- 1) Veri yükle & temizle (aynı adımlar) ---\n",
    "df = (spark.read\n",
    "      .option(\"header\", True)\n",
    "      .option(\"inferSchema\", True)\n",
    "      .csv(\"data/traffic_data/US_Accidents_March23.csv\"))\n",
    "\n",
    "R = 3958.8\n",
    "df2 = (df.dropna(subset=[\"Start_Lat\",\"Start_Lng\",\"End_Lat\",\"End_Lng\"])\n",
    "         .withColumn(\"φ1\", radians(col(\"Start_Lat\")))\n",
    "         .withColumn(\"φ2\", radians(col(\"End_Lat\")))\n",
    "         .withColumn(\"Δφ\", radians(col(\"End_Lat\") - col(\"Start_Lat\")))\n",
    "         .withColumn(\"Δλ\", radians(col(\"End_Lng\") - col(\"Start_Lng\")))\n",
    "         .withColumn(\"a\",\n",
    "             sin(col(\"Δφ\")/2) * sin(col(\"Δφ\")/2) +\n",
    "             cos(col(\"φ1\")) * cos(col(\"φ2\")) *\n",
    "             sin(col(\"Δλ\")/2) * sin(col(\"Δλ\")/2)\n",
    "         )\n",
    "         .withColumn(\"distance_mi\", 2 * R * atan2(sqrt(col(\"a\")), sqrt(1-col(\"a\"))))\n",
    "         .drop(\"φ1\",\"φ2\",\"Δφ\",\"Δλ\",\"a\",\n",
    "               \"Start_Lat\",\"Start_Lng\",\"End_Lat\",\"End_Lng\",\"Distance(mi)\"))\n",
    "q1, q3 = df2.approxQuantile(\"distance_mi\", [0.25,0.75], 1e-4)\n",
    "iqr    = q3 - q1\n",
    "lower, upper = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "df_clean = df2.filter((col(\"distance_mi\") >= lower) & (col(\"distance_mi\") <= upper))\n",
    "\n",
    "# cache’lenmiş eğitim verisi\n",
    "train_full, test_full = df_clean.randomSplit([0.8,0.2], seed=42)\n",
    "train_cached = train_full.repartition(200).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "train_cached.count()\n",
    "\n",
    "# --- 2) Bucketizer (quantile’lar aynen) ---\n",
    "splits = train_cached.approxQuantile(\"distance_mi\",\n",
    "                                     [0.0,0.2,0.4,0.6,0.8,1.0],\n",
    "                                     1e-4)\n",
    "splits[0], splits[-1] = float(\"-inf\"), float(\"inf\")\n",
    "bucketizer = Bucketizer(inputCol=\"distance_mi\",\n",
    "                        outputCol=\"distance_class\",\n",
    "                        splits=splits)\n",
    "\n",
    "# --- 3) Eksik değer doldurucu ---\n",
    "num_feats = [\n",
    "    \"Temperature(F)\",\"Wind_Chill(F)\",\"Humidity(%)\",\n",
    "    \"Pressure(in)\",\"Visibility(mi)\",\"Wind_Speed(mph)\",\n",
    "    \"Precipitation(in)\"\n",
    "]\n",
    "imputer = (Imputer()\n",
    "           .setInputCols(num_feats)\n",
    "           .setOutputCols([f\"imp_{c}\" for c in num_feats])\n",
    "           .setStrategy(\"median\"))\n",
    "\n",
    "# --- 4) Assembler & Scaler ---\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[f\"imp_{c}\" for c in num_feats],\n",
    "    outputCol=\"unscaled_features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"unscaled_features\",\n",
    "    outputCol=\"features\",\n",
    "    withMean=True, withStd=True\n",
    ")\n",
    "\n",
    "# --- 5) Random Forest ---\n",
    "rf = (RandomForestClassifier(labelCol=\"distance_class\",\n",
    "                             featuresCol=\"features\")\n",
    "      .setNumTrees(25)\n",
    "      .setMaxDepth(8)\n",
    "      .setSubsamplingRate(0.8)\n",
    "      .setSeed(42))\n",
    "\n",
    "# --- 6) Pipeline & Eğit ---\n",
    "pipe = Pipeline(stages=[\n",
    "    bucketizer,\n",
    "    imputer,\n",
    "    assembler,\n",
    "    scaler,\n",
    "    rf\n",
    "])\n",
    "model = pipe.fit(train_cached)\n",
    "pred  = model.transform(test_full)\n",
    "\n",
    "# --- 7) Değerlendirme ---\n",
    "evaluator_acc = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"distance_class\", predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\")\n",
    "evaluator_f1  = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"distance_class\", predictionCol=\"prediction\",\n",
    "    metricName=\"f1\")\n",
    "\n",
    "print(\"Impute+Scale Accuracy =\", evaluator_acc.evaluate(pred))\n",
    "print(\"Impute+Scale F1 =\", evaluator_f1.evaluate(pred))\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1408dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-class + Time Accuracy = 0.40093571460704486\n",
      "3-class + Time F1 = 0.3886517222673759\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, radians, sin, cos, atan2, sqrt,\n",
    "    hour, dayofweek\n",
    ")\n",
    "from pyspark.ml.feature import (\n",
    "    Bucketizer, Imputer, VectorAssembler, StandardScaler\n",
    ")\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# --- 0) SparkSession ---\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"DistanceClassification_3Class_TimeFeatures\")\n",
    "         .config(\"spark.executor.memory\",\"12g\")\n",
    "         .config(\"spark.driver.memory\",\"8g\")\n",
    "         .config(\"spark.python.worker.reuse\",\"true\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# --- 1) Veri yükle ve distance + time özelliklerini oluştur ---\n",
    "df = (spark.read\n",
    "      .option(\"header\", True)\n",
    "      .option(\"inferSchema\", True)\n",
    "      .csv(\"data/traffic_data/US_Accidents_March23.csv\"))\n",
    "\n",
    "R = 3958.8\n",
    "df2 = (df\n",
    "       .dropna(subset=[\"Start_Lat\",\"Start_Lng\",\"End_Lat\",\"End_Lng\"])\n",
    "       .withColumn(\"φ1\", radians(col(\"Start_Lat\")))\n",
    "       .withColumn(\"φ2\", radians(col(\"End_Lat\")))\n",
    "       .withColumn(\"Δφ\", radians(col(\"End_Lat\") - col(\"Start_Lat\")))\n",
    "       .withColumn(\"Δλ\", radians(col(\"End_Lng\") - col(\"Start_Lng\")))\n",
    "       .withColumn(\"a\",\n",
    "           sin(col(\"Δφ\")/2) * sin(col(\"Δφ\")/2) +\n",
    "           cos(col(\"φ1\")) * cos(col(\"φ2\")) *\n",
    "           sin(col(\"Δλ\")/2) * sin(col(\"Δλ\")/2)\n",
    "       )\n",
    "       .withColumn(\"distance_mi\", 2 * R * atan2(sqrt(col(\"a\")), sqrt(1-col(\"a\"))))\n",
    "       # Zaman özellikleri\n",
    "       .withColumn(\"hour\",    hour(col(\"Start_Time\")))\n",
    "       .withColumn(\"weekday\", dayofweek(col(\"Start_Time\")))\n",
    "       .drop(\"φ1\",\"φ2\",\"Δφ\",\"Δλ\",\"a\",\n",
    "             \"Start_Lat\",\"Start_Lng\",\"End_Lat\",\"End_Lng\",\"Distance(mi)\"))\n",
    "\n",
    "# --- 2) Outlier temizleme (IQR) ---\n",
    "q1, q3 = df2.approxQuantile(\"distance_mi\", [0.25, 0.75], 1e-4)\n",
    "iqr    = q3 - q1\n",
    "lower, upper = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "df_clean = df2.filter((col(\"distance_mi\") >= lower) & (col(\"distance_mi\") <= upper))\n",
    "\n",
    "# --- 3) Train/Test split & cache ---\n",
    "train_full, test_full = df_clean.randomSplit([0.8, 0.2], seed=42)\n",
    "train_cached = train_full.repartition(200).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "train_cached.count()\n",
    "\n",
    "# --- 4) 3-class bucket eşikleri (%33, %66 quantile) ---\n",
    "qs = train_cached.approxQuantile(\"distance_mi\", [0.33, 0.66], 1e-4)\n",
    "splits = [float(\"-inf\"), qs[0], qs[1], float(\"inf\")]\n",
    "bucketizer = Bucketizer(inputCol=\"distance_mi\",\n",
    "                        outputCol=\"distance_class\",\n",
    "                        splits=splits)\n",
    "\n",
    "# --- 5) Eksik değer doldurucu (median ile) ---\n",
    "num_feats = [\n",
    "    \"Temperature(F)\",\"Wind_Chill(F)\",\"Humidity(%)\",\n",
    "    \"Pressure(in)\",\"Visibility(mi)\",\"Wind_Speed(mph)\",\n",
    "    \"Precipitation(in)\"\n",
    "]\n",
    "imputer = (Imputer()\n",
    "           .setInputCols(num_feats)\n",
    "           .setOutputCols([f\"imp_{c}\" for c in num_feats])\n",
    "           .setStrategy(\"median\"))\n",
    "\n",
    "# --- 6) Assembler & Scaler (time özellikleri de ekle) ---\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[f\"imp_{c}\" for c in num_feats] + [\"hour\", \"weekday\"],\n",
    "    outputCol=\"unscaled_features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"unscaled_features\",\n",
    "    outputCol=\"features\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "# --- 7) Random Forest Model ---\n",
    "rf = (RandomForestClassifier(labelCol=\"distance_class\",\n",
    "                             featuresCol=\"features\")\n",
    "      .setNumTrees(50)\n",
    "      .setMaxDepth(8)\n",
    "      .setSeed(42))\n",
    "\n",
    "# --- 8) Pipeline & Eğitim ---\n",
    "pipe = Pipeline(stages=[\n",
    "    bucketizer,\n",
    "    imputer,\n",
    "    assembler,\n",
    "    scaler,\n",
    "    rf\n",
    "])\n",
    "model = pipe.fit(train_cached)\n",
    "pred  = model.transform(test_full)\n",
    "\n",
    "# --- 9) Değerlendirme ---\n",
    "evaluator_acc = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"distance_class\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\")\n",
    "evaluator_f1  = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"distance_class\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\")\n",
    "\n",
    "print(\"3-class + Time Accuracy =\", evaluator_acc.evaluate(pred))\n",
    "print(\"3-class + Time F1 =\", evaluator_f1.evaluate(pred))\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c12d57aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DropNa + Weather – Accuracy = 0.39598413709239194\n",
      "DropNa + Weather – F1 = 0.3932116074730015\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, radians, sin, cos, atan2, sqrt,\n",
    "    hour, dayofweek\n",
    ")\n",
    "from pyspark.ml.feature import (\n",
    "    Bucketizer,\n",
    "    StringIndexer, OneHotEncoder,\n",
    "    VectorAssembler, StandardScaler\n",
    ")\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# --- 0) SparkSession ---\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"DistanceClassification_DropNa\")\n",
    "         .config(\"spark.executor.memory\",\"12g\")\n",
    "         .config(\"spark.driver.memory\",\"8g\")\n",
    "         .config(\"spark.python.worker.reuse\",\"true\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# --- 1) Veri yükle & distance + time + weather features ---\n",
    "df = (spark.read\n",
    "      .option(\"header\", True)\n",
    "      .option(\"inferSchema\", True)\n",
    "      .csv(\"data/traffic_data/US_Accidents_March23.csv\"))\n",
    "\n",
    "R = 3958.8\n",
    "df2 = (df\n",
    "       .dropna(subset=[\"Start_Lat\",\"Start_Lng\",\"End_Lat\",\"End_Lng\"])\n",
    "       .withColumn(\"φ1\", radians(col(\"Start_Lat\")))\n",
    "       .withColumn(\"φ2\", radians(col(\"End_Lat\")))\n",
    "       .withColumn(\"Δφ\", radians(col(\"End_Lat\") - col(\"Start_Lat\")))\n",
    "       .withColumn(\"Δλ\", radians(col(\"End_Lng\") - col(\"Start_Lng\")))\n",
    "       .withColumn(\"a\",\n",
    "           sin(col(\"Δφ\")/2)*sin(col(\"Δφ\")/2) +\n",
    "           cos(col(\"φ1\"))*cos(col(\"φ2\")) *\n",
    "           sin(col(\"Δλ\")/2)*sin(col(\"Δλ\")/2)\n",
    "       )\n",
    "       .withColumn(\"distance_mi\", 2*R*atan2(sqrt(col(\"a\")), sqrt(1-col(\"a\"))))\n",
    "       .withColumn(\"hour\",    hour(col(\"Start_Time\")))\n",
    "       .withColumn(\"weekday\", dayofweek(col(\"Start_Time\")))\n",
    "       .drop(\"φ1\",\"φ2\",\"Δφ\",\"Δλ\",\"a\",\n",
    "             \"Start_Lat\",\"Start_Lng\",\"End_Lat\",\"End_Lng\",\"Distance(mi)\"))\n",
    "\n",
    "# --- 2) Outlier temizleme (IQR) ---\n",
    "q1, q3 = df2.approxQuantile(\"distance_mi\",[0.25,0.75],1e-4)\n",
    "iqr    = q3 - q1\n",
    "lower, upper = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "df_clean = df2.filter((col(\"distance_mi\")>=lower)&(col(\"distance_mi\")<=upper))\n",
    "\n",
    "# --- 3) NaN’leri at (sadece sayısal feature’larda) ---\n",
    "num_feats = [\n",
    "    \"Temperature(F)\",\"Wind_Chill(F)\",\"Humidity(%)\",\n",
    "    \"Pressure(in)\",\"Visibility(mi)\",\"Wind_Speed(mph)\",\n",
    "    \"Precipitation(in)\"\n",
    "]\n",
    "df_no_na = df_clean.na.drop(subset=num_feats)\n",
    "\n",
    "# --- 4) Split & cache ---\n",
    "train_full, test_full = df_no_na.randomSplit([0.8,0.2], seed=42)\n",
    "train_cached = train_full.repartition(200).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "train_cached.count()\n",
    "\n",
    "# --- 5) 3-class bucket (%33, %66 quantile) ---\n",
    "qs = train_cached.approxQuantile(\"distance_mi\",[0.33,0.66],1e-4)\n",
    "splits = [float(\"-inf\"), qs[0], qs[1], float(\"inf\")]\n",
    "bucketizer = Bucketizer(inputCol=\"distance_mi\",\n",
    "                        outputCol=\"distance_class\",\n",
    "                        splits=splits)\n",
    "\n",
    "# --- 6) Weather_Condition encode ---\n",
    "indexer = StringIndexer(inputCol=\"Weather_Condition\",\n",
    "                        outputCol=\"wc_index\",\n",
    "                        handleInvalid=\"keep\")\n",
    "encoder = OneHotEncoder(inputCols=[\"wc_index\"],\n",
    "                        outputCols=[\"wc_vec\"],\n",
    "                        dropLast=False)\n",
    "\n",
    "# --- 7) Assembler & Scaler (time + numeric + weather) ---\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=num_feats + [\"hour\",\"weekday\",\"wc_vec\"],\n",
    "    outputCol=\"unscaled_features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "scaler = StandardScaler(inputCol=\"unscaled_features\",\n",
    "                        outputCol=\"features\",\n",
    "                        withMean=True, withStd=True)\n",
    "\n",
    "# --- 8) RandomForest ---\n",
    "rf = (RandomForestClassifier(labelCol=\"distance_class\",\n",
    "                             featuresCol=\"features\")\n",
    "      .setNumTrees(80)\n",
    "      .setMaxDepth(10)\n",
    "      .setSeed(42))\n",
    "\n",
    "# --- 9) Pipeline & Eğitim ---\n",
    "pipe = Pipeline(stages=[\n",
    "    bucketizer,\n",
    "    indexer, encoder,\n",
    "    assembler,\n",
    "    scaler,\n",
    "    rf\n",
    "])\n",
    "model = pipe.fit(train_cached)\n",
    "pred  = model.transform(test_full)\n",
    "\n",
    "# --- 10) Değerlendirme ---\n",
    "evaluator_acc = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"distance_class\", predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\")\n",
    "evaluator_f1  = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"distance_class\", predictionCol=\"prediction\",\n",
    "    metricName=\"f1\")\n",
    "\n",
    "print(\"DropNa + Weather – Accuracy =\", evaluator_acc.evaluate(pred))\n",
    "print(\"DropNa + Weather – F1 =\", evaluator_f1.evaluate(pred))\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ed479a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE = 0.492\n",
      "Test R2   = 0.025\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, radians, sin, cos, atan2, sqrt,\n",
    "    hour, dayofweek\n",
    ")\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, OneHotEncoder,\n",
    "    VectorAssembler, StandardScaler\n",
    ")\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 1) SparkSession başlat\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"DistancePrediction\")\n",
    "         .config(\"spark.executor.memory\",\"8g\")\n",
    "         .config(\"spark.driver.memory\",\"4g\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# 2) Veri yükle & temizleme\n",
    "df = (spark.read\n",
    "      .option(\"header\", True)\n",
    "      .option(\"inferSchema\", True)\n",
    "      .csv(\"data/traffic_data/US_Accidents_March23.csv\")\n",
    "      .dropna(subset=[\"Start_Lat\",\"Start_Lng\",\"End_Lat\",\"End_Lng\",\"Start_Time\"])\n",
    ")\n",
    "\n",
    "# 3) Haversine ile distance_mi hesaplama\n",
    "R = 3958.8\n",
    "df2 = (df\n",
    "       .withColumn(\"φ1\", radians(col(\"Start_Lat\")))\n",
    "       .withColumn(\"φ2\", radians(col(\"End_Lat\")))\n",
    "       .withColumn(\"Δφ\", radians(col(\"End_Lat\")-col(\"Start_Lat\")))\n",
    "       .withColumn(\"Δλ\", radians(col(\"End_Lng\")-col(\"Start_Lng\")))\n",
    "       .withColumn(\"a\",\n",
    "           sin(col(\"Δφ\")/2)**2 +\n",
    "           cos(col(\"φ1\"))*cos(col(\"φ2\"))*sin(col(\"Δλ\")/2)**2\n",
    "       )\n",
    "       .withColumn(\"distance_mi\", 2*R*atan2(sqrt(col(\"a\")), sqrt(1-col(\"a\"))))\n",
    "       .drop(\"φ1\",\"φ2\",\"Δφ\",\"Δλ\",\"a\")\n",
    ")\n",
    "\n",
    "# 4) Outlier temizleme (IQR)\n",
    "q1, q3 = df2.approxQuantile(\"distance_mi\", [0.25,0.75], 1e-4)\n",
    "iqr    = q3 - q1\n",
    "df_clean = df2.filter((col(\"distance_mi\")>=q1-1.5*iqr) & (col(\"distance_mi\")<=q3+1.5*iqr))\n",
    "\n",
    "# 5) Özellik sütunları & zaman/hava ekle\n",
    "num_feats = [\n",
    "    \"Temperature(F)\",\"Wind_Chill(F)\",\"Humidity(%)\",\n",
    "    \"Pressure(in)\",\"Visibility(mi)\",\"Wind_Speed(mph)\",\n",
    "    \"Precipitation(in)\"\n",
    "]\n",
    "df_feat = (df_clean\n",
    "           .withColumn(\"hour\",    hour(col(\"Start_Time\")))\n",
    "           .withColumn(\"weekday\", dayofweek(col(\"Start_Time\")))\n",
    "           # sayısal sütunlarda kayıp varsa at\n",
    "           .na.drop(subset=num_feats)\n",
    "           # kategori sütunu doldur\n",
    "           .na.fill({\"Weather_Condition\":\"Unknown\"})\n",
    ")\n",
    "\n",
    "# 6) Weather_Condition → index + one‑hot\n",
    "indexer = StringIndexer(\n",
    "    inputCol=\"Weather_Condition\",\n",
    "    outputCol=\"wc_idx\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[\"wc_idx\"],\n",
    "    outputCols=[\"wc_vec\"],\n",
    "    dropLast=False\n",
    ")\n",
    "\n",
    "# 7) Vektör oluştur & ölçeklendir\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=num_feats + [\"hour\",\"weekday\",\"wc_vec\"],\n",
    "    outputCol=\"raw_features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"raw_features\",\n",
    "    outputCol=\"features\",\n",
    "    withMean=True, withStd=True\n",
    ")\n",
    "\n",
    "# 8) GBTRegressor\n",
    "gbt = GBTRegressor(\n",
    "    labelCol=\"distance_mi\",\n",
    "    featuresCol=\"features\",\n",
    "    maxIter=50,\n",
    "    maxDepth=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# 9) Pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    indexer, encoder,\n",
    "    assembler, scaler,\n",
    "    gbt\n",
    "])\n",
    "\n",
    "# 10) Train/Test split & eğit\n",
    "train, test = df_feat.randomSplit([0.8,0.2], seed=42)\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "# 11) Tahmin & değerlendirme\n",
    "pred = model.transform(test)\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"distance_mi\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "rmse = evaluator.evaluate(pred)\n",
    "r2   = evaluator.setMetricName(\"r2\").evaluate(pred)\n",
    "\n",
    "print(f\"Test RMSE = {rmse:.3f}\")\n",
    "print(f\"Test R2   = {r2:.3f}\")\n",
    "\n",
    "# 12) Kapat\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab63d780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3‑class Accuracy = 0.5007\n",
      "3‑class F1       = 0.5007\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, radians, sin, cos, atan2, sqrt,\n",
    "    hour, dayofweek\n",
    ")\n",
    "from pyspark.ml.feature import (\n",
    "    Bucketizer,\n",
    "    StringIndexer, OneHotEncoder,\n",
    "    VectorAssembler, StandardScaler\n",
    ")\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 1) SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Distance_3Class_Classification\")\n",
    "         .config(\"spark.executor.memory\",\"8g\")\n",
    "         .config(\"spark.driver.memory\",\"4g\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# 2) Veri yükle & kritikleri at\n",
    "df = (spark.read\n",
    "      .option(\"header\", True).option(\"inferSchema\", True)\n",
    "      .csv(\"data/traffic_data/US_Accidents_March23.csv\")\n",
    "      .dropna(subset=[\"Start_Lat\",\"Start_Lng\",\"End_Lat\",\"End_Lng\",\"Start_Time\"])\n",
    ")\n",
    "\n",
    "# 3) distance_mi hesapla\n",
    "R = 3958.8\n",
    "df = (df\n",
    "      .withColumn(\"φ1\", radians(col(\"Start_Lat\")))\n",
    "      .withColumn(\"φ2\", radians(col(\"End_Lat\")))\n",
    "      .withColumn(\"Δφ\", radians(col(\"End_Lat\")-col(\"Start_Lat\")))\n",
    "      .withColumn(\"Δλ\", radians(col(\"End_Lng\")-col(\"Start_Lng\")))\n",
    "      .withColumn(\"a\",\n",
    "          sin(col(\"Δφ\")/2)**2 +\n",
    "          cos(col(\"φ1\"))*cos(col(\"φ2\"))*sin(col(\"Δλ\")/2)**2\n",
    "      )\n",
    "      .withColumn(\"distance_mi\",\n",
    "          2*R*atan2(sqrt(col(\"a\")), sqrt(1-col(\"a\")))\n",
    "      )\n",
    "      .drop(\"φ1\",\"φ2\",\"Δφ\",\"Δλ\",\"a\")\n",
    ")\n",
    "\n",
    "# 4) Aykırıları at (IQR)\n",
    "q1, q3 = df.approxQuantile(\"distance_mi\",[0.25,0.75],1e-4)\n",
    "iqr    = q3 - q1\n",
    "df = df.filter((col(\"distance_mi\")>=q1-1.5*iqr) &\n",
    "               (col(\"distance_mi\")<=q3+1.5*iqr))\n",
    "\n",
    "# 5) Sayısal özellikler ve time/ weather hazırlığı\n",
    "num_feats = [\n",
    "    \"Temperature(F)\",\"Wind_Chill(F)\",\"Humidity(%)\",\n",
    "    \"Pressure(in)\",\"Visibility(mi)\",\"Wind_Speed(mph)\",\n",
    "    \"Precipitation(in)\"\n",
    "]\n",
    "df = (df.withColumn(\"hour\",    hour(col(\"Start_Time\")))\n",
    "        .withColumn(\"weekday\", dayofweek(col(\"Start_Time\")))\n",
    "        # sayısal nan'leri at\n",
    "        .na.drop(subset=num_feats)\n",
    "        # weather boşsa Unknown\n",
    "        .na.fill({\"Weather_Condition\":\"Unknown\"})\n",
    ")\n",
    "\n",
    "# 6) 3-class bucket eşikleri\n",
    "# Örnek: 0.2 ve 1.0 mil\n",
    "threshold_low  = 0.2\n",
    "threshold_high = 1.0\n",
    "splits = [float(\"-inf\"), threshold_low, threshold_high, float(\"inf\")]\n",
    "bucketizer = Bucketizer(inputCol=\"distance_mi\",\n",
    "                        outputCol=\"distance_class\",\n",
    "                        splits=splits)\n",
    "\n",
    "# 7) Weather_Condition → index + one‐hot\n",
    "indexer = StringIndexer(inputCol=\"Weather_Condition\",\n",
    "                        outputCol=\"wc_idx\",\n",
    "                        handleInvalid=\"keep\")\n",
    "encoder = OneHotEncoder(inputCols=[\"wc_idx\"],\n",
    "                        outputCols=[\"wc_vec\"],\n",
    "                        dropLast=False)\n",
    "\n",
    "# 8) Özellik vektörü: sayısal + time + weather vec\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=num_feats + [\"hour\",\"weekday\",\"wc_vec\"],\n",
    "    outputCol=\"raw_features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "scaler = StandardScaler(inputCol=\"raw_features\",\n",
    "                        outputCol=\"features\",\n",
    "                        withMean=True, withStd=True)\n",
    "\n",
    "# 9) Sınıflandırıcı\n",
    "rf = (RandomForestClassifier(labelCol=\"distance_class\",\n",
    "                             featuresCol=\"features\")\n",
    "      .setNumTrees(50)\n",
    "      .setMaxDepth(8)\n",
    "      .setSeed(42))\n",
    "\n",
    "# 10) Pipeline ve eğitim\n",
    "pipe = Pipeline(stages=[\n",
    "    bucketizer,\n",
    "    indexer, encoder,\n",
    "    assembler, scaler,\n",
    "    rf\n",
    "])\n",
    "\n",
    "train, test = df.randomSplit([0.8,0.2], seed=42)\n",
    "model = pipe.fit(train)\n",
    "pred  = model.transform(test)\n",
    "\n",
    "# 11) Değerlendirme\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"distance_class\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "acc = evaluator.setMetricName(\"accuracy\").evaluate(pred)\n",
    "f1  = evaluator.evaluate(pred)\n",
    "\n",
    "print(f\"3‑class Accuracy = {acc:.4f}\")\n",
    "print(f\"3‑class F1       = {f1:.4f}\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0256c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3‑class Accuracy = 0.7157\n",
      "3‑class F1       = 0.7107\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "3‑class distance classification without any grid search or CV.\n",
    "Fixed hyperparameters only.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, radians, sin, cos, atan2, sqrt,\n",
    "    hour, dayofweek\n",
    ")\n",
    "from pyspark.ml.feature import (\n",
    "    Bucketizer,\n",
    "    StringIndexer, OneHotEncoder,\n",
    "    VectorAssembler, StandardScaler\n",
    ")\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 1) SparkSession başlat\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Distance_3Class_NoGrid\")\n",
    "         .config(\"spark.executor.memory\",\"8g\")\n",
    "         .config(\"spark.driver.memory\",  \"4g\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# 2) Veriyi yükle ve temel temizleme\n",
    "df = (spark.read\n",
    "      .option(\"header\", True).option(\"inferSchema\", True)\n",
    "      .csv(\"data/traffic_data/US_Accidents_March23.csv\")\n",
    "      .dropna(subset=[\"Start_Lat\",\"Start_Lng\",\"End_Lat\",\"End_Lng\",\"Start_Time\"])\n",
    ")\n",
    "\n",
    "# 3) distance_mi hesapla (Haversine)\n",
    "R = 3958.8\n",
    "df = (df\n",
    "      .withColumn(\"φ1\", radians(col(\"Start_Lat\")))\n",
    "      .withColumn(\"φ2\", radians(col(\"End_Lat\")))\n",
    "      .withColumn(\"Δφ\", radians(col(\"End_Lat\") - col(\"Start_Lat\")))\n",
    "      .withColumn(\"Δλ\", radians(col(\"End_Lng\") - col(\"Start_Lng\")))\n",
    "      .withColumn(\"a\",\n",
    "          sin(col(\"Δφ\")/2)**2 +\n",
    "          cos(col(\"φ1\")) * cos(col(\"φ2\")) * sin(col(\"Δλ\")/2)**2\n",
    "      )\n",
    "      .withColumn(\"distance_mi\",\n",
    "          2 * R * atan2(sqrt(col(\"a\")), sqrt(1 - col(\"a\")))\n",
    "      )\n",
    "      .drop(\"φ1\",\"φ2\",\"Δφ\",\"Δλ\",\"a\")\n",
    ")\n",
    "\n",
    "# 4) Aykırı değerleri IQR ile at\n",
    "q1, q3 = df.approxQuantile(\"distance_mi\", [0.25,0.75], 1e-4)\n",
    "iqr    = q3 - q1\n",
    "df = df.filter((col(\"distance_mi\") >= q1 - 1.5*iqr) &\n",
    "               (col(\"distance_mi\") <= q3 + 1.5*iqr))\n",
    "\n",
    "# 5) Ek özellikler: zaman + coğrafi farklar\n",
    "df = (df\n",
    "      .withColumn(\"dLat\",    col(\"End_Lat\") - col(\"Start_Lat\"))\n",
    "      .withColumn(\"dLon\",    col(\"End_Lng\") - col(\"Start_Lng\"))\n",
    "      .withColumn(\"hour\",    hour(col(\"Start_Time\")))\n",
    "      .withColumn(\"weekday\", dayofweek(col(\"Start_Time\")))\n",
    ")\n",
    "\n",
    "# 6) Eksik değer temizleme\n",
    "num_feats = [\n",
    "    \"Temperature(F)\",\"Wind_Chill(F)\",\"Humidity(%)\",\n",
    "    \"Pressure(in)\",\"Visibility(mi)\",\"Wind_Speed(mph)\",\n",
    "    \"Precipitation(in)\",\"dLat\",\"dLon\"\n",
    "]\n",
    "df = df.na.drop(subset=num_feats) \\\n",
    "       .na.fill({\"Weather_Condition\":\"Unknown\"})\n",
    "\n",
    "# 7) Train/Test split\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "train = train.repartition(200)\n",
    "\n",
    "# 8) Quantile‐based 3‐class eşikler\n",
    "qs = train.approxQuantile(\"distance_mi\", [0.33, 0.66], 1e-4)\n",
    "splits = [float(\"-inf\"), qs[0], qs[1], float(\"inf\")]\n",
    "bucketizer = Bucketizer(inputCol=\"distance_mi\",\n",
    "                        outputCol=\"distance_class\",\n",
    "                        splits=splits)\n",
    "\n",
    "# 9) Weather_Condition encode\n",
    "indexer = StringIndexer(inputCol=\"Weather_Condition\",\n",
    "                        outputCol=\"wc_idx\", handleInvalid=\"keep\")\n",
    "encoder = OneHotEncoder(inputCols=[\"wc_idx\"],\n",
    "                        outputCols=[\"wc_vec\"], dropLast=False)\n",
    "\n",
    "# 10) Özellik vektörü oluştur ve ölçeklendir\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=num_feats + [\"hour\",\"weekday\",\"wc_vec\"],\n",
    "    outputCol=\"raw_features\", handleInvalid=\"skip\"\n",
    ")\n",
    "scaler = StandardScaler(inputCol=\"raw_features\",\n",
    "                        outputCol=\"features\",\n",
    "                        withMean=True, withStd=True)\n",
    "\n",
    "# 11) Sabit parametreli RandomForest\n",
    "rf = RandomForestClassifier(labelCol=\"distance_class\",\n",
    "                            featuresCol=\"features\",\n",
    "                            numTrees=50,\n",
    "                            maxDepth=8,\n",
    "                            seed=42)\n",
    "\n",
    "# 12) Pipeline ve eğitim\n",
    "pipeline = Pipeline(stages=[\n",
    "    bucketizer,\n",
    "    indexer, encoder,\n",
    "    assembler, scaler,\n",
    "    rf\n",
    "])\n",
    "model = pipeline.fit(train)\n",
    "pred  = model.transform(test)\n",
    "\n",
    "# 13) Değerlendirme\n",
    "evaluator_acc = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"distance_class\", predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\")\n",
    "evaluator_f1  = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"distance_class\", predictionCol=\"prediction\",\n",
    "    metricName=\"f1\")\n",
    "\n",
    "acc = evaluator_acc.evaluate(pred)\n",
    "f1  = evaluator_f1.evaluate(pred)\n",
    "print(f\"3‑class Accuracy = {acc:.4f}\")\n",
    "print(f\"3‑class F1       = {f1:.4f}\")\n",
    "\n",
    "# 14) SparkSession’ı kapat\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48f65762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6029788883819928\n",
      "F1       = 0.5807768702230311\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "3‑class distance classification with cell_id and saving the trained model.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, radians, sin, cos, atan2, sqrt,\n",
    "    hour, dayofweek\n",
    ")\n",
    "from pyspark.ml.feature import (\n",
    "    Bucketizer,\n",
    "    StringIndexer, OneHotEncoder,\n",
    "    VectorAssembler, StandardScaler\n",
    ")\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 1) SparkSession başlat\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Distance_3Class_WithCell_SaveModel\")\n",
    "         .config(\"spark.executor.memory\",\"8g\")\n",
    "         .config(\"spark.driver.memory\",\"4g\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# --- Veri hazırlık ---\n",
    "df = (spark.read\n",
    "      .option(\"header\", True).option(\"inferSchema\", True)\n",
    "      .csv(\"data/traffic_data/US_Accidents_March23.csv\")\n",
    "      .dropna(subset=[\"Start_Lat\",\"Start_Lng\",\"End_Lat\",\"End_Lng\",\"Start_Time\"])\n",
    ")\n",
    "\n",
    "# distance_mi hesapla\n",
    "R = 3958.8\n",
    "df = (df\n",
    "      .withColumn(\"φ1\", radians(col(\"Start_Lat\")))\n",
    "      .withColumn(\"φ2\", radians(col(\"End_Lat\")))\n",
    "      .withColumn(\"Δφ\", radians(col(\"End_Lat\")-col(\"Start_Lat\")))\n",
    "      .withColumn(\"Δλ\", radians(col(\"End_Lng\")-col(\"Start_Lng\")))\n",
    "      .withColumn(\"a\", sin(col(\"Δφ\")/2)**2 +\n",
    "                       cos(col(\"φ1\"))*cos(col(\"φ2\"))*sin(col(\"Δλ\")/2)**2)\n",
    "      .withColumn(\"distance_mi\", 2*R*atan2(sqrt(col(\"a\")), sqrt(1-col(\"a\"))))\n",
    "      .drop(\"φ1\",\"φ2\",\"Δφ\",\"Δλ\",\"a\")\n",
    ")\n",
    "\n",
    "# IQR ile outlier at\n",
    "q1, q3 = df.approxQuantile(\"distance_mi\",[0.25,0.75],1e-4)\n",
    "iqr    = q3 - q1\n",
    "df = df.filter((col(\"distance_mi\")>=q1-1.5*iqr)&(col(\"distance_mi\")<=q3+1.5*iqr))\n",
    "\n",
    "# coğrafi farklar + zaman + hücre kimliği\n",
    "min_lat = df.agg({\"Start_Lat\":\"min\"}).collect()[0][0]\n",
    "min_lng = df.agg({\"Start_Lng\":\"min\"}).collect()[0][0]\n",
    "grid_size = 0.045\n",
    "\n",
    "df = (df\n",
    "      .withColumn(\"dLat\",     col(\"End_Lat\")-col(\"Start_Lat\"))\n",
    "      .withColumn(\"dLon\",     col(\"End_Lng\")-col(\"Start_Lng\"))\n",
    "      .withColumn(\"hour\",     hour(col(\"Start_Time\")))\n",
    "      .withColumn(\"weekday\",  dayofweek(col(\"Start_Time\")))\n",
    "      .withColumn(\"lat_bin\",  ((col(\"Start_Lat\")-min_lat)/grid_size).cast(\"int\"))\n",
    "      .withColumn(\"lng_bin\",  ((col(\"Start_Lng\")-min_lng)/grid_size).cast(\"int\"))\n",
    "      .withColumn(\"cell_id\",  col(\"lat_bin\")*10000 + col(\"lng_bin\"))\n",
    ")\n",
    "\n",
    "# sayısal NaN’leri at, weather doldur\n",
    "num_feats = [\"Temperature(F)\",\"Wind_Chill(F)\",\"Humidity(%)\",\n",
    "             \"Pressure(in)\",\"Visibility(mi)\",\"Wind_Speed(mph)\",\n",
    "             \"Precipitation(in)\",\"dLat\",\"dLon\"]\n",
    "df = df.na.drop(subset=num_feats).na.fill({\"Weather_Condition\":\"Unknown\"})\n",
    "\n",
    "# train/test\n",
    "train, test = df.randomSplit([0.8,0.2], seed=42)\n",
    "\n",
    "# 3‑sınıf quantile eşikleri\n",
    "qs = train.approxQuantile(\"distance_mi\",[0.33,0.66],1e-4)\n",
    "splits = [float(\"-inf\"), qs[0], qs[1], float(\"inf\")]\n",
    "bucket = Bucketizer(inputCol=\"distance_mi\",\n",
    "                    outputCol=\"distance_class\",\n",
    "                    splits=splits)\n",
    "\n",
    "# feature encoding\n",
    "idx_wc    = StringIndexer(inputCol=\"Weather_Condition\", outputCol=\"wc_idx\", handleInvalid=\"keep\")\n",
    "enc_wc    = OneHotEncoder(inputCols=[\"wc_idx\"], outputCols=[\"wc_vec\"], dropLast=False)\n",
    "idx_cell  = StringIndexer(inputCol=\"cell_id\",           outputCol=\"cell_idx\", handleInvalid=\"keep\")\n",
    "enc_cell  = OneHotEncoder(inputCols=[\"cell_idx\"],       outputCols=[\"cell_vec\"], dropLast=False)\n",
    "\n",
    "# assembler & scaler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=num_feats + [\"hour\",\"weekday\",\"wc_vec\",\"cell_vec\"],\n",
    "    outputCol=\"raw_features\", handleInvalid=\"skip\"\n",
    ")\n",
    "scaler = StandardScaler(inputCol=\"raw_features\",\n",
    "                        outputCol=\"features\",\n",
    "                        withMean=True, withStd=True)\n",
    "\n",
    "# RF sabit ayar\n",
    "rf = RandomForestClassifier(labelCol=\"distance_class\",\n",
    "                            featuresCol=\"features\",\n",
    "                            numTrees=100,\n",
    "                            maxDepth=10,\n",
    "                            seed=42)\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    bucket,\n",
    "    idx_wc, enc_wc,\n",
    "    idx_cell, enc_cell,\n",
    "    assembler, scaler,\n",
    "    rf\n",
    "])\n",
    "\n",
    "# model eğit\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "# test & değerlendirme\n",
    "pred  = model.transform(test)\n",
    "e_acc = MulticlassClassificationEvaluator(labelCol=\"distance_class\",\n",
    "                                           predictionCol=\"prediction\",\n",
    "                                           metricName=\"accuracy\")\n",
    "e_f1  = MulticlassClassificationEvaluator(labelCol=\"distance_class\",\n",
    "                                           predictionCol=\"prediction\",\n",
    "                                           metricName=\"f1\")\n",
    "print(\"Accuracy =\", e_acc.evaluate(pred))\n",
    "print(\"F1       =\", e_f1.evaluate(pred))\n",
    "\n",
    "# --- Modeli kaydet ---\n",
    "model.write().overwrite().save(\"models/distance_3class_rf_with_cell\")\n",
    "\n",
    "# SparkSession’ı kapat\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "760fb5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3‑class Accuracy = 0.7243\n",
      "3‑class F1       = 0.7217\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "3‑class distance classification without any grid search or CV.\n",
    "Fixed hyperparameters only.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, radians, sin, cos, atan2, sqrt,\n",
    "    hour, dayofweek\n",
    ")\n",
    "from pyspark.ml.feature import (\n",
    "    Bucketizer,\n",
    "    StringIndexer, OneHotEncoder,\n",
    "    VectorAssembler, StandardScaler\n",
    ")\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 1) SparkSession başlat\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Distance_3Class_NoGrid\")\n",
    "         .config(\"spark.executor.memory\",\"8g\")\n",
    "         .config(\"spark.driver.memory\",  \"4g\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# 2) Veriyi yükle ve temel temizleme\n",
    "df = (spark.read\n",
    "      .option(\"header\", True).option(\"inferSchema\", True)\n",
    "      .csv(\"data/traffic_data/US_Accidents_March23.csv\")\n",
    "      .dropna(subset=[\"Start_Lat\",\"Start_Lng\",\"End_Lat\",\"End_Lng\",\"Start_Time\"])\n",
    ")\n",
    "\n",
    "# 3) distance_mi hesapla (Haversine)\n",
    "R = 3958.8\n",
    "df = (df\n",
    "      .withColumn(\"φ1\", radians(col(\"Start_Lat\")))\n",
    "      .withColumn(\"φ2\", radians(col(\"End_Lat\")))\n",
    "      .withColumn(\"Δφ\", radians(col(\"End_Lat\") - col(\"Start_Lat\")))\n",
    "      .withColumn(\"Δλ\", radians(col(\"End_Lng\") - col(\"Start_Lng\")))\n",
    "      .withColumn(\"a\",\n",
    "          sin(col(\"Δφ\")/2)**2 +\n",
    "          cos(col(\"φ1\")) * cos(col(\"φ2\")) * sin(col(\"Δλ\")/2)**2\n",
    "      )\n",
    "      .withColumn(\"distance_mi\",\n",
    "          2 * R * atan2(sqrt(col(\"a\")), sqrt(1 - col(\"a\")))\n",
    "      )\n",
    "      .drop(\"φ1\",\"φ2\",\"Δφ\",\"Δλ\",\"a\")\n",
    ")\n",
    "\n",
    "# 4) Aykırı değerleri IQR ile at\n",
    "q1, q3 = df.approxQuantile(\"distance_mi\", [0.25,0.75], 1e-4)\n",
    "iqr    = q3 - q1\n",
    "df = df.filter((col(\"distance_mi\") >= q1 - 1.5*iqr) &\n",
    "               (col(\"distance_mi\") <= q3 + 1.5*iqr))\n",
    "\n",
    "# 5) Ek özellikler: zaman + coğrafi farklar\n",
    "df = (df\n",
    "      .withColumn(\"dLat\",    col(\"End_Lat\") - col(\"Start_Lat\"))\n",
    "      .withColumn(\"dLon\",    col(\"End_Lng\") - col(\"Start_Lng\"))\n",
    "      .withColumn(\"hour\",    hour(col(\"Start_Time\")))\n",
    "      .withColumn(\"weekday\", dayofweek(col(\"Start_Time\")))\n",
    ")\n",
    "\n",
    "# 6) Eksik değer temizleme\n",
    "num_feats = [\n",
    "    \"Temperature(F)\",\"Wind_Chill(F)\",\"Humidity(%)\",\n",
    "    \"Pressure(in)\",\"Visibility(mi)\",\"Wind_Speed(mph)\",\n",
    "    \"Precipitation(in)\",\"dLat\",\"dLon\"\n",
    "]\n",
    "df = df.na.drop(subset=num_feats) \\\n",
    "       .na.fill({\"Weather_Condition\":\"Unknown\"})\n",
    "\n",
    "# 7) Train/Test split\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "train = train.repartition(200)\n",
    "\n",
    "# 8) Quantile‐based 3‐class eşikler\n",
    "qs = train.approxQuantile(\"distance_mi\", [0.33, 0.66], 1e-4)\n",
    "splits = [float(\"-inf\"), qs[0], qs[1], float(\"inf\")]\n",
    "bucketizer = Bucketizer(inputCol=\"distance_mi\",\n",
    "                        outputCol=\"distance_class\",\n",
    "                        splits=splits)\n",
    "\n",
    "# 9) Weather_Condition encode\n",
    "indexer = StringIndexer(inputCol=\"Weather_Condition\",\n",
    "                        outputCol=\"wc_idx\", handleInvalid=\"keep\")\n",
    "encoder = OneHotEncoder(inputCols=[\"wc_idx\"],\n",
    "                        outputCols=[\"wc_vec\"], dropLast=False)\n",
    "\n",
    "# 10) Özellik vektörü oluştur ve ölçeklendir\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=num_feats + [\"hour\",\"weekday\",\"wc_vec\"],\n",
    "    outputCol=\"raw_features\", handleInvalid=\"skip\"\n",
    ")\n",
    "scaler = StandardScaler(inputCol=\"raw_features\",\n",
    "                        outputCol=\"features\",\n",
    "                        withMean=True, withStd=True)\n",
    "\n",
    "# 11) Sabit parametreli RandomForest\n",
    "rf = RandomForestClassifier(labelCol=\"distance_class\",\n",
    "                            featuresCol=\"features\",\n",
    "                            numTrees=50,\n",
    "                            maxDepth=8,\n",
    "                            seed=42)\n",
    "\n",
    "# 12) Pipeline ve eğitim\n",
    "pipeline = Pipeline(stages=[\n",
    "    bucketizer,\n",
    "    indexer, encoder,\n",
    "    assembler, scaler,\n",
    "    rf\n",
    "])\n",
    "model = pipeline.fit(train)\n",
    "pred  = model.transform(test)\n",
    "\n",
    "# 13) Değerlendirme\n",
    "evaluator_acc = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"distance_class\", predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\")\n",
    "evaluator_f1  = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"distance_class\", predictionCol=\"prediction\",\n",
    "    metricName=\"f1\")\n",
    "\n",
    "acc = evaluator_acc.evaluate(pred)\n",
    "f1  = evaluator_f1.evaluate(pred)\n",
    "print(f\"3‑class Accuracy = {acc:.4f}\")\n",
    "print(f\"3‑class F1       = {f1:.4f}\")\n",
    "\n",
    "model.write().overwrite().save(\"models/distance_3class_rf_with_final_cell\")\n",
    "# 14) SparkSession’ı kapat\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fac6a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] CWD: c:\\Users\\aslay\\Desktop\\BİL 401\\project\n",
      "[DEBUG] Loading PipelineModel from: c:\\Users\\aslay\\Desktop\\BİL 401\\project\\models\\us_accidents_dt_final_last_full_spark\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (MSI executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.io.EOFException\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.io.EOFException\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 32 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 74\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mAccidentSeverityPredictor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AccidentSeverityPredictor\n\u001b[0;32m      6\u001b[0m veri \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      7\u001b[0m     {\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart_Time\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2023-12-10 07:00:00\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m     }\n\u001b[0;32m     71\u001b[0m ]\n\u001b[1;32m---> 74\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mAccidentSeverityPredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/us_accidents_dt_final_last_full_spark\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m predictor\u001b[38;5;241m.\u001b[39mpredict_from_rows(    \n\u001b[0;32m     77\u001b[0m     rows\u001b[38;5;241m=\u001b[39mveri,\n\u001b[0;32m     78\u001b[0m     show_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCity_Cleaned\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStreet_Cleaned\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     79\u001b[0m     n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     80\u001b[0m )\n\u001b[0;32m     82\u001b[0m predictor\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32mc:\\Users\\aslay\\Desktop\\BİL 401\\project\\AccidentSeverityPredictor.py:36\u001b[0m, in \u001b[0;36mAccidentSeverityPredictor.__init__\u001b[1;34m(self, model_path, top_n)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipelineModel klasörü bulunamadı: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mabs_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[DEBUG] Loading PipelineModel from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mabs_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mPipelineModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabs_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_n \u001b[38;5;241m=\u001b[39m top_n\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\util.py:369\u001b[0m, in \u001b[0;36mMLReadable.load\u001b[1;34m(cls, path)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RL:\n\u001b[0;32m    368\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\pipeline.py:282\u001b[0m, in \u001b[0;36mPipelineModelReader.load\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipelineModel\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 282\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mDefaultParamsReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadMetadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m JavaMLReader(cast(Type[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJavaMLReadable[PipelineModel]\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls))\u001b[38;5;241m.\u001b[39mload(path)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\util.py:579\u001b[0m, in \u001b[0;36mDefaultParamsReader.loadMetadata\u001b[1;34m(path, sc, expectedClassName)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;124;03mLoad metadata saved using :py:meth:`DefaultParamsWriter.saveMetadata`\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;124;03m    If non empty, this is checked against the loaded metadata.\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    578\u001b[0m metadataPath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 579\u001b[0m metadataStr \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadataPath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    580\u001b[0m loadedVals \u001b[38;5;241m=\u001b[39m DefaultParamsReader\u001b[38;5;241m.\u001b[39m_parseMetaData(metadataStr, expectedClassName)\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loadedVals\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\rdd.py:2888\u001b[0m, in \u001b[0;36mRDD.first\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2862\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m   2863\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2864\u001b[0m \u001b[38;5;124;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[0;32m   2865\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2886\u001b[0m \u001b[38;5;124;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[0;32m   2887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2888\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2889\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[0;32m   2890\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\rdd.py:2855\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   2852\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2854\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[1;32m-> 2855\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2857\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[0;32m   2858\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\context.py:2510\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   2508\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[0;32m   2509\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2510\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (MSI executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.io.EOFException\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.io.EOFException\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 32 more\r\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from AccidentSeverityPredictor import AccidentSeverityPredictor\n",
    "\n",
    "veri = [\n",
    "    {\n",
    "        \"Start_Time\": \"2023-12-10 07:00:00\",\n",
    "        \"End_Time\": \"2023-12-10 07:45:00\",\n",
    "        \"Temperature(F)\": 15.0,\n",
    "        \"Humidity(%)\": 85.0,\n",
    "        \"Pressure(in)\": 29.5,\n",
    "        \"Visibility(mi)\": 2.0,\n",
    "        \"Wind_Speed(mph)\": 20.0,\n",
    "        \"Precipitation(in)\": 0.4,\n",
    "        \"Weather_Condition\": \"Snow\",\n",
    "        \"Wind_Direction\": \"N\",\n",
    "        \"Civil_Twilight\": \"Night\",\n",
    "        \"Sunrise_Sunset\": \"Night\",\n",
    "        \"State\": \"IL\",\n",
    "        \"Junction\": True,\n",
    "        \"Traffic_Signal\": False,\n",
    "        \"Crossing\": True,\n",
    "        \"City\": \"Chicago\",\n",
    "        \"Street\": \"W Adams St\",\n",
    "        \"Wind_Chill(F)\": 10.0\n",
    "    },\n",
    "    {\n",
    "        \"Start_Time\": \"2024-03-20 18:30:00\",\n",
    "        \"End_Time\": \"2024-03-20 18:40:00\",\n",
    "        \"Temperature(F)\": 85.0,\n",
    "        \"Humidity(%)\": 30.0,\n",
    "        \"Pressure(in)\": 30.2,\n",
    "        \"Visibility(mi)\": 10.0,\n",
    "        \"Wind_Speed(mph)\": 3.0,\n",
    "        \"Precipitation(in)\": 0.0,\n",
    "        \"Weather_Condition\": \"Clear\",\n",
    "        \"Wind_Direction\": \"SE\",\n",
    "        \"Civil_Twilight\": \"Day\",\n",
    "        \n",
    "        \"Sunrise_Sunset\": \"Day\",\n",
    "        \"State\": \"TX\",\n",
    "        \"Junction\": False,\n",
    "        \"Traffic_Signal\": True,\n",
    "        \"Crossing\": False,\n",
    "        \"City\": \"Houston\",\n",
    "        \"Street\": \"Main St\",\n",
    "        \"Wind_Chill(F)\": 84.0\n",
    "    },\n",
    "    {\n",
    "        \"Start_Time\": \"2023-09-01 05:00:00\",\n",
    "        \"End_Time\": \"2023-09-01 06:00:00\",\n",
    "        \"Temperature(F)\": 40.0,\n",
    "        \"Humidity(%)\": 95.0,\n",
    "        \"Pressure(in)\": 29.3,\n",
    "        \"Visibility(mi)\": 1.0,\n",
    "        \"Wind_Speed(mph)\": 15.0,\n",
    "        \"Precipitation(in)\": 1.0,\n",
    "        \"Weather_Condition\": \"Heavy Rain\",\n",
    "        \"Wind_Direction\": \"W\",\n",
    "        \"Civil_Twilight\": \"Night\",\n",
    "        \"Sunrise_Sunset\": \"Night\",\n",
    "        \"State\": \"FL\",\n",
    "        \"Junction\": True,\n",
    "        \"Traffic_Signal\": True,\n",
    "        \"Crossing\": True,\n",
    "        \"City\": \"Miami\",\n",
    "        \"Street\": \"Biscayne Blvd\",\n",
    "        \"Wind_Chill(F)\": 39.0\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "predictor = AccidentSeverityPredictor(\"models/us_accidents_dt_final_last_full_spark\")\n",
    "\n",
    "predictor.predict_from_rows(    \n",
    "    rows=veri,\n",
    "    show_columns=[\"City_Cleaned\", \"Street_Cleaned\", \"prediction\"],\n",
    "    n=10\n",
    ")\n",
    "\n",
    "predictor.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d252d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
